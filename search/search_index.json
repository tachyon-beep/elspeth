{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Elspeth Documentation","text":"<p>Extensible Layered Secure Pipeline Engine for Transformation and Handling</p> <p>Elspeth is a general-purpose orchestration platform implementing sense-decide-act workflows: sources provide inputs, transforms apply logic (analytical, decisional, or procedural), and sinks handle outputs\u2014whether storing results, triggering automation, or actuating real-world effects.</p> <p>Transformation covers any source-to-output logic: data ETL, LLM inference, statistical analysis, rule evaluation, or custom processing. Handling encompasses the full range of sink behaviors: persisting to databases, writing reports, sending notifications, invoking APIs, or commanding external systems.</p> <p>While Elspeth excels at LLM experimentation with hardened runners, policy-aware registries, and comparative studies, the plugin architecture supports any workflow topology. Security controls\u2014Bell-LaPadula Multi-Level Security (MLS) enforcement, artifact signing, audit logging, spreadsheet sanitization\u2014are baked into every pipeline stage.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<ul> <li> <p> Installation</p> <p>Get Elspeth running on your system in minutes</p> <p> Install now</p> </li> <li> <p> Quickstart</p> <p>Run your first experiment in 5 minutes (no API keys needed)</p> <p> Try it out</p> </li> <li> <p> First Experiment</p> <p>Build a complete experiment from scratch (15-20 minutes)</p> <p> Get started</p> </li> <li> <p> Security Model</p> <p>Understand Bell-LaPadula MLS enforcement</p> <p> Learn more</p> </li> </ul>"},{"location":"#core-features","title":"Core Features","text":""},{"location":"#security-first-design","title":"Security-First Design","text":"<ul> <li>\u2705 Bell-LaPadula Multi-Level Security (MLS) - Immutable classification with fail-fast validation</li> <li>\u2705 Artifact Signing - HMAC-SHA256/SHA512, RSA-PSS-SHA256, ECDSA-P256-SHA256</li> <li>\u2705 PII Detection - Block emails, SSNs, credit cards, Australian TFN/Medicare</li> <li>\u2705 Classified Material Detection - Block SECRET, TOP SECRET, TS//SCI markings</li> <li>\u2705 Formula Sanitization - Prevent Excel/CSV injection attacks</li> <li>\u2705 Comprehensive Audit Logging - JSONL audit trail with correlation IDs</li> </ul>"},{"location":"#flexible-architecture","title":"Flexible Architecture","text":"<ul> <li>\ud83d\udd0c 40+ Built-in Plugins - Datasources, transforms (LLM, ETL, analytics, rules), sinks, middleware</li> <li>\ud83d\udd04 Middleware Pipeline - Security filters, monitoring, content safety, custom logic</li> <li>\ud83d\udcca Workflow Helpers - Validation, aggregation, baseline comparison, early stop</li> <li>\ud83c\udfaf Dependency-Ordered Execution - Artifact pipeline with sink chaining</li> <li>\u26a1 Concurrency Support - Parallel execution with rate limiting</li> <li>\ud83d\udcbe Checkpoint Recovery - Resume long-running workflows</li> </ul>"},{"location":"#production-ready","title":"Production-Ready","text":"<ul> <li>\ud83d\udcdd Configuration as Code - Validated YAML with schema enforcement</li> <li>\ud83d\udd01 Retry Logic - Exponential backoff with exhaustion hooks</li> <li>\ud83d\udcb0 Cost Tracking - Token usage and API cost aggregation</li> <li>\ud83d\udcc8 Baseline Comparison - Statistical significance, effect size, Bayesian analysis</li> <li>\ud83c\udfa8 Visual Analytics - Charts (PNG/HTML) with metadata embedding</li> <li>\ud83d\udce6 Signed Bundles - Tamper-evident artifact packages</li> </ul>"},{"location":"#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  YAML Configuration \u2192 Validated \u2192 Environment Resolution    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                        \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Datasources (ClassifiedDataFrame)                          \u2502\n\u2502  CSV Local/Blob, Azure Blob \u2192 Tagged with SecurityLevel    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                        \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Workflow Orchestrator (Operating Level = MIN of all)       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                        \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Workflow Runner (Concurrency, Retries, Checkpoints)        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                        \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Transforms + Middleware (Security Filters, Monitoring)     \u2502\n\u2502  LLM (Azure OpenAI, HTTP, Mock), ETL, Analytics, Rules     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                        \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Artifact Pipeline (Dependency-Ordered Sinks)                \u2502\n\u2502  CSV, Excel, Signed Bundles, Azure Blob, GitHub Repos      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Key Principle: Security levels propagate throughout the pipeline with fail-fast enforcement at every boundary.</p>"},{"location":"#documentation-sections","title":"Documentation Sections","text":""},{"location":"#getting-started","title":"Getting Started","text":"<p>Perfect for new users learning Elspeth:</p> <ul> <li>Installation - Setup guide with lockfile requirements</li> <li>Quickstart - 5-minute hello world (no API keys)</li> <li>First Experiment - 15-20 minute tutorial from scratch</li> </ul>"},{"location":"#user-guide","title":"User Guide","text":"<p>In-depth guides for using Elspeth effectively:</p> <ul> <li>Security Model - Bell-LaPadula MLS explained with 4 scenarios</li> <li>Configuration - YAML reference with merge order, environment variables, 3 runnable patterns</li> </ul>"},{"location":"#plugins","title":"Plugins","text":"<p>Discover and configure 40+ built-in plugins:</p> <ul> <li>Plugin Catalogue - Use case-driven plugin documentation</li> <li>Loading Data (3 datasources)</li> <li>Processing with LLMs (4 clients + 8 middleware)</li> <li>Saving Results (11 sinks)</li> <li>Experiment Helpers (validation, aggregation, baseline, early stop)</li> <li>RAG (2 vector stores)</li> <li>Cost &amp; Rate Limiting (5 controls)</li> </ul>"},{"location":"#architecture_1","title":"Architecture","text":"<p>Understand Elspeth's design and decisions:</p> <ul> <li>Architecture Overview - System architecture, component layers, data flow</li> <li>ADR Catalogue - 13 Architecture Decision Records explaining \"why\"</li> <li>ADR-001: Design Philosophy (security-first, fail-closed)</li> <li>ADR-002: Multi-Level Security (Bell-LaPadula MLS)</li> <li>ADR-002a: Trusted Container Model (ClassifiedDataFrame)</li> <li>ADR-004: Mandatory BasePlugin Inheritance (security bones)</li> <li>ADR-005: Frozen Plugin Protection (dedicated infrastructure)</li> <li>...and 8 more</li> </ul>"},{"location":"#api-reference","title":"API Reference","text":"<p>Auto-generated API documentation for developers:</p> <ul> <li>API Overview - Quick navigation and examples</li> <li>Core - BasePlugin, ClassifiedDataFrame, SecurityLevel</li> <li>Registries - Plugin registration and factories</li> <li>Plugins - Datasource, Transform, Sink APIs</li> <li>Pipeline - Dependency resolution and chaining</li> </ul>"},{"location":"#common-use-cases","title":"Common Use Cases","text":""},{"location":"#data-etl-analytics","title":"Data ETL &amp; Analytics","text":"<pre><code># Transform and analyze data without external APIs\ndatasource:\n  type: csv_local\n  path: data/raw_data.csv\n  security_level: OFFICIAL\n\ntransform:\n  type: custom_analytics  # Rule-based logic, statistical analysis\n  security_level: OFFICIAL\n\nsinks:\n  - type: excel_workbook\n    base_path: reports/\n    security_level: OFFICIAL\n  - type: analytics_report\n    formats: [json, markdown]\n</code></pre> <p>Use for: Data validation, statistical analysis, business rule evaluation, compliance reporting</p>"},{"location":"#environmental-monitoring-alerting-sense-decide-act","title":"Environmental Monitoring &amp; Alerting (Sense-Decide-Act)","text":"<pre><code># Weather monitoring with automated public safety alerts\ndatasource:\n  type: satellite_telemetry  # Custom plugin: satellite weather data stream\n  refresh_interval: 300  # Poll every 5 minutes\n  security_level: OFFICIAL\n\ntransform:\n  type: meteorology_analyzer  # Custom plugin: specialist analysis system\n  thresholds:\n    severe_weather: 0.8\n    flood_risk: 0.7\n    fire_danger: 0.9\n  security_level: OFFICIAL\n\nsinks:\n  # Conditional routing based on analysis results\n  - type: emergency_broadcast  # Custom plugin: SMS/radio alert system\n    condition: severity &gt;= 0.8\n    recipients: regional_contacts\n    security_level: OFFICIAL\n\n  - type: weather_api  # Custom plugin: public weather service\n    condition: severity &gt;= 0.5\n    security_level: OFFICIAL\n\n  - type: archive_csv  # Standard plugin: historical record\n    path: data/weather_log.csv\n    security_level: OFFICIAL\n</code></pre> <p>Real-world pattern: - Sense: Satellite telemetry ingress (temperature, pressure, humidity) - Decide: Specialist meteorology system analyzes conditions, assigns severity scores - Act: Route alerts to emergency broadcast systems based on thresholds (high severity \u2192 SMS alerts, moderate \u2192 API updates, all \u2192 archive)</p> <p>Use for: Environmental monitoring, infrastructure sensors (IoT), industrial process control, public safety alerting, automated operations</p>"},{"location":"#llm-experimentation-development-testing","title":"LLM Experimentation (Development &amp; Testing)","text":"<pre><code># Simple LLM configuration for testing (no sensitive data)\ndatasource:\n  type: csv_local\n  path: data/test.csv\n  security_level: UNOFFICIAL\n\nllm:\n  type: mock  # No API keys needed\n  response_template: \"Mock: {text}\"\n  security_level: UNOFFICIAL\n\nsinks:\n  - type: csv\n    path: results.csv\n    security_level: UNOFFICIAL\n</code></pre> <p>Start here: Quickstart</p>"},{"location":"#llm-production-with-security","title":"LLM Production with Security","text":"<pre><code># Secure configuration with middleware and signing\ndatasource:\n  type: azure_blob\n  security_level: PROTECTED\n\nllm:\n  type: azure_openai\n  security_level: PROTECTED\n  middleware:\n    - type: pii_shield        # Block PII\n    - type: classified_material  # Block classified markings\n    - type: azure_content_safety  # External content check\n    - type: audit_logger      # Comprehensive logging\n\nsinks:\n  - type: signed_artifact\n    algorithm: HMAC-SHA256\n    security_level: PROTECTED\n</code></pre> <p>Learn more: Security Model, Configuration Guide</p>"},{"location":"#rag-with-baseline-comparison","title":"RAG with Baseline Comparison","text":"<pre><code># RAG-enabled experiment with statistical comparison\nexperiment:\n  datasource:\n    type: csv_local\n    path: data/questions.csv\n\n  llm:\n    type: azure_openai\n\n  row_plugins:\n    - type: retrieval_context  # Add vector store context\n      provider: pgvector\n      top_k: 5\n\n  baseline:\n    experiment_name: without_rag\n    comparison_plugins:\n      - type: score_significance\n        criteria: [accuracy, relevance]\n\n  sinks:\n    - type: analytics_report\n      formats: [json, markdown]\n</code></pre> <p>Learn more: Plugin Catalogue: RAG, Architecture Overview</p>"},{"location":"#project-information","title":"Project Information","text":""},{"location":"#version-status","title":"Version &amp; Status","text":"<ul> <li>Version: 0.1.0-dev (pre-release)</li> <li>Python: 3.12+</li> <li>License: [Check repository for details]</li> <li>Repository: GitHub</li> </ul>"},{"location":"#documentation-organization","title":"Documentation Organization","text":"<p>This documentation site provides user-facing guides and API reference. For comprehensive developer documentation including:</p> <ul> <li>ADR source files (full text with implementation details)</li> <li>Refactoring methodology (complexity reduction)</li> <li>Migration plans (plugin architecture evolution)</li> <li>Testing overview and strategy</li> <li>Compliance controls and traceability matrices</li> </ul> <p>...see the <code>docs/</code> directory in the repository.</p>"},{"location":"#getting-help","title":"Getting Help","text":"<ul> <li>Issues: GitHub Issues</li> <li>Discussions: GitHub Discussions</li> <li>Security: See <code>docs/compliance/incident-response.md</code> in repository</li> </ul>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li> <p> Run Your First Experiment</p> <p>Follow the quickstart guide to run a complete experiment in 5 minutes</p> <p> Quickstart</p> </li> <li> <p> Learn the Security Model</p> <p>Understand Bell-LaPadula MLS with 4 worked scenarios</p> <p> Security Model</p> </li> <li> <p> Explore Plugins</p> <p>Discover 40+ built-in plugins organized by use case</p> <p> Plugin Catalogue</p> </li> <li> <p> Develop Plugins</p> <p>Build custom datasources, transforms, and sinks</p> <p> API Reference</p> </li> </ul> <p>Welcome to Elspeth!</p> <p>Elspeth brings security-first orchestration to sense-decide-act workflows with:</p> <ul> <li>\u2705 Bell-LaPadula MLS enforcement (immutable classification)</li> <li>\u2705 40+ built-in plugins (extensible architecture)</li> <li>\u2705 Fail-fast validation (catch errors before data retrieval)</li> <li>\u2705 Comprehensive audit trails (JSONL logs with correlation IDs)</li> <li>\u2705 Production-ready features (signing, checkpointing, retries)</li> </ul> <p>Ready to start? Head to the Quickstart or First Experiment guide!</p>"},{"location":"api-reference/","title":"API Reference","text":"<p>Complete API documentation for Elspeth's core modules, plugins, and interfaces.</p> <p>Auto-Generated Documentation</p> <p>API documentation is automatically generated from docstrings in the source code using mkdocstrings. All examples shown are extracted from the actual codebase.</p>"},{"location":"api-reference/#overview","title":"Overview","text":"<p>Elspeth's API is organized into functional modules:</p> <pre><code>Core                    \u2192 BasePlugin, ClassifiedDataFrame, SecurityLevel\nRegistries              \u2192 Plugin registration and factory patterns\nPlugins                 \u2192 Datasources, Transforms (LLMs), Sinks\nPipeline                \u2192 Artifact pipeline, chaining, execution\nSecurity                \u2192 Security validation, signing, PII detection\n</code></pre>"},{"location":"api-reference/#quick-navigation","title":"Quick Navigation","text":""},{"location":"api-reference/#core-abstractions","title":"Core Abstractions","text":"<p>BasePlugin Abstract base class for all plugins with security enforcement</p> <p>ClassifiedDataFrame DataFrame wrapper with immutable classification metadata</p> <p>SecurityLevel Enumeration of security clearance levels (UNOFFICIAL \u2192 SECRET)</p>"},{"location":"api-reference/#plugin-development","title":"Plugin Development","text":"<p>Plugin Registry Factory pattern for plugin registration and instantiation</p> <p>Datasources Data loading plugins (CSV, Azure Blob, etc.)</p> <p>Transforms LLM clients and middleware</p> <p>Sinks Output plugins (CSV, Excel, signed artifacts, etc.)</p>"},{"location":"api-reference/#pipeline-execution","title":"Pipeline Execution","text":"<p>Artifact Pipeline Dependency-ordered sink execution with security enforcement</p>"},{"location":"api-reference/#key-concepts","title":"Key Concepts","text":""},{"location":"api-reference/#plugin-inheritance","title":"Plugin Inheritance","text":"<p>All plugins must inherit from <code>BasePlugin</code>:</p> <pre><code>from elspeth.core.base.plugin import BasePlugin\nfrom elspeth.core.base.types import SecurityLevel\n\nclass MyDatasource(BasePlugin):\n    def __init__(self, *, security_level: SecurityLevel, path: str):\n        super().__init__(security_level=security_level)\n        self.path = path\n</code></pre> <p>See BasePlugin for complete documentation.</p>"},{"location":"api-reference/#security-enforcement","title":"Security Enforcement","text":"<p>All data is wrapped in <code>ClassifiedDataFrame</code> with immutable classification:</p> <pre><code>from elspeth.core.security.classified_data import ClassifiedDataFrame\n\n# Created by datasource (trusted source)\nframe = ClassifiedDataFrame.create_from_datasource(\n    data, SecurityLevel.OFFICIAL\n)\n\n# Uplifted by plugin (automatic max operation)\nresult = frame.with_uplifted_classification(plugin.get_security_level())\n</code></pre> <p>See ClassifiedDataFrame for complete documentation.</p>"},{"location":"api-reference/#plugin-registration","title":"Plugin Registration","text":"<p>Plugins are registered via factory pattern:</p> <pre><code>from elspeth.core.registries.base import BasePluginRegistry\n\nregistry = BasePluginRegistry[MyPluginType]()\n\nregistry.register(\n    \"my_plugin\",\n    factory=my_plugin_factory,\n    schema=my_plugin_schema\n)\n\nplugin = registry.create(\"my_plugin\", options, context)\n</code></pre> <p>See Plugin Registry for complete documentation.</p>"},{"location":"api-reference/#architecture-decisions","title":"Architecture Decisions","text":"<p>API design is guided by ADRs (Architecture Decision Records):</p> ADR Topic Impact on API ADR-001 Design Philosophy Security-first priority hierarchy ADR-002 Multi-Level Security ClassifiedDataFrame immutability, validation ADR-004 Mandatory BasePlugin Inheritance All plugins inherit BasePlugin (nominal typing) ADR-005 Frozen Plugin Protection <code>allow_downgrade</code> parameter <p>See Architecture for complete ADR catalog.</p>"},{"location":"api-reference/#conventions","title":"Conventions","text":""},{"location":"api-reference/#docstring-style","title":"Docstring Style","text":"<p>All modules use Google-style docstrings:</p> <pre><code>def process_data(data: pd.DataFrame, threshold: float) -&gt; pd.DataFrame:\n    \"\"\"Process dataframe with threshold filtering.\n\n    Filters rows where score column is above the threshold and\n    applies standardization to numeric columns.\n\n    Args:\n        data: Input dataframe with 'score' column\n        threshold: Minimum score value (0.0 to 1.0)\n\n    Returns:\n        Filtered and standardized dataframe\n\n    Raises:\n        ValueError: If threshold is outside [0.0, 1.0] range\n        KeyError: If 'score' column missing\n\n    Example:\n        &gt;&gt;&gt; df = pd.DataFrame({'score': [0.3, 0.7, 0.9]})\n        &gt;&gt;&gt; result = process_data(df, threshold=0.5)\n        &gt;&gt;&gt; len(result)\n        2\n    \"\"\"\n</code></pre>"},{"location":"api-reference/#type-annotations","title":"Type Annotations","text":"<p>All public APIs use type annotations:</p> <pre><code>from typing import Optional\nfrom elspeth.core.base.types import SecurityLevel\n\ndef validate_level(\n    level: SecurityLevel,\n    *,\n    allow_unofficial: bool = False\n) -&gt; Optional[str]:\n    \"\"\"Validate security level meets policy requirements.\"\"\"\n</code></pre>"},{"location":"api-reference/#error-handling","title":"Error Handling","text":"<p>Security-critical errors raise specific exceptions:</p> <pre><code>from elspeth.core.validation.base import (\n    SecurityValidationError,  # Security policy violations\n    ConfigurationError,       # Invalid configuration\n)\n</code></pre> <p>See individual module documentation for exception hierarchies.</p>"},{"location":"api-reference/#usage-examples","title":"Usage Examples","text":""},{"location":"api-reference/#creating-a-custom-datasource","title":"Creating a Custom Datasource","text":"<pre><code>from elspeth.core.base.plugin import BasePlugin\nfrom elspeth.core.base.types import SecurityLevel\nfrom elspeth.core.security.classified_data import ClassifiedDataFrame\nimport pandas as pd\n\nclass CustomDatasource(BasePlugin):\n    \"\"\"Load data from custom source.\"\"\"\n\n    def __init__(self, *, security_level: SecurityLevel, source_path: str):\n        super().__init__(security_level=security_level)\n        self.source_path = source_path\n\n    def load_data(self) -&gt; ClassifiedDataFrame:\n        \"\"\"Load data from source.\n\n        Returns:\n            ClassifiedDataFrame with source data\n\n        Raises:\n            FileNotFoundError: If source_path doesn't exist\n        \"\"\"\n        data = pd.read_csv(self.source_path)\n        return ClassifiedDataFrame.create_from_datasource(\n            data, self.get_security_level()\n        )\n</code></pre>"},{"location":"api-reference/#creating-a-custom-sink","title":"Creating a Custom Sink","text":"<pre><code>from elspeth.core.base.plugin import BasePlugin\nfrom elspeth.core.security.classified_data import ClassifiedDataFrame\n\nclass CustomSink(BasePlugin):\n    \"\"\"Write data to custom destination.\"\"\"\n\n    def __init__(self, *, security_level: SecurityLevel, output_path: str):\n        super().__init__(security_level=security_level)\n        self.output_path = output_path\n\n    def write(self, frame: ClassifiedDataFrame, metadata: dict) -&gt; None:\n        \"\"\"Write classified dataframe to destination.\n\n        Args:\n            frame: Data to write\n            metadata: Experiment metadata\n\n        Raises:\n            PermissionError: If insufficient write permissions\n        \"\"\"\n        # Validate security level\n        self.validate_can_operate_at_level(frame.classification)\n\n        # Write data\n        frame.data.to_csv(self.output_path, index=False)\n</code></pre>"},{"location":"api-reference/#further-reading","title":"Further Reading","text":"<ul> <li>Plugin Catalogue - User-facing plugin documentation</li> <li>Security Model - Understanding Bell-LaPadula MLS</li> <li>Configuration - YAML configuration reference</li> <li>Architecture - System design and ADRs</li> </ul> <p>Contributing</p> <p>When adding new APIs, ensure:</p> <ul> <li>\u2705 Google-style docstrings with examples</li> <li>\u2705 Type annotations on all public methods</li> <li>\u2705 Security considerations documented</li> <li>\u2705 ADR cross-references where applicable</li> <li>\u2705 Unit tests with \u226580% coverage</li> </ul>"},{"location":"api-reference/core/base-plugin/","title":"BasePlugin","text":"<p>Abstract base class for all Elspeth plugins with security enforcement.</p> <p>ADR-004: Mandatory BasePlugin Inheritance</p> <p>All plugins must explicitly inherit from <code>BasePlugin</code>. This is not a Protocol (structural typing) but an ABC (nominal typing) to prevent security bypass attacks.</p>"},{"location":"api-reference/core/base-plugin/#overview","title":"Overview","text":"<p><code>BasePlugin</code> provides security bones - concrete, non-overridable methods that implement security-critical invariants. Plugins inherit security enforcement without implementing it.</p> <p>Key Design Principle: Security methods are <code>@final</code> and cannot be overridden by subclasses. This ensures consistent security enforcement across all plugins.</p>"},{"location":"api-reference/core/base-plugin/#class-documentation","title":"Class Documentation","text":""},{"location":"api-reference/core/base-plugin/#elspeth.core.base.plugin.BasePlugin","title":"BasePlugin","text":"<pre><code>BasePlugin(*, security_level: SecurityLevel, allow_downgrade: bool, **kwargs: object)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract base class for all Elspeth plugins with concrete security enforcement.</p> <p>This ABC provides \"security bones\" - concrete, non-overridable methods that implement security-critical invariants. Plugins inherit security enforcement without implementing it.</p> <p>CRITICAL DESIGN: This is an ABC (not a Protocol) to enforce nominal typing. Plugins MUST explicitly declare inheritance:</p> <pre><code>class MyPlugin(BasePlugin):  # \u2190 Explicit inheritance required\n    def __init__(self, *, security_level: SecurityLevel, ...):\n        super().__init__(security_level=security_level)\n</code></pre> <p>Why ABC Over Protocol: - isinstance(plugin, BasePlugin) requires explicit inheritance (nominal typing) - Protocol would accept any duck-typed class with matching methods (structural typing) - ADR-002 validation requires nominal typing to prevent bypass attacks</p> <p>Sealed Methods (cannot be overridden by subclasses): - get_security_level() - Returns plugin's security clearance - validate_can_operate_at_level() - Enforces security level constraints</p> <p>Constructor Contract: - security_level must be provided as keyword-only argument - security_level cannot be None - Subclasses MUST call super().init(security_level=...)</p> <p>Runtime Enforcement: init_subclass hook prevents subclasses from overriding sealed methods. Attempting to override raises TypeError at class definition time.</p> Example <p>class MyDatasource(BasePlugin): ...     def init(self, *, security_level: SecurityLevel): ...         super().init(security_level=security_level) ... ds = MyDatasource(security_level=SecurityLevel.SECRET) ds.get_security_level() SecurityLevel.SECRET ds.validate_can_operate_at_level(SecurityLevel.SECRET)  # \u2705 OK (exact) ds.validate_can_operate_at_level(SecurityLevel.OFFICIAL)  # \u2705 OK (trusted downgrade)</p> <p>Example (Frozen Plugin - ADR-005):     &gt;&gt;&gt; class FrozenDatasource(BasePlugin):     ...     def init(self):     ...         super().init(security_level=SecurityLevel.SECRET, allow_downgrade=False)     ...     &gt;&gt;&gt; frozen = FrozenDatasource()     &gt;&gt;&gt; frozen.get_security_level()     SecurityLevel.SECRET     &gt;&gt;&gt; frozen.allow_downgrade     False     &gt;&gt;&gt; frozen.validate_can_operate_at_level(SecurityLevel.SECRET)  # \u2705 OK (exact)     &gt;&gt;&gt; frozen.validate_can_operate_at_level(SecurityLevel.OFFICIAL)  # \u274c Raises (frozen)</p> <p>Attributes:</p> Name Type Description <code>_security_level</code> <code>SecurityLevel</code> <p>Plugin's security clearance (private storage).</p> <code>_allow_downgrade</code> <code>bool</code> <p>Whether plugin can operate at lower levels (private storage).</p> Properties <p>security_level (SecurityLevel): Read-only access to security clearance. allow_downgrade (bool): Read-only access to downgrade permission.</p> <p>Methods:</p> Name Description <code>get_security_level</code> <p>Returns plugin's declared clearance.</p> <code>get_effective_level</code> <p>Returns pipeline operating level (effective level).</p> <code>validate_can_operate_at_level</code> <p>Validates operating level.</p> <p>Initialize BasePlugin with mandatory security level and downgrade policy.</p> <p>Parameters:</p> Name Type Description Default <code>security_level</code> <code>SecurityLevel</code> <p>Plugin's security clearance (MANDATORY keyword-only argument).</p> required <code>allow_downgrade</code> <code>bool</code> <p>Whether plugin can operate at lower pipeline levels (MANDATORY - no default). - True: Trusted downgrade - plugin can filter/downgrade to lower levels - False: Frozen plugin - must operate at exact declared level (ADR-005) - \u26a0\ufe0f NO DEFAULT: Explicit security choice required (security-first principle)</p> required <code>**kwargs</code> <code>object</code> <p>Additional keyword arguments passed to super().init().</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If security_level is None.</p> <code>TypeError</code> <p>If allow_downgrade not provided (no default - explicit choice required).</p> Design Notes <ul> <li>security_level is keyword-only (forces explicit declaration)</li> <li>allow_downgrade is keyword-only with NO DEFAULT (security-first: explicit &gt; implicit)</li> <li>\u26a0\ufe0f BREAKING CHANGE from previous version that defaulted to True</li> <li>Stored in private fields (discourages direct access)</li> <li>Public access via properties (read-only)</li> <li>**kwargs allows cooperative multiple inheritance</li> </ul> Example Source code in <code>elspeth/core/base/plugin.py</code> <pre><code>def __init__(\n    self,\n    *,\n    security_level: SecurityLevel,\n    allow_downgrade: bool,\n    **kwargs: object\n) -&gt; None:\n    \"\"\"Initialize BasePlugin with mandatory security level and downgrade policy.\n\n    Args:\n        security_level: Plugin's security clearance (MANDATORY keyword-only argument).\n        allow_downgrade: Whether plugin can operate at lower pipeline levels (MANDATORY - no default).\n            - True: Trusted downgrade - plugin can filter/downgrade to lower levels\n            - False: Frozen plugin - must operate at exact declared level (ADR-005)\n            - \u26a0\ufe0f NO DEFAULT: Explicit security choice required (security-first principle)\n        **kwargs: Additional keyword arguments passed to super().__init__().\n\n    Raises:\n        ValueError: If security_level is None.\n        TypeError: If allow_downgrade not provided (no default - explicit choice required).\n\n    Design Notes:\n        - security_level is keyword-only (forces explicit declaration)\n        - allow_downgrade is keyword-only with NO DEFAULT (security-first: explicit &gt; implicit)\n        - \u26a0\ufe0f BREAKING CHANGE from previous version that defaulted to True\n        - Stored in private fields (discourages direct access)\n        - Public access via properties (read-only)\n        - **kwargs allows cooperative multiple inheritance\n\n    Example:\n        &gt;&gt;&gt; # Trusted downgrade (EXPLICIT - required)\n        &gt;&gt;&gt; plugin = MyPlugin(security_level=SecurityLevel.SECRET, allow_downgrade=True)\n        &gt;&gt;&gt; plugin.allow_downgrade\n        True\n\n        &gt;&gt;&gt; # Frozen: Strict level enforcement (ADR-005)\n        &gt;&gt;&gt; frozen = MyPlugin(security_level=SecurityLevel.SECRET, allow_downgrade=False)\n        &gt;&gt;&gt; frozen.allow_downgrade\n        False\n\n        &gt;&gt;&gt; # ERROR: Missing allow_downgrade (no default)\n        &gt;&gt;&gt; bad = MyPlugin(security_level=SecurityLevel.SECRET)  # TypeError!\n    \"\"\"\n    if security_level is None:\n        raise ValueError(f\"{type(self).__name__}: security_level cannot be None (ADR-004 requirement)\")\n\n    self._security_level = security_level\n    self._allow_downgrade = allow_downgrade\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"api-reference/core/base-plugin/#elspeth.core.base.plugin.BasePlugin--trusted-downgrade-explicit-required","title":"Trusted downgrade (EXPLICIT - required)","text":"<p>plugin = MyPlugin(security_level=SecurityLevel.SECRET, allow_downgrade=True) plugin.allow_downgrade True</p>"},{"location":"api-reference/core/base-plugin/#elspeth.core.base.plugin.BasePlugin--frozen-strict-level-enforcement-adr-005","title":"Frozen: Strict level enforcement (ADR-005)","text":"<p>frozen = MyPlugin(security_level=SecurityLevel.SECRET, allow_downgrade=False) frozen.allow_downgrade False</p>"},{"location":"api-reference/core/base-plugin/#elspeth.core.base.plugin.BasePlugin--error-missing-allow_downgrade-no-default","title":"ERROR: Missing allow_downgrade (no default)","text":"<p>bad = MyPlugin(security_level=SecurityLevel.SECRET)  # TypeError!</p>"},{"location":"api-reference/core/base-plugin/#elspeth.core.base.plugin.BasePlugin-attributes","title":"Attributes","text":""},{"location":"api-reference/core/base-plugin/#elspeth.core.base.plugin.BasePlugin.security_level","title":"security_level  <code>property</code>","text":"<pre><code>security_level: SecurityLevel\n</code></pre> <p>Read-only property for security level (convenience accessor).</p> <p>This property allows convenient access to the security level in factory methods and other contexts where self.security_level is more readable than self.get_security_level().</p> <p>Returns:</p> Name Type Description <code>SecurityLevel</code> <code>SecurityLevel</code> <p>Plugin's security clearance.</p> Example <p>plugin = MyPlugin(security_level=SecurityLevel.SECRET, allow_downgrade=True) plugin.security_level  # \u2705 Convenient access SecurityLevel.SECRET plugin.security_level = SecurityLevel.UNOFFICIAL  # \u274c AttributeError (read-only)</p>"},{"location":"api-reference/core/base-plugin/#elspeth.core.base.plugin.BasePlugin.allow_downgrade","title":"allow_downgrade  <code>property</code>","text":"<pre><code>allow_downgrade: bool\n</code></pre> <p>Read-only property for downgrade permission (ADR-005).</p> <p>This property indicates whether the plugin can operate at pipeline levels LOWER than its declared security clearance.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>Whether plugin can operate at lower pipeline levels. - True: Trusted downgrade - can filter/downgrade data to lower levels - False: Frozen plugin - must operate at exact declared level only</p> Design Notes <ul> <li>MANDATORY parameter (no default - explicit security choice required per ADR-005)</li> <li>Set to True for trusted downgrade (standard behavior per ADR-002)</li> <li>Set to False for frozen plugins (strict enforcement per ADR-005)</li> <li>Read-only to prevent runtime modification (prevents TOCTOU attacks)</li> </ul> Example"},{"location":"api-reference/core/base-plugin/#elspeth.core.base.plugin.BasePlugin.allow_downgrade--trusted-downgrade-explicit-most-common","title":"Trusted downgrade (explicit - most common)","text":"<p>plugin = MyPlugin(security_level=SecurityLevel.SECRET, allow_downgrade=True) plugin.allow_downgrade True</p>"},{"location":"api-reference/core/base-plugin/#elspeth.core.base.plugin.BasePlugin.allow_downgrade--frozen-plugin-explicit-strict-enforcement","title":"Frozen plugin (explicit - strict enforcement)","text":"<p>frozen = MyPlugin(security_level=SecurityLevel.SECRET, allow_downgrade=False) frozen.allow_downgrade False</p>"},{"location":"api-reference/core/base-plugin/#elspeth.core.base.plugin.BasePlugin-functions","title":"Functions","text":""},{"location":"api-reference/core/base-plugin/#elspeth.core.base.plugin.BasePlugin.__init_subclass__","title":"__init_subclass__","text":"<pre><code>__init_subclass__(**kwargs: object) -&gt; None\n</code></pre> <p>Runtime enforcement: prevent subclasses from overriding security methods.</p> <p>This hook runs when a subclass of BasePlugin is DEFINED (not instantiated). It inspects the subclass's dict to detect if any sealed methods were overridden.</p> <p>If a sealed method is found in the subclass, raises TypeError immediately, preventing the class from being defined.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>object</code> <p>Standard init_subclass keyword arguments.</p> <code>{}</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If subclass attempts to override a sealed security method.</p> Note <p>This is runtime enforcement complementing @final (static type checking). MyPy catches overrides at type-check time, this catches them at runtime.</p> Source code in <code>elspeth/core/base/plugin.py</code> <pre><code>def __init_subclass__(cls, **kwargs: object) -&gt; None:\n    \"\"\"Runtime enforcement: prevent subclasses from overriding security methods.\n\n    This hook runs when a subclass of BasePlugin is DEFINED (not instantiated).\n    It inspects the subclass's __dict__ to detect if any sealed methods were overridden.\n\n    If a sealed method is found in the subclass, raises TypeError immediately,\n    preventing the class from being defined.\n\n    Args:\n        **kwargs: Standard __init_subclass__ keyword arguments.\n\n    Raises:\n        TypeError: If subclass attempts to override a sealed security method.\n\n    Note:\n        This is runtime enforcement complementing @final (static type checking).\n        MyPy catches overrides at type-check time, this catches them at runtime.\n    \"\"\"\n    super().__init_subclass__(**kwargs)\n\n    # Sealed methods that cannot be overridden (ADR-004 security invariants)\n    sealed_methods = (\"get_security_level\", \"get_effective_level\", \"validate_can_operate_at_level\")\n\n    for method_name in sealed_methods:\n        if method_name in cls.__dict__:\n            raise TypeError(\n                f\"{cls.__name__} may not override {method_name} (ADR-004 security invariant). \"\n                f\"Security enforcement is provided by BasePlugin and cannot be customized. \"\n                f\"If you need custom security logic, please consult the architecture team.\"\n            )\n</code></pre>"},{"location":"api-reference/core/base-plugin/#elspeth.core.base.plugin.BasePlugin.get_security_level","title":"get_security_level","text":"<pre><code>get_security_level() -&gt; SecurityLevel\n</code></pre> <p>Return the plugin's declared security level (SEALED - cannot be overridden).</p> <p>This method is marked @final to prevent subclasses from overriding it. MyPy will flag any override attempts at type-check time. Additionally, init_subclass will raise TypeError at class definition time.</p> <p>Returns:</p> Name Type Description <code>SecurityLevel</code> <code>SecurityLevel</code> <p>Plugin's security clearance.</p> Design Notes <ul> <li>@final provides static enforcement (MyPy catches at type-check time)</li> <li>init_subclass provides runtime enforcement (raises TypeError at class definition)</li> <li>Dual enforcement ensures security method cannot be tampered with</li> </ul> Example <p>plugin = MyPlugin(security_level=SecurityLevel.SECRET) plugin.get_security_level() SecurityLevel.SECRET</p> Source code in <code>elspeth/core/base/plugin.py</code> <pre><code>@final\ndef get_security_level(self) -&gt; SecurityLevel:\n    \"\"\"Return the plugin's declared security level (SEALED - cannot be overridden).\n\n    This method is marked @final to prevent subclasses from overriding it.\n    MyPy will flag any override attempts at type-check time.\n    Additionally, __init_subclass__ will raise TypeError at class definition time.\n\n    Returns:\n        SecurityLevel: Plugin's security clearance.\n\n    Design Notes:\n        - @final provides static enforcement (MyPy catches at type-check time)\n        - __init_subclass__ provides runtime enforcement (raises TypeError at class definition)\n        - Dual enforcement ensures security method cannot be tampered with\n\n    Example:\n        &gt;&gt;&gt; plugin = MyPlugin(security_level=SecurityLevel.SECRET)\n        &gt;&gt;&gt; plugin.get_security_level()\n        SecurityLevel.SECRET\n    \"\"\"\n    return self._security_level\n</code></pre>"},{"location":"api-reference/core/base-plugin/#elspeth.core.base.plugin.BasePlugin.validate_can_operate_at_level","title":"validate_can_operate_at_level","text":"<pre><code>validate_can_operate_at_level(operating_level: SecurityLevel) -&gt; None\n</code></pre> <p>Validate that plugin can operate at the given security level (SEALED).</p> <p>Bell-LaPadula Multi-Level Security (MLS) enforcement with optional frozen behavior: - Plugin with HIGHER clearance can operate at LOWER level (trusted downgrade, ADR-002) - Plugin with LOWER clearance CANNOT operate at HIGHER level (insufficient clearance) - Frozen plugin (allow_downgrade=False) CANNOT operate at LOWER level (strict enforcement, ADR-005)</p> <p>Parameters:</p> Name Type Description Default <code>operating_level</code> <code>SecurityLevel</code> <p>Security level of the pipeline/suite.</p> required <p>Raises:</p> Type Description <code>SecurityValidationError</code> <p>If insufficient clearance OR frozen downgrade violation.</p> Validation Logic <ol> <li>Check insufficient clearance: operating_level &gt; security_level \u2192 REJECT (always)</li> <li>Check frozen downgrade: operating_level &lt; security_level AND not allow_downgrade \u2192 REJECT</li> <li>Otherwise: ALLOW (exact match or trusted downgrade)</li> </ol> Design Notes <ul> <li>Check 1 implements Bell-LaPadula \"no read up\" rule</li> <li>Check 2 implements ADR-005 frozen plugin capability</li> <li>allow_downgrade parameter is MANDATORY (no default, explicit security choice)</li> <li>Fail-fast: Validation happens BEFORE any data processing</li> </ul> <p>Example (Trusted Downgrade - allow_downgrade=True):     &gt;&gt;&gt; plugin = MyPlugin(security_level=SecurityLevel.PROTECTED, allow_downgrade=True)     &gt;&gt;&gt; plugin.validate_can_operate_at_level(SecurityLevel.OFFICIAL)  # \u2705 OK (trusted downgrade)     &gt;&gt;&gt; plugin.validate_can_operate_at_level(SecurityLevel.PROTECTED)  # \u2705 OK (exact match)     &gt;&gt;&gt; plugin.validate_can_operate_at_level(SecurityLevel.SECRET)  # \u274c Raises (insufficient clearance)</p> <p>Example (Frozen Plugin - allow_downgrade=False):     &gt;&gt;&gt; frozen = MyPlugin(security_level=SecurityLevel.PROTECTED, allow_downgrade=False)     &gt;&gt;&gt; frozen.validate_can_operate_at_level(SecurityLevel.PROTECTED)  # \u2705 OK (exact match only)     &gt;&gt;&gt; frozen.validate_can_operate_at_level(SecurityLevel.OFFICIAL)  # \u274c Raises (frozen, no downgrade)     &gt;&gt;&gt; frozen.validate_can_operate_at_level(SecurityLevel.SECRET)  # \u274c Raises (insufficient clearance)</p> Source code in <code>elspeth/core/base/plugin.py</code> <pre><code>@final\ndef validate_can_operate_at_level(self, operating_level: SecurityLevel) -&gt; None:\n    \"\"\"Validate that plugin can operate at the given security level (SEALED).\n\n    Bell-LaPadula Multi-Level Security (MLS) enforcement with optional frozen behavior:\n    - Plugin with HIGHER clearance can operate at LOWER level (trusted downgrade, ADR-002)\n    - Plugin with LOWER clearance CANNOT operate at HIGHER level (insufficient clearance)\n    - Frozen plugin (allow_downgrade=False) CANNOT operate at LOWER level (strict enforcement, ADR-005)\n\n    Args:\n        operating_level: Security level of the pipeline/suite.\n\n    Raises:\n        SecurityValidationError: If insufficient clearance OR frozen downgrade violation.\n\n    Validation Logic:\n        1. Check insufficient clearance: operating_level &gt; security_level \u2192 REJECT (always)\n        2. Check frozen downgrade: operating_level &lt; security_level AND not allow_downgrade \u2192 REJECT\n        3. Otherwise: ALLOW (exact match or trusted downgrade)\n\n    Design Notes:\n        - Check 1 implements Bell-LaPadula \"no read up\" rule\n        - Check 2 implements ADR-005 frozen plugin capability\n        - allow_downgrade parameter is MANDATORY (no default, explicit security choice)\n        - Fail-fast: Validation happens BEFORE any data processing\n\n    Example (Trusted Downgrade - allow_downgrade=True):\n        &gt;&gt;&gt; plugin = MyPlugin(security_level=SecurityLevel.PROTECTED, allow_downgrade=True)\n        &gt;&gt;&gt; plugin.validate_can_operate_at_level(SecurityLevel.OFFICIAL)  # \u2705 OK (trusted downgrade)\n        &gt;&gt;&gt; plugin.validate_can_operate_at_level(SecurityLevel.PROTECTED)  # \u2705 OK (exact match)\n        &gt;&gt;&gt; plugin.validate_can_operate_at_level(SecurityLevel.SECRET)  # \u274c Raises (insufficient clearance)\n\n    Example (Frozen Plugin - allow_downgrade=False):\n        &gt;&gt;&gt; frozen = MyPlugin(security_level=SecurityLevel.PROTECTED, allow_downgrade=False)\n        &gt;&gt;&gt; frozen.validate_can_operate_at_level(SecurityLevel.PROTECTED)  # \u2705 OK (exact match only)\n        &gt;&gt;&gt; frozen.validate_can_operate_at_level(SecurityLevel.OFFICIAL)  # \u274c Raises (frozen, no downgrade)\n        &gt;&gt;&gt; frozen.validate_can_operate_at_level(SecurityLevel.SECRET)  # \u274c Raises (insufficient clearance)\n    \"\"\"\n    # Check 1: Insufficient clearance (Bell-LaPadula \"no read up\")\n    if operating_level &gt; self._security_level:\n        raise SecurityValidationError(\n            f\"{type(self).__name__} has clearance {self._security_level.name}, \"\n            f\"but pipeline requires {operating_level.name}. \"\n            f\"Insufficient clearance for higher classification (Bell-LaPadula MLS violation).\"\n        )\n\n    # Check 2: Frozen plugin downgrade rejection (ADR-005)\n    if operating_level &lt; self._security_level and not self._allow_downgrade:\n        raise SecurityValidationError(\n            f\"{type(self).__name__} is frozen at {self._security_level.name} \"\n            f\"(allow_downgrade=False). Cannot operate at lower level {operating_level.name}. \"\n            f\"This plugin requires exact level matching and does not support trusted downgrade.\"\n        )\n</code></pre>"},{"location":"api-reference/core/base-plugin/#usage-examples","title":"Usage Examples","text":""},{"location":"api-reference/core/base-plugin/#basic-plugin","title":"Basic Plugin","text":"<pre><code>from elspeth.core.base.plugin import BasePlugin\nfrom elspeth.core.base.types import SecurityLevel\n\nclass MyDatasource(BasePlugin):\n    \"\"\"Simple datasource plugin.\"\"\"\n\n    def __init__(self, *, security_level: SecurityLevel, path: str):\n        super().__init__(security_level=security_level)\n        self.path = path\n\n    def load_data(self):\n        \"\"\"Load data from path.\"\"\"\n        # Implementation...\n        pass\n\n# Usage\nds = MyDatasource(security_level=SecurityLevel.OFFICIAL, path=\"data.csv\")\nprint(ds.get_security_level())  # SecurityLevel.OFFICIAL\n</code></pre>"},{"location":"api-reference/core/base-plugin/#frozen-plugin-adr-005","title":"Frozen Plugin (ADR-005)","text":"<pre><code>class FrozenSecretDatasource(BasePlugin):\n    \"\"\"Datasource that refuses to operate below SECRET level.\"\"\"\n\n    def __init__(self, *, database_url: str):\n        super().__init__(\n            security_level=SecurityLevel.SECRET,\n            allow_downgrade=False  # \u2190 Frozen at SECRET only\n        )\n        self.database_url = database_url\n\n# Usage\nfrozen = FrozenSecretDatasource(database_url=\"postgresql://...\")\nprint(frozen.allow_downgrade)  # False\n\n# Can operate at SECRET level (exact match)\nfrozen.validate_can_operate_at_level(SecurityLevel.SECRET)  # \u2705 OK\n\n# Cannot operate at lower levels (frozen)\nfrozen.validate_can_operate_at_level(SecurityLevel.OFFICIAL)  # \u274c Raises SecurityValidationError\n</code></pre>"},{"location":"api-reference/core/base-plugin/#validation-example","title":"Validation Example","text":"<pre><code># Create plugin with SECRET clearance\nplugin = MyDatasource(security_level=SecurityLevel.SECRET, path=\"data.csv\")\n\n# Validate operating levels\nplugin.validate_can_operate_at_level(SecurityLevel.SECRET)      # \u2705 OK (exact match)\nplugin.validate_can_operate_at_level(SecurityLevel.OFFICIAL)    # \u2705 OK (trusted downgrade)\nplugin.validate_can_operate_at_level(SecurityLevel.UNOFFICIAL)  # \u2705 OK (trusted downgrade)\n\n# Create plugin with UNOFFICIAL clearance\nplugin_low = MyDatasource(security_level=SecurityLevel.UNOFFICIAL, path=\"public.csv\")\n\n# Cannot operate above clearance\nplugin_low.validate_can_operate_at_level(SecurityLevel.UNOFFICIAL)  # \u2705 OK (exact)\nplugin_low.validate_can_operate_at_level(SecurityLevel.SECRET)      # \u274c Raises (insufficient clearance)\n</code></pre>"},{"location":"api-reference/core/base-plugin/#security-enforcement","title":"Security Enforcement","text":""},{"location":"api-reference/core/base-plugin/#security-bones-design","title":"\"Security Bones\" Design","text":"<p>BasePlugin provides concrete (not abstract) security methods:</p> Method Purpose Overridable? <code>get_security_level()</code> Returns plugin's clearance \u274c No (@final) <code>validate_can_operate_at_level()</code> Validates operating level \u274c No (@final) <p>Why concrete methods?</p> <ol> <li>Consistency: All plugins use identical security logic</li> <li>Security: Can't accidentally break enforcement</li> <li>Simplicity: Plugins inherit security for free</li> <li>Maintainability: Security logic in one place</li> <li>Trust Boundary: Security enforcement isolated from plugin code</li> </ol>"},{"location":"api-reference/core/base-plugin/#runtime-enforcement","title":"Runtime Enforcement","text":"<p><code>__init_subclass__</code> hook prevents subclass override attempts:</p> <pre><code>class BadPlugin(BasePlugin):\n    def get_security_level(self):  # \u274c Attempt to override\n        return SecurityLevel.SECRET  # (Always return SECRET - bypass!)\n\n# Raises TypeError at class definition time:\n# TypeError: Subclass BadPlugin cannot override final method 'get_security_level'\n</code></pre>"},{"location":"api-reference/core/base-plugin/#constructor-contract","title":"Constructor Contract","text":""},{"location":"api-reference/core/base-plugin/#required-parameters","title":"Required Parameters","text":"<pre><code>def __init__(self, *, security_level: SecurityLevel, allow_downgrade: bool = True):\n    \"\"\"Initialize BasePlugin.\n\n    Args:\n        security_level: Plugin's security clearance (keyword-only, required)\n        allow_downgrade: Whether plugin can operate at lower levels (default: True)\n    \"\"\"\n</code></pre> <p>Rules:</p> <ul> <li>\u2705 <code>security_level</code> is keyword-only (must use <code>security_level=...</code>)</li> <li>\u2705 <code>security_level</code> is required (no default value)</li> <li>\u2705 Must call <code>super().__init__(security_level=...)</code></li> <li>\u2705 <code>allow_downgrade</code> is optional (defaults to <code>True</code>)</li> </ul>"},{"location":"api-reference/core/base-plugin/#invalid-constructors","title":"Invalid Constructors","text":"<pre><code># \u274c Missing security_level\nclass BadPlugin1(BasePlugin):\n    def __init__(self):\n        super().__init__()  # TypeError: missing required argument\n\n# \u274c Positional argument\nclass BadPlugin2(BasePlugin):\n    def __init__(self, level: SecurityLevel):\n        super().__init__(level)  # TypeError: takes keyword-only arguments\n\n# \u2705 Correct\nclass GoodPlugin(BasePlugin):\n    def __init__(self, *, security_level: SecurityLevel):\n        super().__init__(security_level=security_level)\n</code></pre>"},{"location":"api-reference/core/base-plugin/#properties","title":"Properties","text":""},{"location":"api-reference/core/base-plugin/#security_level","title":"<code>security_level</code>","text":"<p>Read-only property returning the plugin's security clearance.</p> <pre><code>plugin = MyDatasource(security_level=SecurityLevel.OFFICIAL, path=\"data.csv\")\nprint(plugin.security_level)  # SecurityLevel.OFFICIAL\n\n# Read-only (cannot modify)\nplugin.security_level = SecurityLevel.SECRET  # \u274c AttributeError\n</code></pre>"},{"location":"api-reference/core/base-plugin/#allow_downgrade","title":"<code>allow_downgrade</code>","text":"<p>Read-only property indicating whether plugin can operate at lower levels.</p> <pre><code># Standard plugin (can downgrade)\nplugin = MyDatasource(security_level=SecurityLevel.SECRET, path=\"data.csv\")\nprint(plugin.allow_downgrade)  # True\n\n# Frozen plugin (cannot downgrade)\nfrozen = FrozenSecretDatasource(database_url=\"...\")\nprint(frozen.allow_downgrade)  # False\n</code></pre>"},{"location":"api-reference/core/base-plugin/#related-documentation","title":"Related Documentation","text":"<ul> <li>ClassifiedDataFrame - Data container with classification</li> <li>SecurityLevel - Security clearance enumeration</li> <li>Security Model - Bell-LaPadula MLS explanation</li> <li>Plugin Development - Creating custom plugins</li> </ul>"},{"location":"api-reference/core/base-plugin/#adr-cross-references","title":"ADR Cross-References","text":"<ul> <li>ADR-002: Multi-Level Security Enforcement - Requires <code>isinstance(plugin, BasePlugin)</code> checks</li> <li>ADR-004: Mandatory BasePlugin Inheritance - This ABC enables ADR-002 validation</li> <li>ADR-005: Frozen Plugin Protection - <code>allow_downgrade=False</code> use cases</li> </ul>"},{"location":"api-reference/core/classified-dataframe/","title":"SecureDataFrame","text":"<p>DataFrame wrapper with immutable classification metadata implementing ADR-002 security enforcement.</p> <p>Security-Critical Component</p> <p><code>SecureDataFrame</code> enforces immutable classification and automatic uplifting. Classification can only increase (UNOFFICIAL \u2192 SECRET), never decrease. This prevents data laundering attacks.</p>"},{"location":"api-reference/core/classified-dataframe/#overview","title":"Overview","text":"<p><code>SecureDataFrame</code> wraps Pandas DataFrames with a security classification that:</p> <ul> <li>\u2705 Cannot be downgraded (classification only increases)</li> <li>\u2705 Is immutable (frozen dataclass)</li> <li>\u2705 Created only by datasources (constructor protection)</li> <li>\u2705 Uplifted automatically (max operation on classifications)</li> </ul> <p>Security Model: ADR-002 Trusted Container with constructor protection against classification laundering.</p>"},{"location":"api-reference/core/classified-dataframe/#class-documentation","title":"Class Documentation","text":""},{"location":"api-reference/core/classified-dataframe/#elspeth.core.security.secure_data.SecureDataFrame","title":"SecureDataFrame  <code>dataclass</code>","text":"<pre><code>SecureDataFrame(data: DataFrame, security_level: SecurityLevel)\n</code></pre> <p>DataFrame wrapper with immutable security level metadata.</p> <p>Represents data with a specific security level that cannot be downgraded. Supports automatic security level uplifting when data passes through higher-security components.</p> <p>Security Model (ADR-002-A Trusted Container):     - Security level is immutable (frozen dataclass)     - Only datasources can create instances (constructor protection)     - Plugins can only uplift, never relabel (prevents laundering)     - Uplifting creates new instance (original unchanged)     - Downgrading is impossible (max() operation prevents it)     - Access validation provides runtime failsafe</p> Creation Patterns <p>Datasource (trusted source):     &gt;&gt;&gt; frame = SecureDataFrame.create_from_datasource(     ...     data, SecurityLevel.OFFICIAL     ... )</p> <p>Plugin transformation (in-place mutation):     &gt;&gt;&gt; frame.data['processed'] = transform(frame.data['input'])     &gt;&gt;&gt; result = frame.with_uplifted_security_level(plugin.get_security_level())</p> <p>Plugin data generation (LLMs, aggregations):     &gt;&gt;&gt; new_df = llm.generate(...)     &gt;&gt;&gt; result = frame.with_new_data(new_df).with_uplifted_security_level(     ...     plugin.get_security_level()     ... )</p> <p>Anti-Pattern (BLOCKED):     &gt;&gt;&gt; SecureDataFrame(data, level)  # SecurityValidationError</p> ADR-002 Threat Prevention <ul> <li>T4 (Security Level Mislabeling): Constructor protection prevents laundering</li> <li>T3 (Runtime Bypass): validate_compatible_with() catches start-time validation bypass</li> </ul>"},{"location":"api-reference/core/classified-dataframe/#elspeth.core.security.secure_data.SecureDataFrame-attributes","title":"Attributes","text":""},{"location":"api-reference/core/classified-dataframe/#elspeth.core.security.secure_data.SecureDataFrame.data","title":"data  <code>instance-attribute</code>","text":"<pre><code>data: DataFrame\n</code></pre>"},{"location":"api-reference/core/classified-dataframe/#elspeth.core.security.secure_data.SecureDataFrame-functions","title":"Functions","text":""},{"location":"api-reference/core/classified-dataframe/#elspeth.core.security.secure_data.SecureDataFrame.create_from_datasource","title":"create_from_datasource  <code>classmethod</code>","text":"<pre><code>create_from_datasource(data: DataFrame, security_level: SecurityLevel) -&gt; SecureDataFrame\n</code></pre> <p>Create SecureDataFrame from datasource (trusted source only).</p> <p>This is the ONLY way to create a SecureDataFrame from scratch. Datasources are trusted to label data with correct security level.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Pandas DataFrame containing the data</p> required <code>security_level</code> <code>SecurityLevel</code> <p>Security level of the data</p> required <p>Returns:</p> Type Description <code>SecureDataFrame</code> <p>New SecureDataFrame with datasource-authorized creation</p> Security <ul> <li>This factory method sets _created_by_datasource=True</li> <li>This allows post_init validation to pass</li> <li>Only datasources should call this method (verified during certification)</li> </ul> Example Source code in <code>elspeth/core/security/secure_data.py</code> <pre><code>@classmethod\ndef create_from_datasource(\n    cls, data: pd.DataFrame, security_level: SecurityLevel\n) -&gt; \"SecureDataFrame\":\n    \"\"\"Create SecureDataFrame from datasource (trusted source only).\n\n    This is the ONLY way to create a SecureDataFrame from scratch.\n    Datasources are trusted to label data with correct security level.\n\n    Args:\n        data: Pandas DataFrame containing the data\n        security_level: Security level of the data\n\n    Returns:\n        New SecureDataFrame with datasource-authorized creation\n\n    Security:\n        - This factory method sets _created_by_datasource=True\n        - This allows __post_init__ validation to pass\n        - Only datasources should call this method (verified during certification)\n\n    Example:\n        &gt;&gt;&gt; # In datasource implementation\n        &gt;&gt;&gt; df = pd.DataFrame({\"data\": [1, 2, 3]})\n        &gt;&gt;&gt; frame = SecureDataFrame.create_from_datasource(\n        ...     df, SecurityLevel.OFFICIAL\n        ... )\n    \"\"\"\n    # Use __new__ to bypass __init__ and set fields manually\n    instance = cls.__new__(cls)\n    object.__setattr__(instance, \"data\", data)\n    object.__setattr__(instance, \"security_level\", security_level)\n    object.__setattr__(instance, \"_created_by_datasource\", True)\n    return instance\n</code></pre>"},{"location":"api-reference/core/classified-dataframe/#elspeth.core.security.secure_data.SecureDataFrame.create_from_datasource--in-datasource-implementation","title":"In datasource implementation","text":"<p>df = pd.DataFrame({\"data\": [1, 2, 3]}) frame = SecureDataFrame.create_from_datasource( ...     df, SecurityLevel.OFFICIAL ... )</p>"},{"location":"api-reference/core/classified-dataframe/#elspeth.core.security.secure_data.SecureDataFrame.with_new_data","title":"with_new_data","text":"<pre><code>with_new_data(new_data: DataFrame) -&gt; SecureDataFrame\n</code></pre> <p>Create frame with different data, preserving current security level.</p> <p>For plugins that generate entirely new DataFrames (LLMs, aggregations) that cannot mutate .data in-place due to schema changes.</p> <p>Parameters:</p> Name Type Description Default <code>new_data</code> <code>DataFrame</code> <p>New pandas DataFrame to wrap</p> required <p>Returns:</p> Type Description <code>SecureDataFrame</code> <p>New SecureDataFrame with new data but SAME security level</p> Security <ul> <li>Preserves current security level (cannot downgrade)</li> <li>Plugin must still call with_uplifted_security_level() afterwards</li> <li>Bypasses post_init validation (trusted internal method)</li> </ul> Example Source code in <code>elspeth/core/security/secure_data.py</code> <pre><code>def with_new_data(self, new_data: pd.DataFrame) -&gt; \"SecureDataFrame\":\n    \"\"\"Create frame with different data, preserving current security level.\n\n    For plugins that generate entirely new DataFrames (LLMs, aggregations)\n    that cannot mutate .data in-place due to schema changes.\n\n    Args:\n        new_data: New pandas DataFrame to wrap\n\n    Returns:\n        New SecureDataFrame with new data but SAME security level\n\n    Security:\n        - Preserves current security level (cannot downgrade)\n        - Plugin must still call with_uplifted_security_level() afterwards\n        - Bypasses __post_init__ validation (trusted internal method)\n\n    Example:\n        &gt;&gt;&gt; # LLM generates new DataFrame\n        &gt;&gt;&gt; input_frame = SecureDataFrame.create_from_datasource(\n        ...     input_df, SecurityLevel.OFFICIAL\n        ... )\n        &gt;&gt;&gt; new_df = llm.generate(...)\n        &gt;&gt;&gt; output_frame = input_frame.with_new_data(new_df)\n        &gt;&gt;&gt; # Must still uplift to LLM's security level\n        &gt;&gt;&gt; final_frame = output_frame.with_uplifted_security_level(\n        ...     SecurityLevel.SECRET\n        ... )\n    \"\"\"\n    # Use __new__ to bypass __init__ (same pattern as create_from_datasource)\n    instance = SecureDataFrame.__new__(SecureDataFrame)\n    object.__setattr__(instance, \"data\", new_data)\n    object.__setattr__(instance, \"security_level\", self.security_level)\n    object.__setattr__(instance, \"_created_by_datasource\", False)\n    return instance\n</code></pre>"},{"location":"api-reference/core/classified-dataframe/#elspeth.core.security.secure_data.SecureDataFrame.with_new_data--llm-generates-new-dataframe","title":"LLM generates new DataFrame","text":"<p>input_frame = SecureDataFrame.create_from_datasource( ...     input_df, SecurityLevel.OFFICIAL ... ) new_df = llm.generate(...) output_frame = input_frame.with_new_data(new_df)</p>"},{"location":"api-reference/core/classified-dataframe/#elspeth.core.security.secure_data.SecureDataFrame.with_new_data--must-still-uplift-to-llms-security-level","title":"Must still uplift to LLM's security level","text":"<p>final_frame = output_frame.with_uplifted_security_level( ...     SecurityLevel.SECRET ... )</p>"},{"location":"api-reference/core/classified-dataframe/#usage-patterns","title":"Usage Patterns","text":""},{"location":"api-reference/core/classified-dataframe/#pattern-1-datasource-creation-trusted-source","title":"Pattern 1: Datasource Creation (Trusted Source)","text":"<p>Only datasources can create <code>SecureDataFrame</code> instances from scratch:</p> <pre><code>from elspeth.core.security.secure_data import SecureDataFrame\nfrom elspeth.core.base.types import SecurityLevel\nimport pandas as pd\n\n# Datasource creates classified frame\nraw_data = pd.DataFrame({'text': ['Hello', 'World'], 'score': [0.9, 0.8]})\nframe = SecureDataFrame.create_from_datasource(\n    raw_data, SecurityLevel.OFFICIAL\n)\n\nprint(frame.classification)  # SecurityLevel.OFFICIAL\nprint(len(frame.data))       # 2\n</code></pre>"},{"location":"api-reference/core/classified-dataframe/#pattern-2-plugin-transformation-in-place-mutation","title":"Pattern 2: Plugin Transformation (In-Place Mutation)","text":"<p>Plugins can mutate the underlying DataFrame, then uplift classification:</p> <pre><code># Plugin modifies data in-place\nframe.data['processed'] = frame.data['text'].str.upper()\n\n# Uplift classification to plugin's level\nplugin_level = SecurityLevel.PROTECTED\nresult = frame.with_uplifted_classification(plugin_level)\n\nprint(result.classification)  # SecurityLevel.PROTECTED (uplifted)\nprint('processed' in result.data.columns)  # True\n</code></pre>"},{"location":"api-reference/core/classified-dataframe/#pattern-3-plugin-data-generation-llms-aggregations","title":"Pattern 3: Plugin Data Generation (LLMs, Aggregations)","text":"<p>Plugins can generate new data and attach it with uplifted classification:</p> <pre><code># LLM generates new dataframe\nllm_output = pd.DataFrame({\n    'input': ['Hello', 'World'],\n    'llm_response': ['Processed: Hello', 'Processed: World']\n})\n\n# Attach new data with uplifted classification\nresult = frame.with_new_data(llm_output).with_uplifted_classification(\n    plugin.get_security_level()\n)\n\nprint(result.data.columns)    # ['input', 'llm_response']\nprint(result.classification)  # Uplifted to plugin level\n</code></pre>"},{"location":"api-reference/core/classified-dataframe/#anti-pattern-direct-construction-blocked","title":"Anti-Pattern: Direct Construction (BLOCKED)","text":"<p>Direct construction is prohibited to prevent classification laundering:</p> <pre><code>import pandas as pd\nfrom elspeth.core.security.secure_data import SecureDataFrame\nfrom elspeth.core.base.types import SecurityLevel\n\n# \u274c Direct construction raises SecurityValidationError\ndf = pd.DataFrame({'data': [1, 2, 3]})\nframe = SecureDataFrame(df, SecurityLevel.OFFICIAL)\n# Raises: SecurityValidationError: SecureDataFrame must be created via create_from_datasource\n</code></pre>"},{"location":"api-reference/core/classified-dataframe/#security-guarantees","title":"Security Guarantees","text":""},{"location":"api-reference/core/classified-dataframe/#immutability","title":"Immutability","text":"<p>Classification cannot be modified after creation (frozen dataclass):</p> <pre><code>frame = SecureDataFrame.create_from_datasource(\n    data, SecurityLevel.OFFICIAL\n)\n\n# \u274c Cannot modify classification\nframe.classification = SecurityLevel.SECRET  # AttributeError (frozen)\n</code></pre>"},{"location":"api-reference/core/classified-dataframe/#classification-uplifting-only","title":"Classification Uplifting Only","text":"<p>Classification can only increase, never decrease:</p> <pre><code># Start with OFFICIAL data\nframe = SecureDataFrame.create_from_datasource(\n    data, SecurityLevel.OFFICIAL\n)\n\n# \u2705 Can uplift to PROTECTED\nuplifted = frame.with_uplifted_classification(SecurityLevel.PROTECTED)\nprint(uplifted.classification)  # SecurityLevel.PROTECTED\n\n# \u2705 Can uplift again to SECRET\nsecret = uplifted.with_uplifted_classification(SecurityLevel.SECRET)\nprint(secret.classification)  # SecurityLevel.SECRET\n\n# \u2705 \"Downgrade\" attempt is actually a no-op (max operation)\nattempt_downgrade = secret.with_uplifted_classification(SecurityLevel.OFFICIAL)\nprint(attempt_downgrade.classification)  # Still SecurityLevel.SECRET\n</code></pre> <p>Why max() operation?</p> <p>Uplifting uses <code>max(current, requested)</code>: - If requested &gt; current \u2192 uplift to requested - If requested &lt; current \u2192 no-op (stay at current) - This prevents accidental downgrading</p>"},{"location":"api-reference/core/classified-dataframe/#constructor-protection","title":"Constructor Protection","text":"<p>Only datasources can create instances:</p> <pre><code># \u2705 Datasource context (trusted)\nframe = SecureDataFrame.create_from_datasource(\n    data, SecurityLevel.OFFICIAL\n)\n\n# \u274c Plugin context (untrusted)\nframe = SecureDataFrame(data, SecurityLevel.OFFICIAL)\n# Raises: SecurityValidationError\n</code></pre> <p>Why this restriction?</p> <p>Prevents plugins from creating \"fresh\" frames with lower classifications, bypassing uplifting logic:</p> <pre><code># Attack scenario (prevented):\n# 1. Plugin receives SECRET data\n# 2. Plugin creates new frame with UNOFFICIAL classification\n# 3. SECRET data now appears as UNOFFICIAL (laundering attack)\n# \u2192 BLOCKED by constructor protection\n</code></pre>"},{"location":"api-reference/core/classified-dataframe/#access-validation","title":"Access Validation","text":""},{"location":"api-reference/core/classified-dataframe/#validate_access_by","title":"validate_access_by()","text":"<p>Runtime failsafe ensuring component has sufficient clearance:</p> <pre><code>from elspeth.core.base.plugin import BasePlugin\n\n# Create PROTECTED data\nframe = SecureDataFrame.create_from_datasource(\n    data, SecurityLevel.PROTECTED\n)\n\n# Plugin with PROTECTED clearance\nplugin_protected = MyPlugin(security_level=SecurityLevel.PROTECTED)\nframe.validate_access_by(plugin_protected)  # \u2705 OK\n\n# Plugin with UNOFFICIAL clearance\nplugin_low = MyPlugin(security_level=SecurityLevel.UNOFFICIAL)\nframe.validate_access_by(plugin_low)  # \u274c Raises SecurityValidationError\n</code></pre> <p>When to use: - Runtime checks in sinks before writing data - Validation before passing data to external systems - Defensive programming for security-critical paths</p>"},{"location":"api-reference/core/classified-dataframe/#common-operations","title":"Common Operations","text":""},{"location":"api-reference/core/classified-dataframe/#accessing-underlying-dataframe","title":"Accessing Underlying DataFrame","text":"<pre><code>frame = SecureDataFrame.create_from_datasource(\n    data, SecurityLevel.OFFICIAL\n)\n\n# Access underlying DataFrame\ndf = frame.data\nprint(type(df))  # pandas.DataFrame\n\n# Modify DataFrame in-place\nframe.data['new_column'] = frame.data['old_column'] * 2\n\n# Uplift after modifications\nresult = frame.with_uplifted_classification(plugin.get_security_level())\n</code></pre>"},{"location":"api-reference/core/classified-dataframe/#replacing-dataframe","title":"Replacing DataFrame","text":"<pre><code># Generate new DataFrame\nnew_df = pd.DataFrame({'result': [1, 2, 3]})\n\n# Replace data and uplift\nresult = frame.with_new_data(new_df).with_uplifted_classification(\n    SecurityLevel.PROTECTED\n)\n\nprint(result.data.columns)    # ['result']\nprint(result.classification)  # SecurityLevel.PROTECTED\n</code></pre>"},{"location":"api-reference/core/classified-dataframe/#chaining-operations","title":"Chaining Operations","text":"<pre><code>result = (\n    frame\n    .with_new_data(llm_output)\n    .with_uplifted_classification(SecurityLevel.PROTECTED)\n    .with_uplifted_classification(SecurityLevel.SECRET)  # Chain uplifts\n)\n\nprint(result.classification)  # SecurityLevel.SECRET\n</code></pre>"},{"location":"api-reference/core/classified-dataframe/#error-handling","title":"Error Handling","text":""},{"location":"api-reference/core/classified-dataframe/#securityvalidationerror","title":"SecurityValidationError","text":"<p>Raised when security constraints are violated:</p> <pre><code>from elspeth.core.validation.base import SecurityValidationError\n\ntry:\n    # Direct construction (blocked)\n    frame = SecureDataFrame(data, SecurityLevel.OFFICIAL)\nexcept SecurityValidationError as e:\n    print(f\"Security violation: {e}\")\n\ntry:\n    # Insufficient clearance\n    frame = SecureDataFrame.create_from_datasource(\n        data, SecurityLevel.SECRET\n    )\n    plugin = MyPlugin(security_level=SecurityLevel.UNOFFICIAL)\n    frame.validate_access_by(plugin)\nexcept SecurityValidationError as e:\n    print(f\"Access denied: {e}\")\n</code></pre>"},{"location":"api-reference/core/classified-dataframe/#adr-threat-prevention","title":"ADR Threat Prevention","text":"<p>SecureDataFrame prevents ADR-002 threat scenarios:</p> Threat Prevention Mechanism T3: Runtime Bypass <code>validate_access_by()</code> catches start-time validation bypass T4: Classification Mislabeling Constructor protection prevents laundering attacks Downgrade Attacks Immutability + max() operation prevents classification reduction"},{"location":"api-reference/core/classified-dataframe/#related-documentation","title":"Related Documentation","text":"<ul> <li>BasePlugin - Plugin base class with security enforcement</li> <li>SecurityLevel - Security clearance enumeration</li> <li>Security Model - Bell-LaPadula MLS explanation</li> </ul>"},{"location":"api-reference/core/classified-dataframe/#adr-cross-references","title":"ADR Cross-References","text":"<ul> <li>ADR-002: Multi-Level Security Enforcement - SecureDataFrame implements trusted container</li> <li>ADR-002a: SecureDataFrame Constructor - Constructor protection design</li> </ul>"},{"location":"api-reference/core/security-level/","title":"SecurityLevel","text":"<p>Enumeration of security clearance levels implementing Australian Government PSPF classifications.</p>"},{"location":"api-reference/core/security-level/#overview","title":"Overview","text":"<p><code>SecurityLevel</code> defines five hierarchical security clearances from lowest (UNOFFICIAL) to highest (SECRET):</p> <pre><code>UNOFFICIAL \u2192 OFFICIAL \u2192 OFFICIAL_SENSITIVE \u2192 PROTECTED \u2192 SECRET\n    (0)         (1)            (2)              (3)        (4)\n</code></pre> <p>Ordering: Security levels support comparison operations (<code>&lt;</code>, <code>&gt;</code>, <code>&lt;=</code>, <code>&gt;=</code>) based on integer values.</p>"},{"location":"api-reference/core/security-level/#class-documentation","title":"Class Documentation","text":""},{"location":"api-reference/core/security-level/#elspeth.core.base.types.SecurityLevel","title":"SecurityLevel","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Australian Government PSPF security classification levels.</p> <p>These levels form a strict hierarchy from least to most restrictive: UNOFFICIAL &lt; OFFICIAL &lt; OFFICIAL_SENSITIVE &lt; PROTECTED &lt; SECRET</p> <p>Security aggregation rule: MOST restrictive wins. Example: OFFICIAL + SECRET \u2192 SECRET</p> <p>Note: The string \"SECRET\" below is a classification level name, not a password.</p>"},{"location":"api-reference/core/security-level/#elspeth.core.base.types.SecurityLevel-attributes","title":"Attributes","text":""},{"location":"api-reference/core/security-level/#elspeth.core.base.types.SecurityLevel.UNOFFICIAL","title":"UNOFFICIAL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>UNOFFICIAL = 'UNOFFICIAL'\n</code></pre>"},{"location":"api-reference/core/security-level/#elspeth.core.base.types.SecurityLevel.OFFICIAL","title":"OFFICIAL  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>OFFICIAL = 'OFFICIAL'\n</code></pre>"},{"location":"api-reference/core/security-level/#elspeth.core.base.types.SecurityLevel.OFFICIAL_SENSITIVE","title":"OFFICIAL_SENSITIVE  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>OFFICIAL_SENSITIVE = 'OFFICIAL: SENSITIVE'\n</code></pre>"},{"location":"api-reference/core/security-level/#elspeth.core.base.types.SecurityLevel.PROTECTED","title":"PROTECTED  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>PROTECTED = 'PROTECTED'\n</code></pre>"},{"location":"api-reference/core/security-level/#elspeth.core.base.types.SecurityLevel.SECRET","title":"SECRET  <code>class-attribute</code> <code>instance-attribute</code>","text":"<pre><code>SECRET = 'SECRET'\n</code></pre>"},{"location":"api-reference/core/security-level/#elspeth.core.base.types.SecurityLevel-functions","title":"Functions","text":""},{"location":"api-reference/core/security-level/#elspeth.core.base.types.SecurityLevel.__lt__","title":"__lt__","text":"<pre><code>__lt__(other: str) -&gt; Any\n</code></pre> <p>Support comparison for hierarchy enforcement.</p> Source code in <code>elspeth/core/base/types.py</code> <pre><code>def __lt__(self, other: str) -&gt; Any:\n    \"\"\"Support comparison for hierarchy enforcement.\"\"\"\n    if not isinstance(other, SecurityLevel):\n        return NotImplemented\n    order = [\n        SecurityLevel.UNOFFICIAL,\n        SecurityLevel.OFFICIAL,\n        SecurityLevel.OFFICIAL_SENSITIVE,\n        SecurityLevel.PROTECTED,\n        SecurityLevel.SECRET,\n    ]\n    return order.index(self) &lt; order.index(other)\n</code></pre>"},{"location":"api-reference/core/security-level/#elspeth.core.base.types.SecurityLevel.__le__","title":"__le__","text":"<pre><code>__le__(other: str) -&gt; Any\n</code></pre> <p>Support comparison for hierarchy enforcement.</p> Source code in <code>elspeth/core/base/types.py</code> <pre><code>def __le__(self, other: str) -&gt; Any:\n    \"\"\"Support comparison for hierarchy enforcement.\"\"\"\n    return self == other or self &lt; other\n</code></pre>"},{"location":"api-reference/core/security-level/#elspeth.core.base.types.SecurityLevel.__gt__","title":"__gt__","text":"<pre><code>__gt__(other: str) -&gt; Any\n</code></pre> <p>Support comparison for hierarchy enforcement.</p> Source code in <code>elspeth/core/base/types.py</code> <pre><code>def __gt__(self, other: str) -&gt; Any:\n    \"\"\"Support comparison for hierarchy enforcement.\"\"\"\n    if not isinstance(other, SecurityLevel):\n        return NotImplemented\n    return other &lt; self\n</code></pre>"},{"location":"api-reference/core/security-level/#elspeth.core.base.types.SecurityLevel.__ge__","title":"__ge__","text":"<pre><code>__ge__(other: str) -&gt; Any\n</code></pre> <p>Support comparison for hierarchy enforcement.</p> Source code in <code>elspeth/core/base/types.py</code> <pre><code>def __ge__(self, other: str) -&gt; Any:\n    \"\"\"Support comparison for hierarchy enforcement.\"\"\"\n    return self == other or self &gt; other\n</code></pre>"},{"location":"api-reference/core/security-level/#security-levels","title":"Security Levels","text":""},{"location":"api-reference/core/security-level/#unofficial-level-0","title":"UNOFFICIAL (Level 0)","text":"<p>Description: Public information, no sensitivity</p> <p>Example Use Cases: - Marketing copy - Public datasets - Open-source documentation - Test data for development</p> <p>YAML Configuration: <pre><code>datasource:\n  security_level: UNOFFICIAL\n</code></pre></p>"},{"location":"api-reference/core/security-level/#official-level-1","title":"OFFICIAL (Level 1)","text":"<p>Description: Routine business data, limited distribution</p> <p>Example Use Cases: - Customer names - Product lists - Internal reports (non-sensitive) - Business correspondence</p> <p>YAML Configuration: <pre><code>datasource:\n  security_level: OFFICIAL\n</code></pre></p>"},{"location":"api-reference/core/security-level/#official_sensitive-level-2","title":"OFFICIAL_SENSITIVE (Level 2)","text":"<p>Description: Sensitive business data, controlled access</p> <p>Example Use Cases: - Customer emails and phone numbers - Internal financial reports - Employee performance data - Project roadmaps</p> <p>YAML Configuration: <pre><code>datasource:\n  security_level: OFFICIAL_SENSITIVE\n</code></pre></p>"},{"location":"api-reference/core/security-level/#protected-level-3","title":"PROTECTED (Level 3)","text":"<p>Description: Highly sensitive data, strict access controls</p> <p>Example Use Cases: - Detailed financial records - HR personnel files - Contracts and legal documents - Customer payment information</p> <p>YAML Configuration: <pre><code>datasource:\n  security_level: PROTECTED\n</code></pre></p>"},{"location":"api-reference/core/security-level/#secret-level-4","title":"SECRET (Level 4)","text":"<p>Description: Classified information, maximum protection</p> <p>Example Use Cases: - Government classified data - Regulated healthcare data (HIPAA) - National security information - Trade secrets</p> <p>YAML Configuration: <pre><code>datasource:\n  security_level: SECRET\n</code></pre></p>"},{"location":"api-reference/core/security-level/#usage-examples","title":"Usage Examples","text":""},{"location":"api-reference/core/security-level/#comparison-operations","title":"Comparison Operations","text":"<p>Security levels support rich comparisons:</p> <pre><code>from elspeth.core.base.types import SecurityLevel\n\n# Inequality comparisons\nprint(SecurityLevel.UNOFFICIAL &lt; SecurityLevel.OFFICIAL)  # True\nprint(SecurityLevel.SECRET &gt; SecurityLevel.PROTECTED)     # True\n\n# Equality\nprint(SecurityLevel.OFFICIAL == SecurityLevel.OFFICIAL)   # True\nprint(SecurityLevel.UNOFFICIAL == SecurityLevel.SECRET)   # False\n\n# Ordering\nlevels = [\n    SecurityLevel.SECRET,\n    SecurityLevel.UNOFFICIAL,\n    SecurityLevel.PROTECTED\n]\nsorted_levels = sorted(levels)\n# [SecurityLevel.UNOFFICIAL, SecurityLevel.PROTECTED, SecurityLevel.SECRET]\n</code></pre>"},{"location":"api-reference/core/security-level/#min-and-max-operations","title":"min() and max() Operations","text":"<p>Computing operating levels and uplifting:</p> <pre><code># Operating level = MIN of all component levels\ndatasource_level = SecurityLevel.SECRET\nllm_level = SecurityLevel.OFFICIAL\nsink_level = SecurityLevel.PROTECTED\n\noperating_level = min(datasource_level, llm_level, sink_level)\nprint(operating_level)  # SecurityLevel.OFFICIAL (lowest)\n\n# Classification uplifting = MAX operation\ncurrent_classification = SecurityLevel.OFFICIAL\nplugin_level = SecurityLevel.PROTECTED\n\nuplifted = max(current_classification, plugin_level)\nprint(uplifted)  # SecurityLevel.PROTECTED\n</code></pre>"},{"location":"api-reference/core/security-level/#validation-logic","title":"Validation Logic","text":"<pre><code>def validate_clearance(\n    component_level: SecurityLevel,\n    required_level: SecurityLevel\n) -&gt; bool:\n    \"\"\"Check if component has sufficient clearance.\n\n    Component must be &gt;= required level (higher or equal).\n    \"\"\"\n    return component_level &gt;= required_level\n\n# Examples\nprint(validate_clearance(\n    SecurityLevel.SECRET,\n    SecurityLevel.OFFICIAL\n))  # True (SECRET &gt;= OFFICIAL)\n\nprint(validate_clearance(\n    SecurityLevel.UNOFFICIAL,\n    SecurityLevel.SECRET\n))  # False (UNOFFICIAL &lt; SECRET)\n</code></pre>"},{"location":"api-reference/core/security-level/#string-representation","title":"String Representation","text":"<p>Security levels have string representations matching YAML configuration:</p> <pre><code>level = SecurityLevel.OFFICIAL\nprint(str(level))   # \"SecurityLevel.OFFICIAL\"\nprint(level.name)   # \"OFFICIAL\"\nprint(level.value)  # 1\n</code></pre>"},{"location":"api-reference/core/security-level/#parsing-from-strings","title":"Parsing from Strings","text":"<pre><code># From name\nlevel = SecurityLevel['OFFICIAL']\nprint(level)  # SecurityLevel.OFFICIAL\n\n# From value\nlevel = SecurityLevel(1)\nprint(level)  # SecurityLevel.OFFICIAL\n</code></pre>"},{"location":"api-reference/core/security-level/#integer-values","title":"Integer Values","text":"<p>Each level has an integer value for ordering:</p> Level Value Comparison UNOFFICIAL 0 Lowest OFFICIAL 1 Low OFFICIAL_SENSITIVE 2 Medium PROTECTED 3 High SECRET 4 Highest <pre><code>print(SecurityLevel.UNOFFICIAL.value)  # 0\nprint(SecurityLevel.SECRET.value)      # 4\n\n# Comparisons use integer values\nprint(SecurityLevel.SECRET.value &gt; SecurityLevel.OFFICIAL.value)  # True\n</code></pre>"},{"location":"api-reference/core/security-level/#common-patterns","title":"Common Patterns","text":""},{"location":"api-reference/core/security-level/#operating-level-computation","title":"Operating Level Computation","text":"<p>Pipeline operating level is the minimum across all components:</p> <pre><code>def compute_operating_level(*components) -&gt; SecurityLevel:\n    \"\"\"Compute pipeline operating level.\n\n    Returns minimum security level across all components.\n    This is the \"weakest link\" in the security chain.\n    \"\"\"\n    return min(c.get_security_level() for c in components)\n\n# Usage\noperating_level = compute_operating_level(\n    datasource,  # SECRET\n    llm,         # OFFICIAL\n    sink1,       # PROTECTED\n    sink2        # OFFICIAL\n)\nprint(operating_level)  # OFFICIAL (minimum)\n</code></pre>"},{"location":"api-reference/core/security-level/#classification-uplifting","title":"Classification Uplifting","text":"<p>Data classification can only increase (max operation):</p> <pre><code>def uplift_classification(\n    current: SecurityLevel,\n    plugin_level: SecurityLevel\n) -&gt; SecurityLevel:\n    \"\"\"Uplift classification to plugin level.\n\n    Returns max(current, plugin_level) to prevent downgrading.\n    \"\"\"\n    return max(current, plugin_level)\n\n# Examples\nprint(uplift_classification(\n    SecurityLevel.OFFICIAL,\n    SecurityLevel.PROTECTED\n))  # PROTECTED (uplifted)\n\nprint(uplift_classification(\n    SecurityLevel.SECRET,\n    SecurityLevel.OFFICIAL\n))  # SECRET (no downgrade)\n</code></pre>"},{"location":"api-reference/core/security-level/#validation-with-bell-lapadula-no-read-up","title":"Validation with Bell-LaPadula \"No Read Up\"","text":"<pre><code>def can_access(\n    component_clearance: SecurityLevel,\n    data_classification: SecurityLevel\n) -&gt; bool:\n    \"\"\"Check if component can access data (Bell-LaPadula \"no read up\").\n\n    Component must have clearance &gt;= data classification.\n    \"\"\"\n    return component_clearance &gt;= data_classification\n\n# Examples\nprint(can_access(\n    SecurityLevel.SECRET,      # Component clearance\n    SecurityLevel.OFFICIAL     # Data classification\n))  # True (SECRET &gt;= OFFICIAL)\n\nprint(can_access(\n    SecurityLevel.UNOFFICIAL,  # Component clearance\n    SecurityLevel.SECRET       # Data classification\n))  # False (UNOFFICIAL &lt; SECRET, insufficient clearance)\n</code></pre>"},{"location":"api-reference/core/security-level/#related-documentation","title":"Related Documentation","text":"<ul> <li>BasePlugin - Plugin base class using SecurityLevel</li> <li>ClassifiedDataFrame - Data container with SecurityLevel</li> <li>Security Model - Complete MLS explanation</li> </ul>"},{"location":"api-reference/core/security-level/#adr-cross-references","title":"ADR Cross-References","text":"<ul> <li>ADR-002: Multi-Level Security Enforcement - SecurityLevel enumeration defines classification hierarchy</li> <li>ADR-001: Design Philosophy - Security-first priority hierarchy</li> </ul>"},{"location":"api-reference/pipeline/artifact-pipeline/","title":"Artifact Pipeline","text":"<p>Dependency-ordered sink execution with security enforcement and chaining support.</p>"},{"location":"api-reference/pipeline/artifact-pipeline/#overview","title":"Overview","text":"<p>The Artifact Pipeline executes sinks in dependency order, ensuring:</p> <ol> <li>Dependency Resolution - Sinks run after their dependencies complete</li> <li>Security Enforcement - Each sink validates it can handle data classification</li> <li>Metadata Chaining - Sinks can consume outputs from previous sinks</li> <li>Error Isolation - Sink failures don't block independent sinks</li> </ol>"},{"location":"api-reference/pipeline/artifact-pipeline/#class-documentation","title":"Class Documentation","text":""},{"location":"api-reference/pipeline/artifact-pipeline/#elspeth.core.pipeline.artifact_pipeline.ArtifactPipeline","title":"ArtifactPipeline","text":"<pre><code>ArtifactPipeline(bindings: list[SinkBinding])\n</code></pre> <p>Resolves sink execution order based on declared artifact dependencies.</p> <p>Prepare bindings and calculate execution order.</p> Source code in <code>elspeth/core/pipeline/artifact_pipeline.py</code> <pre><code>def __init__(self, bindings: list[SinkBinding]) -&gt; None:\n    \"\"\"Prepare bindings and calculate execution order.\"\"\"\n\n    self._bindings = [self._prepare_binding(binding) for binding in bindings]\n    self._ordered_bindings = self._resolve_order(self._bindings)\n</code></pre>"},{"location":"api-reference/pipeline/artifact-pipeline/#elspeth.core.pipeline.artifact_pipeline.ArtifactPipeline-functions","title":"Functions","text":""},{"location":"api-reference/pipeline/artifact-pipeline/#elspeth.core.pipeline.artifact_pipeline.ArtifactPipeline.execute","title":"execute","text":"<pre><code>execute(payload: dict[str, Any], metadata: dict[str, Any] | None = None, *, on_error: str = 'raise', failures: list[dict[str, Any]] | None = None) -&gt; ArtifactStore\n</code></pre> <p>Run all sinks in dependency order, producing the final artifact store.</p> <p>Parameters:</p> Name Type Description Default <code>payload</code> <code>dict[str, Any]</code> <p>Experiment/job payload passed to sinks</p> required <code>metadata</code> <code>dict[str, Any] | None</code> <p>Metadata to pass alongside payload</p> <code>None</code> <code>on_error</code> <code>str</code> <p>Error handling strategy: \"raise\" (default) or \"continue\"</p> <code>'raise'</code> <code>failures</code> <code>list[dict[str, Any]] | None</code> <p>Optional list to append failure dicts when continuing on errors</p> <code>None</code> <p>Returns:</p> Type Description <code>ArtifactStore</code> <p>ArtifactStore with all registered artifacts</p> Source code in <code>elspeth/core/pipeline/artifact_pipeline.py</code> <pre><code>def execute(\n    self,\n    payload: dict[str, Any],\n    metadata: dict[str, Any] | None = None,\n    *,\n    on_error: str = \"raise\",\n    failures: list[dict[str, Any]] | None = None,\n) -&gt; ArtifactStore:\n    \"\"\"Run all sinks in dependency order, producing the final artifact store.\n\n    Args:\n        payload: Experiment/job payload passed to sinks\n        metadata: Metadata to pass alongside payload\n        on_error: Error handling strategy: \"raise\" (default) or \"continue\"\n        failures: Optional list to append failure dicts when continuing on errors\n\n    Returns:\n        ArtifactStore with all registered artifacts\n    \"\"\"\n\n    if on_error not in {\"raise\", \"continue\"}:  # pragma: no cover - defensive guard\n        raise ValueError(\"on_error must be 'raise' or 'continue'\")\n\n    store = ArtifactStore()\n    metadata_dict: dict[str, Any | None] = dict(metadata) if metadata is not None else {}\n    for binding in self._ordered_bindings:\n        try:\n            consumed = store.resolve_requests(binding.consumes)\n\n            clearance = binding.security_level\n            if clearance:\n                for artifacts in consumed.values():\n                    for artifact in artifacts:\n                        if not is_security_level_allowed(artifact.security_level, clearance):\n                            raise PermissionError(\n                                f\"Sink '{binding.id}' with clearance '{clearance}' cannot consume \"\n                                f\"artifact '{artifact.id}' at level '{artifact.security_level}'\"\n                            )\n\n            prepare = getattr(binding.sink, \"prepare_artifacts\", None)\n            if callable(prepare):\n                prepare(consumed)\n\n            binding.sink.write(payload, metadata=metadata_dict)\n\n            produced: dict[str, Artifact] = {}\n            collector = getattr(binding.sink, \"collect_artifacts\", None)\n            if callable(collector):\n                collected = collector()\n                if collected:\n                    produced = cast(dict[str, Artifact], collected)\n\n            for descriptor in binding.produces:\n                key = descriptor.name\n                candidate = produced.get(key)\n                if not candidate and descriptor.alias:\n                    candidate = produced.get(descriptor.alias)\n                if candidate:\n                    store.register(binding, descriptor, candidate)\n\n            finalize = getattr(binding.sink, \"finalize\", None)\n            if callable(finalize):\n                finalize(dict(store.items()), metadata=metadata_dict)\n        except Exception as exc:  # pylint: disable=broad-except\n            if on_error == \"continue\":\n                sink_name = getattr(getattr(binding.sink, \"__class__\", type(binding.sink)), \"__name__\", binding.plugin or binding.id)\n                failure = {\"sink\": sink_name, \"error\": str(exc)}\n                if failures is not None:\n                    failures.append(failure)\n                # Continue to next binding to maximize delivery\n                continue\n            raise\n\n    return store\n</code></pre>"},{"location":"api-reference/pipeline/artifact-pipeline/#elspeth.core.pipeline.artifact_pipeline.ArtifactPipeline._topological_sort","title":"_topological_sort  <code>staticmethod</code>","text":"<pre><code>_topological_sort(bindings: Iterable[SinkBinding], dependencies: dict[str, set[str]], dependents: dict[str, set[str]], by_id: dict[str, SinkBinding]) -&gt; list[SinkBinding]\n</code></pre> <p>Order bindings based on resolved dependencies.</p> Source code in <code>elspeth/core/pipeline/artifact_pipeline.py</code> <pre><code>@staticmethod\ndef _topological_sort(\n    bindings: Iterable[SinkBinding],\n    dependencies: dict[str, set[str]],\n    dependents: dict[str, set[str]],\n    by_id: dict[str, SinkBinding],\n) -&gt; list[SinkBinding]:\n    \"\"\"Order bindings based on resolved dependencies.\"\"\"\n\n    ready: deque[SinkBinding] = deque(\n        sorted(\n            [binding for binding in bindings if not dependencies[binding.id]],\n            key=lambda b: b.original_index,\n        )\n    )\n\n    ordered: list[SinkBinding] = []\n\n    while ready:\n        current = ready.popleft()\n        ordered.append(current)\n        for dependent_id in dependents[current.id]:\n            deps = dependencies[dependent_id]\n            if current.id not in deps:\n                continue\n            deps.remove(current.id)\n            if deps:\n                continue\n            ready.append(by_id[dependent_id])\n            ready = deque(sorted(ready, key=lambda b: b.original_index))\n\n    return ordered\n</code></pre>"},{"location":"api-reference/pipeline/artifact-pipeline/#basic-usage","title":"Basic Usage","text":""},{"location":"api-reference/pipeline/artifact-pipeline/#simple-pipeline-no-dependencies","title":"Simple Pipeline (No Dependencies)","text":"<pre><code>from elspeth.core.pipeline.artifact_pipeline import ArtifactPipeline\nfrom elspeth.core.security.classified_data import ClassifiedDataFrame\nfrom elspeth.core.base.types import SecurityLevel\n\n# Create sinks\ncsv_sink = CSVSink(path=\"output.csv\", security_level=SecurityLevel.OFFICIAL)\nexcel_sink = ExcelWorkbookSink(base_path=\"report\", security_level=SecurityLevel.OFFICIAL)\n\n# Create pipeline\npipeline = ArtifactPipeline(\n    sinks=[csv_sink, excel_sink],\n    operating_level=SecurityLevel.OFFICIAL\n)\n\n# Execute (sinks run in parallel)\nframe = ClassifiedDataFrame.create_from_datasource(data, SecurityLevel.OFFICIAL)\npipeline.execute(frame, metadata={\"experiment_name\": \"test\"})\n</code></pre>"},{"location":"api-reference/pipeline/artifact-pipeline/#pipeline-with-dependencies","title":"Pipeline with Dependencies","text":"<pre><code># Define sink dependencies\nsink_configs = [\n    {\n        \"name\": \"csv_output\",\n        \"type\": \"csv\",\n        \"path\": \"results.csv\",\n        \"security_level\": \"OFFICIAL\"\n    },\n    {\n        \"name\": \"signed_bundle\",\n        \"type\": \"signed_artifact\",\n        \"base_path\": \"artifacts\",\n        \"consumes\": [\"csv_output\"],  # \u2190 Wait for CSV to complete\n        \"security_level\": \"OFFICIAL\"\n    }\n]\n\n# Pipeline ensures csv_output runs before signed_bundle\npipeline = ArtifactPipeline(sinks=created_sinks, operating_level=SecurityLevel.OFFICIAL)\npipeline.execute(frame, metadata={})\n</code></pre>"},{"location":"api-reference/pipeline/artifact-pipeline/#dependency-resolution","title":"Dependency Resolution","text":""},{"location":"api-reference/pipeline/artifact-pipeline/#topological-sort","title":"Topological Sort","text":"<p>Sinks are executed in topological order based on <code>consumes</code> declarations:</p> <pre><code>sinks:\n  # Independent sinks (run first, in parallel)\n  - name: csv\n    type: csv\n    path: data.csv\n\n  - name: excel\n    type: excel_workbook\n    base_path: report\n\n  # Dependent sinks (run after csv completes)\n  - name: signed\n    type: signed_artifact\n    consumes: [csv]\n\n  - name: zip\n    type: zip_bundle\n    consumes: [csv, excel]  # Waits for both\n</code></pre> <p>Execution Order: <pre><code>1. csv, excel (parallel)\n       \u2193\n2. signed (waits for csv)\n       \u2193\n3. zip (waits for csv AND excel)\n</code></pre></p>"},{"location":"api-reference/pipeline/artifact-pipeline/#cycle-detection","title":"Cycle Detection","text":"<p>Circular dependencies are detected and rejected:</p> <pre><code>sinks:\n  - name: sink_a\n    consumes: [sink_b]  # \u2190 A depends on B\n\n  - name: sink_b\n    consumes: [sink_a]  # \u2190 B depends on A (cycle!)\n\n# Raises: ValueError: Circular dependency detected\n</code></pre>"},{"location":"api-reference/pipeline/artifact-pipeline/#security-enforcement","title":"Security Enforcement","text":""},{"location":"api-reference/pipeline/artifact-pipeline/#per-sink-validation","title":"Per-Sink Validation","text":"<p>Each sink validates it can operate at the pipeline's level:</p> <pre><code># Pipeline operating level = OFFICIAL\npipeline = ArtifactPipeline(\n    sinks=[\n        CSVSink(security_level=SecurityLevel.SECRET),     # \u2705 OK (can downgrade)\n        ExcelSink(security_level=SecurityLevel.OFFICIAL), # \u2705 OK (exact match)\n        PDFSink(security_level=SecurityLevel.UNOFFICIAL)  # \u274c Fails (insufficient clearance)\n    ],\n    operating_level=SecurityLevel.OFFICIAL\n)\n\n# PDFSink validation fails:\n# SecurityValidationError: Sink 'pdf' has insufficient clearance (UNOFFICIAL) for pipeline level (OFFICIAL)\n</code></pre>"},{"location":"api-reference/pipeline/artifact-pipeline/#data-classification-validation","title":"Data Classification Validation","text":"<p>Sinks validate they can handle the data's classification:</p> <pre><code># Data classified as SECRET\nframe = ClassifiedDataFrame.create_from_datasource(data, SecurityLevel.SECRET)\n\n# Sink with OFFICIAL clearance\nsink = CSVSink(security_level=SecurityLevel.OFFICIAL)\n\n# Execution fails:\n# SecurityValidationError: Sink cannot write SECRET data (clearance: OFFICIAL)\n</code></pre>"},{"location":"api-reference/pipeline/artifact-pipeline/#metadata-chaining","title":"Metadata Chaining","text":""},{"location":"api-reference/pipeline/artifact-pipeline/#consuming-outputs","title":"Consuming Outputs","text":"<p>Sinks can access outputs from their dependencies:</p> <pre><code>class SignedBundleSink(BasePlugin):\n    def write(self, frame: ClassifiedDataFrame, metadata: dict) -&gt; dict:\n        \"\"\"Write signed bundle.\"\"\"\n        # Access CSV sink output\n        csv_path = metadata.get('csv_output', {}).get('path')\n\n        if csv_path:\n            # Include CSV in signed bundle\n            bundle.add_file(csv_path)\n\n        # Return metadata for downstream sinks\n        return {\n            \"bundle_path\": bundle_path,\n            \"signature_path\": signature_path\n        }\n</code></pre> <p>Metadata Flow: <pre><code>csv_sink.write()\n   \u2193 returns {\"path\": \"data.csv\"}\n   \u2193\nmetadata[\"csv_output\"] = {\"path\": \"data.csv\"}\n   \u2193\nsigned_sink.write(metadata) \u2190 accesses metadata[\"csv_output\"][\"path\"]\n</code></pre></p>"},{"location":"api-reference/pipeline/artifact-pipeline/#error-handling","title":"Error Handling","text":""},{"location":"api-reference/pipeline/artifact-pipeline/#on_error-policy","title":"on_error Policy","text":"<p>Sinks can specify error handling:</p> <pre><code>sinks:\n  - name: critical_sink\n    type: csv\n    on_error: abort  # \u2190 Stop pipeline on error\n\n  - name: optional_sink\n    type: analytics_report\n    on_error: skip  # \u2190 Log error and continue\n</code></pre>"},{"location":"api-reference/pipeline/artifact-pipeline/#partial-execution","title":"Partial Execution","text":"<p>Independent sinks continue even if others fail:</p> <pre><code>sinks = [\n    csv_sink,      # \u2190 Fails\n    excel_sink,    # \u2190 Succeeds (independent)\n    signed_sink    # \u2190 Skipped (depends on csv_sink)\n]\n\npipeline = ArtifactPipeline(sinks=sinks)\npipeline.execute(frame, metadata={})\n\n# Result:\n# - csv_sink: FAILED\n# - excel_sink: SUCCESS (independent, unaffected)\n# - signed_sink: SKIPPED (dependency failed)\n</code></pre>"},{"location":"api-reference/pipeline/artifact-pipeline/#advanced-usage","title":"Advanced Usage","text":""},{"location":"api-reference/pipeline/artifact-pipeline/#custom-execution-context","title":"Custom Execution Context","text":"<pre><code>class ArtifactPipeline:\n    def execute(\n        self,\n        frame: ClassifiedDataFrame,\n        metadata: dict,\n        *,\n        dry_run: bool = False,\n        timeout: Optional[float] = None\n    ) -&gt; dict:\n        \"\"\"Execute sinks with custom context.\n\n        Args:\n            frame: Data to write\n            metadata: Experiment metadata\n            dry_run: Simulate execution without writing\n            timeout: Per-sink timeout in seconds\n\n        Returns:\n            Aggregated metadata from all sinks\n        \"\"\"\n        pass\n</code></pre>"},{"location":"api-reference/pipeline/artifact-pipeline/#parallel-execution","title":"Parallel Execution","text":"<p>Independent sinks can run in parallel:</p> <pre><code>import concurrent.futures\n\ndef execute_parallel(self, frame, metadata):\n    \"\"\"Execute independent sinks in parallel.\"\"\"\n    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n        # Group sinks by dependency level\n        levels = self._topological_sort()\n\n        for level_sinks in levels:\n            # Run sinks at same level in parallel\n            futures = [\n                executor.submit(sink.write, frame, metadata)\n                for sink in level_sinks\n            ]\n            # Wait for level to complete before next level\n            concurrent.futures.wait(futures)\n</code></pre>"},{"location":"api-reference/pipeline/artifact-pipeline/#sink-interface","title":"Sink Interface","text":""},{"location":"api-reference/pipeline/artifact-pipeline/#required-methods","title":"Required Methods","text":"<pre><code>from elspeth.core.base.plugin import BasePlugin\n\nclass CustomSink(BasePlugin):\n    def write(\n        self,\n        frame: ClassifiedDataFrame,\n        metadata: dict\n    ) -&gt; dict:\n        \"\"\"Write data to destination.\n\n        Args:\n            frame: Data with classification\n            metadata: Experiment metadata + outputs from dependencies\n\n        Returns:\n            Metadata for downstream sinks (optional)\n\n        Raises:\n            SecurityValidationError: If insufficient clearance\n        \"\"\"\n        # Validate can operate at data's classification\n        self.validate_can_operate_at_level(frame.classification)\n\n        # Write data\n        # ...\n\n        # Return metadata for downstream\n        return {\"output_path\": path}\n</code></pre>"},{"location":"api-reference/pipeline/artifact-pipeline/#optional-properties","title":"Optional Properties","text":"<pre><code>class CustomSink(BasePlugin):\n    def consumes(self) -&gt; list[str]:\n        \"\"\"Return names of sinks this sink depends on.\"\"\"\n        return [\"csv_output\", \"excel_output\"]\n\n    def produces(self) -&gt; list[str]:\n        \"\"\"Return names this sink exports to metadata.\"\"\"\n        return [\"signed_bundle_path\", \"signature_path\"]\n</code></pre>"},{"location":"api-reference/pipeline/artifact-pipeline/#related-documentation","title":"Related Documentation","text":"<ul> <li>Sinks - Sink plugin API</li> <li>ClassifiedDataFrame - Data container</li> <li>Security Model - Security enforcement</li> </ul>"},{"location":"api-reference/pipeline/artifact-pipeline/#adr-cross-references","title":"ADR Cross-References","text":"<ul> <li>ADR-002: Multi-Level Security - Pipeline enforces security level validation per sink</li> </ul>"},{"location":"api-reference/plugins/generated-aggregators/","title":"Aggregator Plugins API (Auto-Generated)","text":"<p>Auto-Generated Documentation</p> <p>This file is automatically generated from source code. Do not edit manually.</p> <p>Generated from: <code>scripts/generate_plugin_docs.py</code> Last generated: Run <code>make docs-generate</code> to update</p> <p>API documentation for 8 aggregator plugins that aggregate results after experiments.</p>"},{"location":"api-reference/plugins/generated-aggregators/#costsummaryaggregator","title":"CostSummaryAggregator","text":"<p>Configuration Type: <code>cost_summary</code> Security: \u2705 Trusted Downgrade Source: <code>src/elspeth/plugins/experiments/aggregators/cost_summary.py</code></p> <p>Example Configuration: <pre><code>plugin:\n  type: cost_summary\n  security_level: OFFICIAL\n  allow_downgrade: true\n</code></pre></p>"},{"location":"api-reference/plugins/generated-aggregators/#elspeth.plugins.experiments.aggregators.cost_summary.CostSummaryAggregator","title":"CostSummaryAggregator","text":"<pre><code>CostSummaryAggregator(*, on_error: str = 'abort')\n</code></pre> <p>               Bases: <code>BasePlugin</code></p> <p>Aggregate cost and token usage metrics across all rows.</p> <p>Collects prompt_tokens, completion_tokens, and cost from response metrics (added by cost tracker) and computes totals, averages, min, and max.</p> Source code in <code>elspeth/plugins/experiments/aggregators/cost_summary.py</code> <pre><code>def __init__(\n    self,\n    *,\n    on_error: str = \"abort\",\n) -&gt; None:\n    # ADR-002-B: Security policy is immutable and hard-coded in plugin code\n    super().__init__(\n        security_level=SecurityLevel.UNOFFICIAL,  # Aggregators work with experiment results\n        allow_downgrade=True,  # Trusted to operate at lower levels if needed (ADR-005)\n    )\n    if on_error not in {\"abort\", \"skip\"}:\n        raise ValueError(\"on_error must be 'abort' or 'skip'\")\n    self._on_error = on_error\n</code></pre>"},{"location":"api-reference/plugins/generated-aggregators/#elspeth.plugins.experiments.aggregators.cost_summary.CostSummaryAggregator-functions","title":"Functions","text":""},{"location":"api-reference/plugins/generated-aggregators/#latencysummaryaggregator","title":"LatencySummaryAggregator","text":"<p>Configuration Type: <code>latency_summary</code> Security: \u2705 Trusted Downgrade Source: <code>src/elspeth/plugins/experiments/aggregators/latency_summary.py</code></p> <p>Example Configuration: <pre><code>plugin:\n  type: latency_summary\n  security_level: OFFICIAL\n  allow_downgrade: true\n</code></pre></p>"},{"location":"api-reference/plugins/generated-aggregators/#elspeth.plugins.experiments.aggregators.latency_summary.LatencySummaryAggregator","title":"LatencySummaryAggregator","text":"<pre><code>LatencySummaryAggregator(*, on_error: str = 'abort')\n</code></pre> <p>               Bases: <code>BasePlugin</code></p> <p>Aggregate latency metrics across all rows.</p> <p>Collects latency_seconds from response metrics and computes totals, averages, percentiles, min, and max.</p> Source code in <code>elspeth/plugins/experiments/aggregators/latency_summary.py</code> <pre><code>def __init__(\n    self,\n    *,\n    on_error: str = \"abort\",\n) -&gt; None:\n    # ADR-002-B: Security policy is immutable and hard-coded in plugin code\n    super().__init__(\n        security_level=SecurityLevel.UNOFFICIAL,  # Aggregators work with experiment results\n        allow_downgrade=True,  # Trusted to operate at lower levels if needed (ADR-005)\n    )\n    if on_error not in {\"abort\", \"skip\"}:\n        raise ValueError(\"on_error must be 'abort' or 'skip'\")\n    self._on_error = on_error\n</code></pre>"},{"location":"api-reference/plugins/generated-aggregators/#elspeth.plugins.experiments.aggregators.latency_summary.LatencySummaryAggregator-functions","title":"Functions","text":""},{"location":"api-reference/plugins/generated-aggregators/#rationaleanalysisaggregator","title":"RationaleAnalysisAggregator","text":"<p>Configuration Type: <code>rationale_analysis</code> Security: \u2705 Trusted Downgrade Source: <code>src/elspeth/plugins/experiments/aggregators/rationale_analysis.py</code></p> <p>Example Configuration: <pre><code>plugin:\n  type: rationale_analysis\n  security_level: OFFICIAL\n  allow_downgrade: true\n</code></pre></p>"},{"location":"api-reference/plugins/generated-aggregators/#elspeth.plugins.experiments.aggregators.rationale_analysis.RationaleAnalysisAggregator","title":"RationaleAnalysisAggregator","text":"<pre><code>RationaleAnalysisAggregator(*, rationale_field: str = 'rationale', score_field: str = 'score', criteria: list[str] | None = None, min_word_length: int = 3, top_keywords: int = 10, on_error: str = 'abort')\n</code></pre> <p>               Bases: <code>BasePlugin</code></p> <p>Analyze LLM rationales to understand scoring patterns and provide interpretability.</p> <p>This plugin extracts rationales from response content, analyzes common themes in low vs high scoring responses, computes rationale length statistics, and identifies confidence indicators. Provides qualitative insights to complement quantitative scoring metrics.</p> Source code in <code>elspeth/plugins/experiments/aggregators/rationale_analysis.py</code> <pre><code>def __init__(\n    self,\n    *,\n    rationale_field: str = \"rationale\",\n    score_field: str = \"score\",\n    criteria: list[str] | None = None,\n    min_word_length: int = 3,\n    top_keywords: int = 10,\n    on_error: str = \"abort\",\n) -&gt; None:\n    # ADR-002-B: Security policy is immutable and hard-coded in plugin code\n    super().__init__(\n        security_level=SecurityLevel.UNOFFICIAL,  # Aggregators work with experiment results\n        allow_downgrade=True,  # Trusted to operate at lower levels if needed (ADR-005)\n    )\n    self._rationale_field = rationale_field\n    self._score_field = score_field\n    self._criteria = set(criteria) if criteria else None\n    self._min_word_length = max(int(min_word_length), 2)\n    self._top_keywords = max(int(top_keywords), 1)\n    if on_error not in {\"abort\", \"skip\"}:\n        raise ValueError(\"on_error must be 'abort' or 'skip'\")\n    self._on_error = on_error\n\n    # Common stop words to filter out\n    self._stop_words = {\n        \"the\",\n        \"a\",\n        \"an\",\n        \"and\",\n        \"or\",\n        \"but\",\n        \"in\",\n        \"on\",\n        \"at\",\n        \"to\",\n        \"for\",\n        \"of\",\n        \"with\",\n        \"by\",\n        \"from\",\n        \"is\",\n        \"was\",\n        \"are\",\n        \"were\",\n        \"been\",\n        \"be\",\n        \"have\",\n        \"has\",\n        \"had\",\n        \"do\",\n        \"does\",\n        \"did\",\n        \"will\",\n        \"would\",\n        \"could\",\n        \"should\",\n        \"may\",\n        \"might\",\n        \"can\",\n        \"this\",\n        \"that\",\n        \"these\",\n        \"those\",\n        \"it\",\n        \"as\",\n        \"not\",\n    }\n</code></pre>"},{"location":"api-reference/plugins/generated-aggregators/#elspeth.plugins.experiments.aggregators.rationale_analysis.RationaleAnalysisAggregator-functions","title":"Functions","text":""},{"location":"api-reference/plugins/generated-aggregators/#scoreagreementaggregator","title":"ScoreAgreementAggregator","text":"<p>Configuration Type: <code>score_agreement</code> Security: \u2705 Trusted Downgrade Source: <code>src/elspeth/plugins/experiments/aggregators/score_agreement.py</code></p> <p>Example Configuration: <pre><code>plugin:\n  type: score_agreement\n  security_level: OFFICIAL\n  allow_downgrade: true\n</code></pre></p>"},{"location":"api-reference/plugins/generated-aggregators/#elspeth.plugins.experiments.aggregators.score_agreement.ScoreAgreementAggregator","title":"ScoreAgreementAggregator","text":"<pre><code>ScoreAgreementAggregator(*, criteria: Sequence[str] | None = None, min_items: int = 2, on_error: str = 'abort')\n</code></pre> <p>               Bases: <code>BasePlugin</code></p> <p>Assess agreement/reliability across criteria scores.</p> Source code in <code>elspeth/plugins/experiments/aggregators/score_agreement.py</code> <pre><code>def __init__(\n    self,\n    *,\n    criteria: Sequence[str] | None = None,\n    min_items: int = 2,\n    on_error: str = \"abort\",\n) -&gt; None:\n    # ADR-002-B: Security policy is immutable and hard-coded in plugin code\n    super().__init__(\n        security_level=SecurityLevel.UNOFFICIAL,  # Aggregators work with experiment results\n        allow_downgrade=True,  # Trusted to operate at lower levels if needed (ADR-005)\n    )\n    self._criteria = list(criteria) if criteria else None\n    self._min_items = max(int(min_items), 2)\n    if on_error not in {\"abort\", \"skip\"}:\n        raise ValueError(\"on_error must be 'abort' or 'skip'\")\n    self._on_error = on_error\n</code></pre>"},{"location":"api-reference/plugins/generated-aggregators/#elspeth.plugins.experiments.aggregators.score_agreement.ScoreAgreementAggregator-functions","title":"Functions","text":""},{"location":"api-reference/plugins/generated-aggregators/#scorepoweraggregator","title":"ScorePowerAggregator","text":"<p>Configuration Type: <code>score_power</code> Security: \u2705 Trusted Downgrade Source: <code>src/elspeth/plugins/experiments/aggregators/score_power.py</code></p> <p>Example Configuration: <pre><code>plugin:\n  type: score_power\n  security_level: OFFICIAL\n  allow_downgrade: true\n</code></pre></p>"},{"location":"api-reference/plugins/generated-aggregators/#elspeth.plugins.experiments.aggregators.score_power.ScorePowerAggregator","title":"ScorePowerAggregator","text":"<pre><code>ScorePowerAggregator(*, criteria: Sequence[str] | None = None, min_samples: int = 2, alpha: float = 0.05, target_power: float = 0.8, effect_size: float | None = None, null_mean: float = 0.0, on_error: str = 'abort')\n</code></pre> <p>               Bases: <code>BasePlugin</code></p> <p>Estimate power and required sample size for mean comparisons.</p> Source code in <code>elspeth/plugins/experiments/aggregators/score_power.py</code> <pre><code>def __init__(\n    self,\n    *,\n    criteria: Sequence[str] | None = None,\n    min_samples: int = 2,\n    alpha: float = 0.05,\n    target_power: float = 0.8,\n    effect_size: float | None = None,\n    null_mean: float = 0.0,\n    on_error: str = \"abort\",\n) -&gt; None:\n    # ADR-002-B: Security policy is immutable and hard-coded in plugin code\n    super().__init__(\n        security_level=SecurityLevel.UNOFFICIAL,  # Aggregators work with experiment results\n        allow_downgrade=True,  # Trusted to operate at lower levels if needed (ADR-005)\n    )\n    self._criteria = set(criteria) if criteria else None\n    self._min_samples = max(int(min_samples), 2)\n    self._alpha = min(max(float(alpha), 1e-6), 0.25)\n    self._target_power = min(max(float(target_power), 0.1), 0.999)\n    self._effect_size = effect_size\n    self._null_mean = float(null_mean)\n    if on_error not in {\"abort\", \"skip\"}:\n        raise ValueError(\"on_error must be 'abort' or 'skip'\")\n    self._on_error = on_error\n</code></pre>"},{"location":"api-reference/plugins/generated-aggregators/#elspeth.plugins.experiments.aggregators.score_power.ScorePowerAggregator-functions","title":"Functions","text":""},{"location":"api-reference/plugins/generated-aggregators/#scorerecommendationaggregator","title":"ScoreRecommendationAggregator","text":"<p>Configuration Type: <code>score_recommendation</code> Security: \u2705 Trusted Downgrade Source: <code>src/elspeth/plugins/experiments/aggregators/score_recommendation.py</code></p> <p>Example Configuration: <pre><code>plugin:\n  type: score_recommendation\n  security_level: OFFICIAL\n  allow_downgrade: true\n</code></pre></p>"},{"location":"api-reference/plugins/generated-aggregators/#elspeth.plugins.experiments.aggregators.score_recommendation.ScoreRecommendationAggregator","title":"ScoreRecommendationAggregator","text":"<pre><code>ScoreRecommendationAggregator(*, min_samples: int = 5, improvement_margin: float = 0.05, source_field: str = 'scores', flag_field: str = 'score_flags')\n</code></pre> <p>               Bases: <code>BasePlugin</code></p> <p>Generate a lightweight recommendation based on score statistics.</p> Source code in <code>elspeth/plugins/experiments/aggregators/score_recommendation.py</code> <pre><code>def __init__(\n    self,\n    *,\n    min_samples: int = 5,\n    improvement_margin: float = 0.05,\n    source_field: str = \"scores\",\n    flag_field: str = \"score_flags\",\n) -&gt; None:\n    # ADR-002-B: Security policy is immutable and hard-coded in plugin code\n    super().__init__(\n        security_level=SecurityLevel.UNOFFICIAL,  # Aggregators work with experiment results\n        allow_downgrade=True,  # Trusted to operate at lower levels if needed (ADR-005)\n    )\n    self._min_samples = min_samples\n    self._improvement_margin = improvement_margin\n    # ADR-002-B: ScoreStatsAggregator now hard-codes security_level internally\n    self._stats = ScoreStatsAggregator(\n        source_field=source_field,\n        flag_field=flag_field,\n    )\n</code></pre>"},{"location":"api-reference/plugins/generated-aggregators/#elspeth.plugins.experiments.aggregators.score_recommendation.ScoreRecommendationAggregator-functions","title":"Functions","text":""},{"location":"api-reference/plugins/generated-aggregators/#scorestatsaggregator","title":"ScoreStatsAggregator","text":"<p>Configuration Type: <code>score_stats</code> Security: \u2705 Trusted Downgrade Source: <code>src/elspeth/plugins/experiments/aggregators/score_stats.py</code></p> <p>Example Configuration: <pre><code>plugin:\n  type: score_stats\n  security_level: OFFICIAL\n  allow_downgrade: true\n</code></pre></p>"},{"location":"api-reference/plugins/generated-aggregators/#elspeth.plugins.experiments.aggregators.score_stats.ScoreStatsAggregator","title":"ScoreStatsAggregator","text":"<pre><code>ScoreStatsAggregator(*, source_field: str = 'scores', flag_field: str = 'score_flags', ddof: int = 0)\n</code></pre> <p>               Bases: <code>BasePlugin</code></p> <p>Aggregate score statistics across all rows.</p> Source code in <code>elspeth/plugins/experiments/aggregators/score_stats.py</code> <pre><code>def __init__(\n    self,\n    *,\n    source_field: str = \"scores\",\n    flag_field: str = \"score_flags\",\n    ddof: int = 0,\n) -&gt; None:\n    # ADR-002-B: Security policy is immutable and hard-coded in plugin code\n    super().__init__(\n        security_level=SecurityLevel.UNOFFICIAL,  # Aggregators work with experiment results\n        allow_downgrade=True,  # Trusted to operate at lower levels if needed (ADR-005)\n    )\n    self._source_field = source_field\n    self._flag_field = flag_field\n    self._ddof = ddof\n</code></pre>"},{"location":"api-reference/plugins/generated-aggregators/#elspeth.plugins.experiments.aggregators.score_stats.ScoreStatsAggregator-functions","title":"Functions","text":""},{"location":"api-reference/plugins/generated-aggregators/#scorevariantrankingaggregator","title":"ScoreVariantRankingAggregator","text":"<p>Configuration Type: <code>score_variant_ranking</code> Security: \u2705 Trusted Downgrade Source: <code>src/elspeth/plugins/experiments/aggregators/score_variant_ranking.py</code></p> <p>Example Configuration: <pre><code>plugin:\n  type: score_variant_ranking\n  security_level: OFFICIAL\n  allow_downgrade: true\n</code></pre></p>"},{"location":"api-reference/plugins/generated-aggregators/#elspeth.plugins.experiments.aggregators.score_variant_ranking.ScoreVariantRankingAggregator","title":"ScoreVariantRankingAggregator","text":"<pre><code>ScoreVariantRankingAggregator(*, threshold: float = 0.7, weight_mean: float = 1.0, weight_pass: float = 1.0)\n</code></pre> <p>               Bases: <code>BasePlugin</code></p> <p>Compute a simple composite ranking score for an experiment.</p> Source code in <code>elspeth/plugins/experiments/aggregators/score_variant_ranking.py</code> <pre><code>def __init__(\n    self,\n    *,\n    threshold: float = 0.7,\n    weight_mean: float = 1.0,\n    weight_pass: float = 1.0,\n) -&gt; None:\n    # ADR-002-B: Security policy is immutable and hard-coded in plugin code\n    super().__init__(\n        security_level=SecurityLevel.UNOFFICIAL,  # Aggregators work with experiment results\n        allow_downgrade=True,  # Trusted to operate at lower levels if needed (ADR-005)\n    )\n    self._threshold = float(threshold)\n    self._weight_mean = float(weight_mean)\n    self._weight_pass = float(weight_pass)\n</code></pre>"},{"location":"api-reference/plugins/generated-aggregators/#elspeth.plugins.experiments.aggregators.score_variant_ranking.ScoreVariantRankingAggregator-functions","title":"Functions","text":""},{"location":"api-reference/plugins/generated-baselines/","title":"Baseline Plugins API (Auto-Generated)","text":"<p>Auto-Generated Documentation</p> <p>This file is automatically generated from source code. Do not edit manually.</p> <p>Generated from: <code>scripts/generate_plugin_docs.py</code> Last generated: Run <code>make docs-generate</code> to update</p> <p>API documentation for 12 baseline plugins that compare experiments against baselines.</p>"},{"location":"api-reference/plugins/generated-baselines/#categoryeffectsaggregator","title":"CategoryEffectsAggregator","text":"<p>Configuration Type: <code>category_effects</code> Security: \u2705 Trusted Downgrade Source: <code>src/elspeth/plugins/experiments/baseline/category_effects.py</code></p> <p>Example Configuration: <pre><code>plugin:\n  type: category_effects\n  security_level: OFFICIAL\n  allow_downgrade: true\n</code></pre></p>"},{"location":"api-reference/plugins/generated-baselines/#elspeth.plugins.experiments.baseline.category_effects.CategoryEffectsAggregator","title":"CategoryEffectsAggregator","text":"<pre><code>CategoryEffectsAggregator(*, category_field: str = 'category', criteria: list[str] | None = None, min_samples: int = 2, top_n: int = 10, on_error: str = 'abort')\n</code></pre> <p>               Bases: <code>BasePlugin</code></p> <p>Analyze how categorical variables affect score distributions.</p> <p>Discovers categories dynamically from row data, computes per-category statistics, effect sizes, and ranks categories by impact magnitude. Useful for understanding which subpopulations are most affected by variant changes.</p> <p>Example use case: Understanding performance across document types, user segments, or difficulty levels.</p> Source code in <code>elspeth/plugins/experiments/baseline/category_effects.py</code> <pre><code>def __init__(\n    self,\n    *,\n    category_field: str = \"category\",\n    criteria: list[str] | None = None,\n    min_samples: int = 2,\n    top_n: int = 10,\n    on_error: str = \"abort\",\n) -&gt; None:\n    super().__init__(\n        security_level=SecurityLevel.UNOFFICIAL,  # ADR-002-B: Immutable policy\n        allow_downgrade=True  # ADR-002-B: Immutable policy\n    )\n    self._category_field = category_field\n    self._criteria = set(criteria) if criteria else None\n    self._min_samples = max(int(min_samples), 1)\n    self._top_n = max(int(top_n), 1)\n    if on_error not in {\"abort\", \"skip\"}:\n        raise ValueError(\"on_error must be 'abort' or 'skip'\")\n    self._on_error = on_error\n</code></pre>"},{"location":"api-reference/plugins/generated-baselines/#elspeth.plugins.experiments.baseline.category_effects.CategoryEffectsAggregator-functions","title":"Functions","text":""},{"location":"api-reference/plugins/generated-baselines/#criteriaeffectsbaselineplugin","title":"CriteriaEffectsBaselinePlugin","text":"<p>Configuration Type: <code>criteria_effects</code> Security: \u2705 Trusted Downgrade Source: <code>src/elspeth/plugins/experiments/baseline/criteria_effects.py</code></p> <p>Example Configuration: <pre><code>plugin:\n  type: criteria_effects\n  security_level: OFFICIAL\n  allow_downgrade: true\n</code></pre></p>"},{"location":"api-reference/plugins/generated-baselines/#elspeth.plugins.experiments.baseline.criteria_effects.CriteriaEffectsBaselinePlugin","title":"CriteriaEffectsBaselinePlugin","text":"<pre><code>CriteriaEffectsBaselinePlugin(*, criteria: list[str] | None = None, min_samples: int = 2, alpha: float = 0.05, on_error: str = 'abort')\n</code></pre> <p>               Bases: <code>BasePlugin</code></p> <p>Perform per-criterion statistical comparisons between baseline and variant.</p> <p>Computes detailed statistics for each scoring criterion individually, including means, effect sizes, Mann-Whitney U tests, and significance flags. Provides finer-grained analysis than overall score comparisons.</p> <p>Useful for: understanding which criteria are most affected by changes, identifying criterion-specific regressions or improvements.</p> Source code in <code>elspeth/plugins/experiments/baseline/criteria_effects.py</code> <pre><code>def __init__(\n    self,\n    *,\n    criteria: list[str] | None = None,\n    min_samples: int = 2,\n    alpha: float = 0.05,\n    on_error: str = \"abort\",\n) -&gt; None:\n    super().__init__(\n        security_level=SecurityLevel.UNOFFICIAL,  # ADR-002-B: Immutable policy\n        allow_downgrade=True  # ADR-002-B: Immutable policy\n    )\n    self._criteria = set(criteria) if criteria else None\n    self._min_samples = max(int(min_samples), 2)\n    self._alpha = float(alpha)\n    if on_error not in {\"abort\", \"skip\"}:\n        raise ValueError(\"on_error must be 'abort' or 'skip'\")\n    self._on_error = on_error\n</code></pre>"},{"location":"api-reference/plugins/generated-baselines/#elspeth.plugins.experiments.baseline.criteria_effects.CriteriaEffectsBaselinePlugin-functions","title":"Functions","text":""},{"location":"api-reference/plugins/generated-baselines/#outlierdetectionaggregator","title":"OutlierDetectionAggregator","text":"<p>Configuration Type: <code>outlier_detection</code> Security: \u2705 Trusted Downgrade Source: <code>src/elspeth/plugins/experiments/baseline/outlier_detection.py</code></p> <p>Example Configuration: <pre><code>plugin:\n  type: outlier_detection\n  security_level: OFFICIAL\n  allow_downgrade: true\n</code></pre></p>"},{"location":"api-reference/plugins/generated-baselines/#elspeth.plugins.experiments.baseline.outlier_detection.OutlierDetectionAggregator","title":"OutlierDetectionAggregator","text":"<pre><code>OutlierDetectionAggregator(*, top_n: int = 10, criteria: list[str] | None = None, min_delta: float = 0.0, on_error: str = 'abort')\n</code></pre> <p>               Bases: <code>BasePlugin</code></p> <p>Identify rows with largest score disagreements between baseline and variant.</p> <p>This plugin finds cases where experiments disagree most, helping identify problematic inputs or edge cases. Computes per-row score deltas, sorts by magnitude, and returns configurable top N outliers with full details.</p> <p>Useful for: finding problematic test cases, edge case identification, quality assurance reviews.</p> Source code in <code>elspeth/plugins/experiments/baseline/outlier_detection.py</code> <pre><code>def __init__(\n    self,\n    *,\n    top_n: int = 10,\n    criteria: list[str] | None = None,\n    min_delta: float = 0.0,\n    on_error: str = \"abort\",\n) -&gt; None:\n    super().__init__(\n        security_level=SecurityLevel.UNOFFICIAL,  # ADR-002-B: Immutable policy\n        allow_downgrade=True  # ADR-002-B: Immutable policy\n    )\n    self._top_n = max(int(top_n), 1)\n    self._criteria = set(criteria) if criteria else None\n    self._min_delta = float(min_delta)\n    if on_error not in {\"abort\", \"skip\"}:\n        raise ValueError(\"on_error must be 'abort' or 'skip'\")\n    self._on_error = on_error\n</code></pre>"},{"location":"api-reference/plugins/generated-baselines/#elspeth.plugins.experiments.baseline.outlier_detection.OutlierDetectionAggregator-functions","title":"Functions","text":""},{"location":"api-reference/plugins/generated-baselines/#refereealignmentbaselineplugin","title":"RefereeAlignmentBaselinePlugin","text":"<p>Configuration Type: <code>referee_alignment</code> Security: \u2705 Trusted Downgrade Source: <code>src/elspeth/plugins/experiments/baseline/referee_alignment.py</code></p> <p>Example Configuration: <pre><code>plugin:\n  type: referee_alignment\n  security_level: OFFICIAL\n  allow_downgrade: true\n</code></pre></p>"},{"location":"api-reference/plugins/generated-baselines/#elspeth.plugins.experiments.baseline.referee_alignment.RefereeAlignmentBaselinePlugin","title":"RefereeAlignmentBaselinePlugin","text":"<pre><code>RefereeAlignmentBaselinePlugin(*, referee_fields: list[str] | None = None, score_field: str = 'scores', criteria: list[str] | None = None, min_samples: int = 2, value_mapping: dict[str, float] | None = None, on_error: str = 'abort')\n</code></pre> <p>               Bases: <code>BasePlugin</code></p> <p>Compare LLM scores against human referee/expert judgments.</p> <p>This plugin measures how well LLM scores align with human expert assessments (referees). It computes alignment metrics (mean absolute error, correlation) between baseline and variant to determine if the variant improves agreement with human judgment.</p> <p>Useful for validating that model changes maintain or improve alignment with human expert consensus.</p> Source code in <code>elspeth/plugins/experiments/baseline/referee_alignment.py</code> <pre><code>def __init__(\n    self,\n    *,\n    referee_fields: list[str] | None = None,\n    score_field: str = \"scores\",\n    criteria: list[str] | None = None,\n    min_samples: int = 2,\n    value_mapping: dict[str, float] | None = None,\n    on_error: str = \"abort\",\n) -&gt; None:\n    super().__init__(\n        security_level=SecurityLevel.UNOFFICIAL,  # ADR-002-B: Immutable policy\n        allow_downgrade=True  # ADR-002-B: Immutable policy\n    )\n    self._referee_fields = referee_fields or [\"referee_score\"]\n    self._score_field = score_field\n    self._criteria = set(criteria) if criteria else None\n    self._min_samples = max(int(min_samples), 2)\n    if on_error not in {\"abort\", \"skip\"}:\n        raise ValueError(\"on_error must be 'abort' or 'skip'\")\n    self._on_error = on_error\n\n    # Default value mapping for common string values\n    default_mapping = {\n        \"yes\": 5.0,\n        \"no\": 1.0,\n        \"partially\": 3.0,\n        \"partial\": 3.0,\n        \"n/a\": None,\n        \"na\": None,\n    }\n    self._value_mapping = {**default_mapping, **(value_mapping or {})}\n</code></pre>"},{"location":"api-reference/plugins/generated-baselines/#elspeth.plugins.experiments.baseline.referee_alignment.RefereeAlignmentBaselinePlugin-functions","title":"Functions","text":""},{"location":"api-reference/plugins/generated-baselines/#scoreassumptionsbaselineplugin","title":"ScoreAssumptionsBaselinePlugin","text":"<p>Configuration Type: <code>score_assumptions</code> Security: \u2705 Trusted Downgrade Source: <code>src/elspeth/plugins/experiments/baseline/score_assumptions.py</code></p> <p>Example Configuration: <pre><code>plugin:\n  type: score_assumptions\n  security_level: OFFICIAL\n  allow_downgrade: true\n</code></pre></p>"},{"location":"api-reference/plugins/generated-baselines/#elspeth.plugins.experiments.baseline.score_assumptions.ScoreAssumptionsBaselinePlugin","title":"ScoreAssumptionsBaselinePlugin","text":"<pre><code>ScoreAssumptionsBaselinePlugin(*, criteria: Sequence[str] | None = None, min_samples: int = 3, alpha: float = 0.05, on_error: str = 'abort')\n</code></pre> <p>               Bases: <code>BasePlugin</code></p> <p>Report normality and variance diagnostics for baseline vs. variant scores.</p> Source code in <code>elspeth/plugins/experiments/baseline/score_assumptions.py</code> <pre><code>def __init__(\n    self,\n    *,\n    criteria: Sequence[str] | None = None,\n    min_samples: int = 3,\n    alpha: float = 0.05,\n    on_error: str = \"abort\",\n) -&gt; None:\n    super().__init__(\n        security_level=SecurityLevel.UNOFFICIAL,  # ADR-002-B: Immutable policy\n        allow_downgrade=True  # ADR-002-B: Immutable policy\n    )\n    self._criteria = set(criteria) if criteria else None\n    self._min_samples = max(int(min_samples), 3)\n    self._alpha = float(alpha)\n    if on_error not in {\"abort\", \"skip\"}:\n        raise ValueError(\"on_error must be 'abort' or 'skip'\")\n    self._on_error = on_error\n</code></pre>"},{"location":"api-reference/plugins/generated-baselines/#elspeth.plugins.experiments.baseline.score_assumptions.ScoreAssumptionsBaselinePlugin-functions","title":"Functions","text":""},{"location":"api-reference/plugins/generated-baselines/#scorebayesianbaselineplugin","title":"ScoreBayesianBaselinePlugin","text":"<p>Configuration Type: <code>score_bayesian</code> Security: \u2705 Trusted Downgrade Source: <code>src/elspeth/plugins/experiments/baseline/score_bayesian.py</code></p> <p>Example Configuration: <pre><code>plugin:\n  type: score_bayesian\n  security_level: OFFICIAL\n  allow_downgrade: true\n</code></pre></p>"},{"location":"api-reference/plugins/generated-baselines/#elspeth.plugins.experiments.baseline.score_bayesian.ScoreBayesianBaselinePlugin","title":"ScoreBayesianBaselinePlugin","text":"<pre><code>ScoreBayesianBaselinePlugin(*, criteria: Sequence[str] | None = None, min_samples: int = 2, credible_interval: float = 0.95, on_error: str = 'abort')\n</code></pre> <p>               Bases: <code>BasePlugin</code></p> <p>Estimate posterior probability that a variant beats the baseline.</p> Source code in <code>elspeth/plugins/experiments/baseline/score_bayesian.py</code> <pre><code>def __init__(\n    self,\n    *,\n    criteria: Sequence[str] | None = None,\n    min_samples: int = 2,\n    credible_interval: float = 0.95,\n    on_error: str = \"abort\",\n) -&gt; None:\n    super().__init__(\n        security_level=SecurityLevel.UNOFFICIAL,  # ADR-002-B: Immutable policy\n        allow_downgrade=True  # ADR-002-B: Immutable policy\n    )\n    self._criteria = set(criteria) if criteria else None\n    self._min_samples = max(int(min_samples), 2)\n    self._ci = min(max(float(credible_interval), 0.5), 0.999)\n    if on_error not in {\"abort\", \"skip\"}:\n        raise ValueError(\"on_error must be 'abort' or 'skip'\")\n    self._on_error = on_error\n</code></pre>"},{"location":"api-reference/plugins/generated-baselines/#elspeth.plugins.experiments.baseline.score_bayesian.ScoreBayesianBaselinePlugin-functions","title":"Functions","text":""},{"location":"api-reference/plugins/generated-baselines/#scorecliffsdeltaplugin","title":"ScoreCliffsDeltaPlugin","text":"<p>Configuration Type: <code>score_cliffs_delta</code> Security: \u2705 Trusted Downgrade Source: <code>src/elspeth/plugins/experiments/baseline/score_cliffs_delta.py</code></p> <p>Example Configuration: <pre><code>plugin:\n  type: score_cliffs_delta\n  security_level: OFFICIAL\n  allow_downgrade: true\n</code></pre></p>"},{"location":"api-reference/plugins/generated-baselines/#elspeth.plugins.experiments.baseline.score_cliffs_delta.ScoreCliffsDeltaPlugin","title":"ScoreCliffsDeltaPlugin","text":"<pre><code>ScoreCliffsDeltaPlugin(*, criteria: Sequence[str] | None = None, min_samples: int = 1, on_error: str = 'abort')\n</code></pre> <p>               Bases: <code>BasePlugin</code></p> <p>Compute Cliff's delta effect size between baseline and variant.</p> Source code in <code>elspeth/plugins/experiments/baseline/score_cliffs_delta.py</code> <pre><code>def __init__(\n    self,\n    *,\n    criteria: Sequence[str] | None = None,\n    min_samples: int = 1,\n    on_error: str = \"abort\",\n) -&gt; None:\n    # ADR-002-B: Security policy is immutable and hard-coded in plugin code\n    super().__init__(\n        security_level=SecurityLevel.UNOFFICIAL,  # Baseline analyzers work with experiment results\n        allow_downgrade=True,  # Trusted to operate at lower levels if needed (ADR-005)\n    )\n    self._criteria = set(criteria) if criteria else None\n    self._min_samples = max(int(min_samples), 1)\n    if on_error not in {\"abort\", \"skip\"}:\n        raise ValueError(\"on_error must be 'abort' or 'skip'\")\n    self._on_error = on_error\n</code></pre>"},{"location":"api-reference/plugins/generated-baselines/#elspeth.plugins.experiments.baseline.score_cliffs_delta.ScoreCliffsDeltaPlugin-functions","title":"Functions","text":""},{"location":"api-reference/plugins/generated-baselines/#scoredeltabaselineplugin","title":"ScoreDeltaBaselinePlugin","text":"<p>Configuration Type: <code>score_delta</code> Security: \u2705 Trusted Downgrade Source: <code>src/elspeth/plugins/experiments/baseline/score_delta.py</code></p> <p>Example Configuration: <pre><code>plugin:\n  type: score_delta\n  security_level: OFFICIAL\n  allow_downgrade: true\n</code></pre></p>"},{"location":"api-reference/plugins/generated-baselines/#elspeth.plugins.experiments.baseline.score_delta.ScoreDeltaBaselinePlugin","title":"ScoreDeltaBaselinePlugin","text":"<pre><code>ScoreDeltaBaselinePlugin(*, metric: str = 'mean', criteria: list[str] | None = None)\n</code></pre> <p>               Bases: <code>BasePlugin</code></p> <p>Compare score statistics between baseline and variant.</p> Source code in <code>elspeth/plugins/experiments/baseline/score_delta.py</code> <pre><code>def __init__(\n    self,\n    *,\n    metric: str = \"mean\",\n    criteria: list[str] | None = None,\n) -&gt; None:\n    # ADR-002-B: Security policy is immutable and hard-coded in plugin code\n    super().__init__(\n        security_level=SecurityLevel.UNOFFICIAL,  # Baseline analyzers work with experiment results\n        allow_downgrade=True,  # Trusted to operate at lower levels if needed\n    )\n    self._metric = metric\n    self._criteria = set(criteria) if criteria else None\n</code></pre>"},{"location":"api-reference/plugins/generated-baselines/#elspeth.plugins.experiments.baseline.score_delta.ScoreDeltaBaselinePlugin-functions","title":"Functions","text":""},{"location":"api-reference/plugins/generated-baselines/#scoredistributionaggregator","title":"ScoreDistributionAggregator","text":"<p>Configuration Type: <code>score_distribution</code> Security: \u2705 Trusted Downgrade Source: <code>src/elspeth/plugins/experiments/baseline/score_distribution.py</code></p> <p>Example Configuration: <pre><code>plugin:\n  type: score_distribution\n  security_level: OFFICIAL\n  allow_downgrade: true\n</code></pre></p>"},{"location":"api-reference/plugins/generated-baselines/#elspeth.plugins.experiments.baseline.score_distribution.ScoreDistributionAggregator","title":"ScoreDistributionAggregator","text":"<pre><code>ScoreDistributionAggregator(*, criteria: Sequence[str] | None = None, min_samples: int = 2, on_error: str = 'abort')\n</code></pre> <p>               Bases: <code>BasePlugin</code></p> <p>Assess distribution shifts between baseline and variant deployments.</p> Source code in <code>elspeth/plugins/experiments/baseline/score_distribution.py</code> <pre><code>def __init__(\n    self,\n    *,\n    criteria: Sequence[str] | None = None,\n    min_samples: int = 2,\n    on_error: str = \"abort\",\n) -&gt; None:\n    super().__init__(\n        security_level=SecurityLevel.UNOFFICIAL,  # ADR-002-B: Immutable policy\n        allow_downgrade=True  # ADR-002-B: Immutable policy\n    )\n    self._criteria = set(criteria) if criteria else None\n    self._min_samples = max(int(min_samples), 2)\n    if on_error not in {\"abort\", \"skip\"}:\n        raise ValueError(\"on_error must be 'abort' or 'skip'\")\n    self._on_error = on_error\n</code></pre>"},{"location":"api-reference/plugins/generated-baselines/#elspeth.plugins.experiments.baseline.score_distribution.ScoreDistributionAggregator-functions","title":"Functions","text":""},{"location":"api-reference/plugins/generated-baselines/#scoreflipanalysisaggregator","title":"ScoreFlipAnalysisAggregator","text":"<p>Configuration Type: <code>score_flip_analysis</code> Security: \u2705 Trusted Downgrade Source: <code>src/elspeth/plugins/experiments/baseline/score_flip_analysis.py</code></p> <p>Example Configuration: <pre><code>plugin:\n  type: score_flip_analysis\n  security_level: OFFICIAL\n  allow_downgrade: true\n</code></pre></p>"},{"location":"api-reference/plugins/generated-baselines/#elspeth.plugins.experiments.baseline.score_flip_analysis.ScoreFlipAnalysisAggregator","title":"ScoreFlipAnalysisAggregator","text":"<pre><code>ScoreFlipAnalysisAggregator(*, criteria: list[str] | None = None, pass_threshold: float = 3.0, fail_threshold: float = 2.0, major_change: float = 2.0, on_error: str = 'abort')\n</code></pre> <p>               Bases: <code>BasePlugin</code></p> <p>Analyze score direction changes (flips) between baseline and variant.</p> <p>Identifies fail\u2192pass transitions, pass\u2192fail transitions, major score drops, and major score gains. Useful for understanding where the variant changed scoring patterns most dramatically.</p> <p>Typical use cases: regression detection, improvement tracking, edge case analysis.</p> Source code in <code>elspeth/plugins/experiments/baseline/score_flip_analysis.py</code> <pre><code>def __init__(\n    self,\n    *,\n    criteria: list[str] | None = None,\n    pass_threshold: float = 3.0,\n    fail_threshold: float = 2.0,\n    major_change: float = 2.0,\n    on_error: str = \"abort\",\n) -&gt; None:\n    super().__init__(\n        security_level=SecurityLevel.UNOFFICIAL,  # ADR-002-B: Immutable policy\n        allow_downgrade=True  # ADR-002-B: Immutable policy\n    )\n    self._criteria = set(criteria) if criteria else None\n    self._pass_threshold = float(pass_threshold)\n    self._fail_threshold = float(fail_threshold)\n    self._major_change = float(major_change)\n    if on_error not in {\"abort\", \"skip\"}:\n        raise ValueError(\"on_error must be 'abort' or 'skip'\")\n    self._on_error = on_error\n</code></pre>"},{"location":"api-reference/plugins/generated-baselines/#elspeth.plugins.experiments.baseline.score_flip_analysis.ScoreFlipAnalysisAggregator-functions","title":"Functions","text":""},{"location":"api-reference/plugins/generated-baselines/#scorepracticalbaselineplugin","title":"ScorePracticalBaselinePlugin","text":"<p>Configuration Type: <code>score_practical</code> Security: \u2705 Trusted Downgrade Source: <code>src/elspeth/plugins/experiments/baseline/score_practical.py</code></p> <p>Example Configuration: <pre><code>plugin:\n  type: score_practical\n  security_level: OFFICIAL\n  allow_downgrade: true\n</code></pre></p>"},{"location":"api-reference/plugins/generated-baselines/#elspeth.plugins.experiments.baseline.score_practical.ScorePracticalBaselinePlugin","title":"ScorePracticalBaselinePlugin","text":"<pre><code>ScorePracticalBaselinePlugin(*, criteria: Sequence[str] | None = None, threshold: float = 1.0, success_threshold: float = 4.0, min_samples: int = 1, on_error: str = 'abort')\n</code></pre> <p>               Bases: <code>BasePlugin</code></p> <p>Assess practical significance (meaningful change, NNT, success deltas).</p> Source code in <code>elspeth/plugins/experiments/baseline/score_practical.py</code> <pre><code>def __init__(\n    self,\n    *,\n    criteria: Sequence[str] | None = None,\n    threshold: float = 1.0,\n    success_threshold: float = 4.0,\n    min_samples: int = 1,\n    on_error: str = \"abort\",\n) -&gt; None:\n    super().__init__(\n        security_level=SecurityLevel.UNOFFICIAL,  # ADR-002-B: Immutable policy\n        allow_downgrade=True  # ADR-002-B: Immutable policy\n    )\n    self._criteria = set(criteria) if criteria else None\n    self._threshold = float(threshold)\n    self._success_threshold = float(success_threshold)\n    self._min_samples = max(int(min_samples), 1)\n    if on_error not in {\"abort\", \"skip\"}:\n        raise ValueError(\"on_error must be 'abort' or 'skip'\")\n    self._on_error = on_error\n</code></pre>"},{"location":"api-reference/plugins/generated-baselines/#elspeth.plugins.experiments.baseline.score_practical.ScorePracticalBaselinePlugin-functions","title":"Functions","text":""},{"location":"api-reference/plugins/generated-baselines/#scoresignificancebaselineplugin","title":"ScoreSignificanceBaselinePlugin","text":"<p>Configuration Type: <code>score_significance</code> Security: \u2705 Trusted Downgrade Source: <code>src/elspeth/plugins/experiments/baseline/score_significance.py</code></p> <p>Example Configuration: <pre><code>plugin:\n  type: score_significance\n  security_level: OFFICIAL\n  allow_downgrade: true\n</code></pre></p>"},{"location":"api-reference/plugins/generated-baselines/#elspeth.plugins.experiments.baseline.score_significance.ScoreSignificanceBaselinePlugin","title":"ScoreSignificanceBaselinePlugin","text":"<pre><code>ScoreSignificanceBaselinePlugin(*, criteria: Sequence[str] | None = None, min_samples: int = 2, equal_var: bool = False, adjustment: str = 'none', family_size: int | None = None, on_error: str = 'abort')\n</code></pre> <p>               Bases: <code>BasePlugin</code></p> <p>Compare baseline and variant using effect sizes and t-tests.</p> Source code in <code>elspeth/plugins/experiments/baseline/score_significance.py</code> <pre><code>def __init__(\n    self,\n    *,\n    criteria: Sequence[str] | None = None,\n    min_samples: int = 2,\n    equal_var: bool = False,\n    adjustment: str = \"none\",\n    family_size: int | None = None,\n    on_error: str = \"abort\",\n) -&gt; None:\n    # ADR-002-B: Security policy is immutable and hard-coded in plugin code\n    super().__init__(\n        security_level=SecurityLevel.UNOFFICIAL,  # Baseline analyzers work with experiment results\n        allow_downgrade=True,  # Trusted to operate at lower levels if needed (ADR-005)\n    )\n    self._criteria = set(criteria) if criteria else None\n    self._min_samples = max(int(min_samples), 2)\n    self._equal_var = bool(equal_var)\n    adjustment = (adjustment or \"none\").lower()\n    if adjustment not in {\"none\", \"bonferroni\", \"fdr\"}:\n        adjustment = \"none\"\n    self._adjustment = adjustment\n    self._family_size = int(family_size) if family_size else None\n    if on_error not in {\"abort\", \"skip\"}:\n        raise ValueError(\"on_error must be 'abort' or 'skip'\")\n    self._on_error = on_error\n</code></pre>"},{"location":"api-reference/plugins/generated-baselines/#elspeth.plugins.experiments.baseline.score_significance.ScoreSignificanceBaselinePlugin-functions","title":"Functions","text":""},{"location":"api-reference/plugins/generated-datasources/","title":"Datasources API (Auto-Generated)","text":"<p>Auto-Generated Documentation</p> <p>This file is automatically generated from source code. Do not edit manually.</p> <p>Generated from: <code>scripts/generate_plugin_docs.py</code> Last generated: Run <code>make docs-generate</code> to update</p> <p>API documentation for 1 datasources that load data into experiments.</p>"},{"location":"api-reference/plugins/generated-datasources/#blobdatasource","title":"BlobDataSource","text":"<p>Configuration Type: <code>blob</code> Security: \u2705 Trusted Downgrade Source: <code>src/elspeth/plugins/nodes/sources/blob.py</code></p> <p>Example Configuration: <pre><code>datasource:\n  type: blob\n  security_level: OFFICIAL\n  allow_downgrade: true\n</code></pre></p>"},{"location":"api-reference/plugins/generated-datasources/#elspeth.plugins.nodes.sources.blob.BlobDataSource","title":"BlobDataSource","text":"<pre><code>BlobDataSource(*, config_path: str, profile: str = 'default', pandas_kwargs: dict[str, Any] | None = None, on_error: str = 'abort', determinism_level: DeterminismLevel | None = None, retain_local: bool, retain_local_path: str | None = None)\n</code></pre> <p>               Bases: <code>BasePlugin</code>, <code>DataSource</code></p> <p>Read CSV data from Azure Blob Storage using configured profiles.</p> <p>Inherits from BasePlugin to provide security enforcement (ADR-004).</p> Source code in <code>elspeth/plugins/nodes/sources/blob.py</code> <pre><code>def __init__(\n    self,\n    *,\n    config_path: str,\n    profile: str = \"default\",\n    pandas_kwargs: dict[str, Any] | None = None,\n    on_error: str = \"abort\",\n    determinism_level: DeterminismLevel | None = None,\n    retain_local: bool,  # REQUIRED - no default\n    retain_local_path: str | None = None,\n) -&gt; None:\n    # Initialize BasePlugin with security level and downgrade policy (ADR-002-B)\n    super().__init__(\n        security_level=SecurityLevel.UNOFFICIAL,  # ADR-002-B: Immutable policy\n        allow_downgrade=True,  # ADR-002-B: Immutable policy\n    )\n\n    self.config_path = config_path\n    self.profile = profile\n    self.pandas_kwargs = pandas_kwargs or {}\n    if on_error not in {\"abort\", \"skip\"}:\n        raise ValueError(\"on_error must be 'abort' or 'skip'\")\n    self.on_error = on_error\n    # security_level is now set by BasePlugin.__init__() (ADR-004)\n    self.determinism_level = ensure_determinism_level(determinism_level)\n    self.retain_local = retain_local\n    self.retain_local_path = retain_local_path\n</code></pre>"},{"location":"api-reference/plugins/generated-datasources/#elspeth.plugins.nodes.sources.blob.BlobDataSource-functions","title":"Functions","text":""},{"location":"api-reference/plugins/generated-middlewares/","title":"Middleware API (Auto-Generated)","text":"<p>Auto-Generated Documentation</p> <p>This file is automatically generated from source code. Do not edit manually.</p> <p>Generated from: <code>scripts/generate_plugin_docs.py</code> Last generated: Run <code>make docs-generate</code> to update</p> <p>API documentation for 6 middleware that intercept and modify LLM requests/responses.</p>"},{"location":"api-reference/plugins/generated-middlewares/#auditmiddleware","title":"AuditMiddleware","text":"<p>Configuration Type: <code>audit</code> Security: \u2705 Trusted Downgrade Source: <code>src/elspeth/plugins/nodes/transforms/llm/middleware/audit.py</code></p> <p>Example Configuration: <pre><code>plugin:\n  type: audit\n  security_level: OFFICIAL\n  allow_downgrade: true\n</code></pre></p>"},{"location":"api-reference/plugins/generated-middlewares/#elspeth.plugins.nodes.transforms.llm.middleware.audit.AuditMiddleware","title":"AuditMiddleware","text":"<pre><code>AuditMiddleware(*, include_prompts: bool = False, channel: str | None = None)\n</code></pre> <p>               Bases: <code>BasePlugin</code>, <code>LLMMiddleware</code></p> <p>Structured audit logger for LLM requests and responses.</p> <p>Parameters:</p> Name Type Description Default <code>include_prompts</code> <code>bool</code> <p>Whether to include full prompts in audit logs (default: False).</p> <code>False</code> <code>channel</code> <code>str | None</code> <p>Logger channel name (default: 'elspeth.audit').</p> <code>None</code> <ul> <li>Emits request metadata (and optionally prompts) before dispatch.</li> <li>Emits response metrics (and optionally content) after completion.</li> <li>Channel name is configurable via options or defaults to 'elspeth.audit'.</li> </ul> Source code in <code>elspeth/plugins/nodes/transforms/llm/middleware/audit.py</code> <pre><code>def __init__(\n    self,\n    *,\n    include_prompts: bool = False,\n    channel: str | None = None,\n):\n    super().__init__(\n        security_level=SecurityLevel.UNOFFICIAL,  # ADR-002-B: Immutable policy\n        allow_downgrade=True,  # ADR-002-B: Immutable policy\n    )\n    self.include_prompts = include_prompts\n    self.channel = channel or \"elspeth.audit\"\n</code></pre>"},{"location":"api-reference/plugins/generated-middlewares/#elspeth.plugins.nodes.transforms.llm.middleware.audit.AuditMiddleware-functions","title":"Functions","text":""},{"location":"api-reference/plugins/generated-middlewares/#azurecontentsafetymiddleware","title":"AzureContentSafetyMiddleware","text":"<p>Configuration Type: <code>azure_content_safety</code> Security: \u2705 Trusted Downgrade Source: <code>src/elspeth/plugins/nodes/transforms/llm/middleware/azure_content_safety.py</code></p> <p>Example Configuration: <pre><code>plugin:\n  type: azure_content_safety\n  security_level: OFFICIAL\n  allow_downgrade: true\n</code></pre></p>"},{"location":"api-reference/plugins/generated-middlewares/#elspeth.plugins.nodes.transforms.llm.middleware.azure_content_safety.AzureContentSafetyMiddleware","title":"AzureContentSafetyMiddleware","text":"<pre><code>AzureContentSafetyMiddleware(*, endpoint: str, key: str | None = None, key_env: str | None = None, api_version: str | None = None, categories: Sequence[str] | None = None, severity_threshold: int = 4, on_violation: str = 'abort', mask: str = '[CONTENT BLOCKED]', channel: str | None = None, on_error: str = 'abort', retry_attempts: int = 3)\n</code></pre> <p>               Bases: <code>BasePlugin</code>, <code>LLMMiddleware</code></p> <p>Use Azure Content Safety service to screen prompts before submission.</p> Source code in <code>elspeth/plugins/nodes/transforms/llm/middleware/azure_content_safety.py</code> <pre><code>def __init__(\n    self,\n    *,\n    endpoint: str,\n    key: str | None = None,\n    key_env: str | None = None,\n    api_version: str | None = None,\n    categories: Sequence[str] | None = None,\n    severity_threshold: int = 4,\n    on_violation: str = \"abort\",\n    mask: str = \"[CONTENT BLOCKED]\",\n    channel: str | None = None,\n    on_error: str = \"abort\",\n    retry_attempts: int = 3,\n) -&gt; None:\n    super().__init__(\n        security_level=SecurityLevel.UNOFFICIAL,  # ADR-002-B: Immutable policy\n        allow_downgrade=True,  # ADR-002-B: Immutable policy\n    )\n    if not endpoint:\n        raise ValueError(\"Azure Content Safety requires an endpoint\")\n    self.endpoint = endpoint.rstrip(\"/\")\n    key_value = key or (os.environ.get(key_env) if key_env else None)\n    if not key_value:\n        raise ValueError(\"Azure Content Safety requires an API key or key_env\")\n    self.key = key_value\n    self.api_version = api_version or \"2023-10-01\"\n    self.categories = list(categories or [\"Hate\", \"Violence\", \"SelfHarm\", \"Sexual\"])\n    self.threshold = max(0, min(int(severity_threshold), 7))\n    mode = (on_violation or \"abort\").lower()\n    if mode not in {\"abort\", \"mask\", \"log\"}:\n        mode = \"abort\"\n    self.mode = mode\n    self.mask = mask\n    self.channel = channel or \"elspeth.azure_content_safety\"\n    handler = (on_error or \"abort\").lower()\n    if handler not in {\"abort\", \"skip\"}:\n        handler = \"abort\"\n    self.on_error = handler\n    # Configure bounded retry attempts for Content Safety calls\n    try:\n        self.retry_attempts = max(1, int(retry_attempts))\n    except (ValueError, TypeError):\n        self.retry_attempts = 3\n</code></pre>"},{"location":"api-reference/plugins/generated-middlewares/#elspeth.plugins.nodes.transforms.llm.middleware.azure_content_safety.AzureContentSafetyMiddleware-functions","title":"Functions","text":""},{"location":"api-reference/plugins/generated-middlewares/#classifiedmaterialmiddleware","title":"ClassifiedMaterialMiddleware","text":"<p>Configuration Type: <code>classified_material</code> Security: \u2705 Trusted Downgrade Source: <code>src/elspeth/plugins/nodes/transforms/llm/middleware/classified_material.py</code></p> <p>Example Configuration: <pre><code>plugin:\n  type: classified_material\n  security_level: OFFICIAL\n  allow_downgrade: true\n</code></pre></p>"},{"location":"api-reference/plugins/generated-middlewares/#elspeth.plugins.nodes.transforms.llm.middleware.classified_material.ClassifiedMaterialMiddleware","title":"ClassifiedMaterialMiddleware","text":"<pre><code>ClassifiedMaterialMiddleware(*, classification_markings: Sequence[str] | None = None, on_violation: str = 'abort', mask: str = '[CLASSIFIED]', channel: str | None = None, case_sensitive: bool = False, include_defaults: bool = True, include_optional: bool = False, fuzzy_matching: bool = True, severity_scoring: bool = True, min_severity: str = 'LOW', check_code_fences: bool = True, require_allcaps_confidence: bool = False)\n</code></pre> <p>               Bases: <code>BasePlugin</code>, <code>LLMMiddleware</code></p> <p>Detect and block classified material markings in prompts with advanced fuzzy matching.</p> <p>Features: - Unicode normalization (NFKC) and homoglyph detection - Fuzzy regex matching for spacing variants (A.U.S.T.E.O) - Severity scoring (HIGH/MEDIUM/LOW) - False-positive dampers (code fence detection, all-caps requirement) - REL TO country-list parsing</p> <p>Initialize enhanced classified material middleware.</p> <p>Parameters:</p> Name Type Description Default <code>classification_markings</code> <code>Sequence[str] | None</code> <p>Custom classification markings to detect</p> <code>None</code> <code>on_violation</code> <code>str</code> <p>Action on detection - 'abort', 'mask', or 'log'</p> <code>'abort'</code> <code>mask</code> <code>str</code> <p>Replacement text when masking</p> <code>'[CLASSIFIED]'</code> <code>channel</code> <code>str | None</code> <p>Logging channel name</p> <code>None</code> <code>case_sensitive</code> <code>bool</code> <p>Whether matching should be case-sensitive</p> <code>False</code> <code>include_defaults</code> <code>bool</code> <p>Whether to include default high-signal markings</p> <code>True</code> <code>include_optional</code> <code>bool</code> <p>Whether to include optional low-signal markings (OFFICIAL)</p> <code>False</code> <code>fuzzy_matching</code> <code>bool</code> <p>Enable fuzzy regex matching for spacing variants</p> <code>True</code> <code>severity_scoring</code> <code>bool</code> <p>Enable severity scoring (HIGH/MEDIUM/LOW)</p> <code>True</code> <code>min_severity</code> <code>str</code> <p>Minimum severity to trigger violation (HIGH/MEDIUM/LOW)</p> <code>'LOW'</code> <code>check_code_fences</code> <code>bool</code> <p>Apply false-positive dampers for code fences</p> <code>True</code> <code>require_allcaps_confidence</code> <code>bool</code> <p>Require ALL-CAPS or proximity to trigger on single words</p> <code>False</code> Source code in <code>elspeth/plugins/nodes/transforms/llm/middleware/classified_material.py</code> <pre><code>def __init__(\n    self,\n    *,\n    classification_markings: Sequence[str] | None = None,\n    on_violation: str = \"abort\",\n    mask: str = \"[CLASSIFIED]\",\n    channel: str | None = None,\n    case_sensitive: bool = False,\n    include_defaults: bool = True,\n    include_optional: bool = False,\n    fuzzy_matching: bool = True,\n    severity_scoring: bool = True,\n    min_severity: str = \"LOW\",\n    check_code_fences: bool = True,\n    require_allcaps_confidence: bool = False,\n):\n    \"\"\"Initialize enhanced classified material middleware.\n\n    Args:\n        classification_markings: Custom classification markings to detect\n        on_violation: Action on detection - 'abort', 'mask', or 'log'\n        mask: Replacement text when masking\n        channel: Logging channel name\n        case_sensitive: Whether matching should be case-sensitive\n        include_defaults: Whether to include default high-signal markings\n        include_optional: Whether to include optional low-signal markings (OFFICIAL)\n        fuzzy_matching: Enable fuzzy regex matching for spacing variants\n        severity_scoring: Enable severity scoring (HIGH/MEDIUM/LOW)\n        min_severity: Minimum severity to trigger violation (HIGH/MEDIUM/LOW)\n        check_code_fences: Apply false-positive dampers for code fences\n        require_allcaps_confidence: Require ALL-CAPS or proximity to trigger on single words\n    \"\"\"\n    super().__init__(\n        security_level=SecurityLevel.UNOFFICIAL,  # ADR-002-B: Immutable policy\n        allow_downgrade=True,  # ADR-002-B: Immutable policy\n    )\n    mode = (on_violation or \"abort\").lower()\n    if mode not in {\"abort\", \"mask\", \"log\"}:\n        mode = \"abort\"\n    self.mode = mode\n    self.mask = mask\n    self.channel = channel or \"elspeth.classified_material\"\n    self.case_sensitive = case_sensitive\n    self.fuzzy_matching = fuzzy_matching\n    self.severity_scoring = severity_scoring\n    self.min_severity = min_severity.upper() if min_severity else \"LOW\"\n    self.check_code_fences = check_code_fences\n    self.require_allcaps_confidence = require_allcaps_confidence\n\n    # Build marking list\n    markings = []\n    if include_defaults:\n        markings.extend(self.DEFAULT_MARKINGS)\n    if include_optional:\n        markings.extend(self.OPTIONAL_LOW_SIGNAL)\n    if classification_markings:\n        markings.extend(classification_markings)\n\n    # Normalize for matching\n    if not case_sensitive:\n        self.markings = [m.upper() for m in markings]\n    else:\n        self.markings = list(markings)\n\n    # Compile regex patterns if fuzzy matching enabled\n    self.regex_compiled: dict[str, re.Pattern[str]] = {}\n    if self.fuzzy_matching:\n        for name, pattern in self.REGEX_PATTERNS.items():\n            try:\n                self.regex_compiled[name] = re.compile(pattern, re.IGNORECASE)\n            except re.error as exc:\n                logger.warning(\"[%s] Invalid regex pattern '%s': %s\", self.channel, name, exc)\n</code></pre>"},{"location":"api-reference/plugins/generated-middlewares/#elspeth.plugins.nodes.transforms.llm.middleware.classified_material.ClassifiedMaterialMiddleware-functions","title":"Functions","text":""},{"location":"api-reference/plugins/generated-middlewares/#healthmonitormiddleware","title":"HealthMonitorMiddleware","text":"<p>Configuration Type: <code>health_monitor</code> Security: \u2705 Trusted Downgrade Source: <code>src/elspeth/plugins/nodes/transforms/llm/middleware/health_monitor.py</code></p> <p>Example Configuration: <pre><code>plugin:\n  type: health_monitor\n  security_level: OFFICIAL\n  allow_downgrade: true\n</code></pre></p>"},{"location":"api-reference/plugins/generated-middlewares/#elspeth.plugins.nodes.transforms.llm.middleware.health_monitor.HealthMonitorMiddleware","title":"HealthMonitorMiddleware","text":"<pre><code>HealthMonitorMiddleware(*, heartbeat_interval: float = 60.0, stats_window: int = 50, channel: str | None = None, include_latency: bool = True)\n</code></pre> <p>               Bases: <code>BasePlugin</code>, <code>LLMMiddleware</code></p> <p>Emit heartbeat logs summarising middleware activity.</p> <p>Parameters:</p> Name Type Description Default <code>heartbeat_interval</code> <code>float</code> <p>Seconds between heartbeat emissions (default: 60.0).</p> <code>60.0</code> <code>stats_window</code> <code>int</code> <p>Number of recent requests to track for statistics (default: 50).</p> <code>50</code> <code>channel</code> <code>str | None</code> <p>Logger channel name (default: 'elspeth.health').</p> <code>None</code> <code>include_latency</code> <code>bool</code> <p>Whether to include latency stats in heartbeat (default: True).</p> <code>True</code> Source code in <code>elspeth/plugins/nodes/transforms/llm/middleware/health_monitor.py</code> <pre><code>def __init__(\n    self,\n    *,\n    heartbeat_interval: float = 60.0,\n    stats_window: int = 50,\n    channel: str | None = None,\n    include_latency: bool = True,\n) -&gt; None:\n    super().__init__(\n        security_level=SecurityLevel.UNOFFICIAL,  # ADR-002-B: Immutable policy\n        allow_downgrade=True,  # ADR-002-B: Immutable policy\n    )\n    if heartbeat_interval &lt; 0:\n        raise ValueError(\"heartbeat_interval must be non-negative\")\n    self.interval = float(heartbeat_interval)\n    self.window = max(int(stats_window), 1)\n    self.channel = channel or \"elspeth.health\"\n    self.include_latency = include_latency\n    self._lock = threading.Lock()\n    self._latencies: deque[float] = deque(maxlen=self.window)\n    self._inflight: dict[int, float] = {}\n    self._total_requests = 0\n    self._total_failures = 0\n    self._last_heartbeat = time.monotonic()\n</code></pre>"},{"location":"api-reference/plugins/generated-middlewares/#elspeth.plugins.nodes.transforms.llm.middleware.health_monitor.HealthMonitorMiddleware-functions","title":"Functions","text":""},{"location":"api-reference/plugins/generated-middlewares/#piishieldmiddleware","title":"PIIShieldMiddleware","text":"<p>Configuration Type: <code>pii_shield</code> Security: \u2705 Trusted Downgrade Source: <code>src/elspeth/plugins/nodes/transforms/llm/middleware/pii_shield.py</code></p> <p>Example Configuration: <pre><code>plugin:\n  type: pii_shield\n  security_level: OFFICIAL\n  allow_downgrade: true\n</code></pre></p>"},{"location":"api-reference/plugins/generated-middlewares/#elspeth.plugins.nodes.transforms.llm.middleware.pii_shield.PIIShieldMiddleware","title":"PIIShieldMiddleware","text":"<pre><code>PIIShieldMiddleware(*, patterns: Sequence[dict[str, Any]] | None = None, on_violation: str = 'abort', mask: str = '[PII REDACTED]', channel: str | None = None, include_defaults: bool = True, severity_scoring: bool = True, min_severity: str = 'LOW', checksum_validation: bool = True, context_boosting: bool = True, context_suppression: bool = True, blind_review_mode: bool = False, redaction_salt: str | None = None, bsb_account_window: int = 80)\n</code></pre> <p>               Bases: <code>BasePlugin</code>, <code>LLMMiddleware</code></p> <p>Enhanced PII detection with blind review, checksum validation, and severity scoring.</p> <p>Features: - Checksum validation for Australian identifiers (TFN, ABN, ACN, Medicare) - Luhn algorithm validation for credit cards - Severity classification (HIGH/MEDIUM/LOW) - Context boosting (proximity to strong tokens like \"tfn\", \"medicare\") - Context suppression (URLs, code blocks, hex strings) - Redaction with deterministic pseudonym generation (SHA-256) - Structured output for blind review routing - BSB+Account combo detection - ARBN pattern detection</p> <p>Initialize enhanced PII shield middleware.</p> <p>Parameters:</p> Name Type Description Default <code>patterns</code> <code>Sequence[dict[str, Any]] | None</code> <p>Custom PII patterns (dicts with 'name', 'regex', 'severity', etc.)</p> <code>None</code> <code>on_violation</code> <code>str</code> <p>Action on detection - 'abort', 'mask', or 'log'</p> <code>'abort'</code> <code>mask</code> <code>str</code> <p>Replacement text when masking (not used in blind review)</p> <code>'[PII REDACTED]'</code> <code>channel</code> <code>str | None</code> <p>Logging channel name</p> <code>None</code> <code>include_defaults</code> <code>bool</code> <p>Whether to include default PII patterns</p> <code>True</code> <code>severity_scoring</code> <code>bool</code> <p>Enable severity classification</p> <code>True</code> <code>min_severity</code> <code>str</code> <p>Minimum severity to trigger violation (HIGH/MEDIUM/LOW)</p> <code>'LOW'</code> <code>checksum_validation</code> <code>bool</code> <p>Validate Australian identifiers with checksums</p> <code>True</code> <code>context_boosting</code> <code>bool</code> <p>Use proximity to strong tokens to boost confidence</p> <code>True</code> <code>context_suppression</code> <code>bool</code> <p>Suppress matches in URLs, code blocks, etc.</p> <code>True</code> <code>blind_review_mode</code> <code>bool</code> <p>Enable blind review routing (HIGH+MEDIUM \u2192 manual)</p> <code>False</code> <code>redaction_salt</code> <code>str | None</code> <p>Salt for deterministic pseudonym generation</p> <code>None</code> <code>bsb_account_window</code> <code>int</code> <p>Max distance (chars) for BSB+Account combo detection</p> <code>80</code> Source code in <code>elspeth/plugins/nodes/transforms/llm/middleware/pii_shield.py</code> <pre><code>def __init__(\n    self,\n    *,\n    patterns: Sequence[dict[str, Any]] | None = None,\n    on_violation: str = \"abort\",\n    mask: str = \"[PII REDACTED]\",\n    channel: str | None = None,\n    include_defaults: bool = True,\n    severity_scoring: bool = True,\n    min_severity: str = \"LOW\",\n    checksum_validation: bool = True,\n    context_boosting: bool = True,\n    context_suppression: bool = True,\n    blind_review_mode: bool = False,\n    redaction_salt: str | None = None,\n    bsb_account_window: int = 80,\n):\n    \"\"\"Initialize enhanced PII shield middleware.\n\n    Args:\n        patterns: Custom PII patterns (dicts with 'name', 'regex', 'severity', etc.)\n        on_violation: Action on detection - 'abort', 'mask', or 'log'\n        mask: Replacement text when masking (not used in blind review)\n        channel: Logging channel name\n        include_defaults: Whether to include default PII patterns\n        severity_scoring: Enable severity classification\n        min_severity: Minimum severity to trigger violation (HIGH/MEDIUM/LOW)\n        checksum_validation: Validate Australian identifiers with checksums\n        context_boosting: Use proximity to strong tokens to boost confidence\n        context_suppression: Suppress matches in URLs, code blocks, etc.\n        blind_review_mode: Enable blind review routing (HIGH+MEDIUM \u2192 manual)\n        redaction_salt: Salt for deterministic pseudonym generation\n        bsb_account_window: Max distance (chars) for BSB+Account combo detection\n    \"\"\"\n    super().__init__(\n        security_level=SecurityLevel.UNOFFICIAL,  # ADR-002-B: Immutable policy\n        allow_downgrade=True,  # ADR-002-B: Immutable policy\n    )\n    mode = (on_violation or \"abort\").lower()\n    if mode not in {\"abort\", \"mask\", \"log\"}:\n        mode = \"abort\"\n    self.mode = mode\n    self.mask = mask\n    self.channel = channel or \"elspeth.pii_shield\"\n    self.severity_scoring = severity_scoring\n    self.min_severity = min_severity.upper() if min_severity else \"HIGH\"\n    self.checksum_validation = checksum_validation\n    self.context_boosting = context_boosting\n    self.context_suppression = context_suppression\n    self.blind_review_mode = blind_review_mode\n    self.redaction_salt = redaction_salt or os.environ.get(\"PII_REDACTION_SALT\", \"elspeth-default-salt\")\n    self.bsb_account_window = bsb_account_window\n\n    # Build pattern list\n    all_patterns = []\n    if include_defaults:\n        all_patterns.extend(self.DEFAULT_PATTERNS)\n    if patterns:\n        all_patterns.extend(patterns)\n\n    # Compile regex patterns with metadata\n    self.patterns: list[tuple[str, re.Pattern[str], dict[str, Any]]] = []\n    for pattern_def in all_patterns:\n        try:\n            # Cast pattern_def values from object to proper types\n            regex_str = str(pattern_def[\"regex\"])\n            name_str = str(pattern_def[\"name\"])\n            compiled = re.compile(regex_str)\n            metadata: dict[str, Any] = {\n                \"severity\": pattern_def.get(\"severity\", \"HIGH\"),\n                \"validator\": pattern_def.get(\"validator\"),\n                \"requires_context\": pattern_def.get(\"requires_context\", False),\n                \"context_tokens\": pattern_def.get(\"context_tokens\", []),\n            }\n            self.patterns.append((name_str, compiled, metadata))\n        except re.error as exc:\n            logger.warning(\n                \"[%s] Invalid regex pattern '%s': %s\",\n                self.channel,\n                pattern_def.get(\"name\", \"unknown\"),\n                exc,\n            )\n</code></pre>"},{"location":"api-reference/plugins/generated-middlewares/#elspeth.plugins.nodes.transforms.llm.middleware.pii_shield.PIIShieldMiddleware-functions","title":"Functions","text":""},{"location":"api-reference/plugins/generated-middlewares/#promptshieldmiddleware","title":"PromptShieldMiddleware","text":"<p>Configuration Type: <code>prompt_shield</code> Security: \u2705 Trusted Downgrade Source: <code>src/elspeth/plugins/nodes/transforms/llm/middleware/prompt_shield.py</code></p> <p>Example Configuration: <pre><code>plugin:\n  type: prompt_shield\n  security_level: OFFICIAL\n  allow_downgrade: true\n</code></pre></p>"},{"location":"api-reference/plugins/generated-middlewares/#elspeth.plugins.nodes.transforms.llm.middleware.prompt_shield.PromptShieldMiddleware","title":"PromptShieldMiddleware","text":"<pre><code>PromptShieldMiddleware(*, denied_terms: Sequence[str] | None = None, mask: str = '[REDACTED]', on_violation: str = 'abort', channel: str | None = None)\n</code></pre> <p>               Bases: <code>BasePlugin</code>, <code>LLMMiddleware</code></p> <p>Basic middleware that masks or blocks unsafe prompts before sending to the LLM.</p> <p>Parameters:</p> Name Type Description Default <code>denied_terms</code> <code>Sequence[str] | None</code> <p>List of terms to detect and block/mask.</p> <code>None</code> <code>mask</code> <code>str</code> <p>Replacement text for masked terms (default: '[REDACTED]').</p> <code>'[REDACTED]'</code> <code>on_violation</code> <code>str</code> <p>Action to take ('abort', 'mask', or 'log', default: 'abort').</p> <code>'abort'</code> <code>channel</code> <code>str | None</code> <p>Logger channel name (default: 'elspeth.prompt_shield').</p> <code>None</code> Source code in <code>elspeth/plugins/nodes/transforms/llm/middleware/prompt_shield.py</code> <pre><code>def __init__(\n    self,\n    *,\n    denied_terms: Sequence[str] | None = None,\n    mask: str = \"[REDACTED]\",\n    on_violation: str = \"abort\",\n    channel: str | None = None,\n):\n    super().__init__(\n        security_level=SecurityLevel.UNOFFICIAL,  # ADR-002-B: Immutable policy\n        allow_downgrade=True,  # ADR-002-B: Immutable policy\n    )\n    self.denied_terms = [term.lower() for term in denied_terms or []]\n    self.mask = mask\n    mode = (on_violation or \"abort\").lower()\n    if mode not in {\"abort\", \"mask\", \"log\"}:\n        mode = \"abort\"\n    self.mode = mode\n    self.channel = channel or \"elspeth.prompt_shield\"\n</code></pre>"},{"location":"api-reference/plugins/generated-middlewares/#elspeth.plugins.nodes.transforms.llm.middleware.prompt_shield.PromptShieldMiddleware-functions","title":"Functions","text":""},{"location":"api-reference/plugins/generated-row-experiments/","title":"Row Experiment Plugins API (Auto-Generated)","text":"<p>Auto-Generated Documentation</p> <p>This file is automatically generated from source code. Do not edit manually.</p> <p>Generated from: <code>scripts/generate_plugin_docs.py</code> Last generated: Run <code>make docs-generate</code> to update</p> <p>API documentation for 1 row experiment plugins that process individual rows during experiments.</p>"},{"location":"api-reference/plugins/generated-row-experiments/#scoreextractorplugin","title":"ScoreExtractorPlugin","text":"<p>Configuration Type: <code>score_extractor</code> Security: \u2705 Trusted Downgrade Source: <code>src/elspeth/plugins/experiments/row/score_extractor.py</code></p> <p>Example Configuration: <pre><code>plugin:\n  type: score_extractor\n  security_level: OFFICIAL\n  allow_downgrade: true\n</code></pre></p>"},{"location":"api-reference/plugins/generated-row-experiments/#elspeth.plugins.experiments.row.score_extractor.ScoreExtractorPlugin","title":"ScoreExtractorPlugin","text":"<pre><code>ScoreExtractorPlugin(*, key: str = 'score', criteria: list[str] | None = None, parse_json_content: bool = True, allow_missing: bool = False, threshold: float | None = None, threshold_mode: str = 'gte', flag_field: str = 'score_flags')\n</code></pre> <p>               Bases: <code>BasePlugin</code></p> <p>Extract numeric scores from LLM responses.</p> <p>The plugin inspects the per-criteria response payload for numeric values under the configured key (default: <code>score</code>). Values are normalised to <code>float</code> whenever possible. When <code>threshold</code> is supplied the plugin also flags rows that meet the threshold for downstream aggregators.</p> Source code in <code>elspeth/plugins/experiments/row/score_extractor.py</code> <pre><code>def __init__(\n    self,\n    *,\n    key: str = \"score\",\n    criteria: list[str] | None = None,\n    parse_json_content: bool = True,\n    allow_missing: bool = False,\n    threshold: float | None = None,\n    threshold_mode: str = \"gte\",\n    flag_field: str = \"score_flags\",\n) -&gt; None:\n    # ADR-002-B: Security policy is immutable and hard-coded in plugin code\n    super().__init__(\n        security_level=SecurityLevel.UNOFFICIAL,  # Row plugins process experiment results\n        allow_downgrade=True  # ADR-005: Row computation plugin trusted to downgrade\n    )\n    self._key = key\n    self._criteria = set(criteria) if criteria else None\n    self._parse_json = parse_json_content\n    self._allow_missing = allow_missing\n    self._threshold = threshold\n    self._threshold_mode = threshold_mode\n    self._flag_field = flag_field\n</code></pre>"},{"location":"api-reference/plugins/generated-row-experiments/#elspeth.plugins.experiments.row.score_extractor.ScoreExtractorPlugin-functions","title":"Functions","text":""},{"location":"api-reference/plugins/generated-sinks/","title":"Sinks API (Auto-Generated)","text":"<p>Auto-Generated Documentation</p> <p>This file is automatically generated from source code. Do not edit manually.</p> <p>Generated from: <code>scripts/generate_plugin_docs.py</code> Last generated: Run <code>make docs-generate</code> to update</p> <p>API documentation for 11 sinks that write experiment outputs.</p>"},{"location":"api-reference/plugins/generated-sinks/#analyticsreportsink","title":"AnalyticsReportSink","text":"<p>Configuration Type: <code>analytics_report</code> Security: \u2705 Trusted Downgrade Source: <code>src/elspeth/plugins/nodes/sinks/analytics_report.py</code></p> <p>Example Configuration: <pre><code>sinks:\n  - type: analytics_report\n  security_level: OFFICIAL\n  allow_downgrade: true\n</code></pre></p>"},{"location":"api-reference/plugins/generated-sinks/#elspeth.plugins.nodes.sinks.analytics_report.AnalyticsReportSink","title":"AnalyticsReportSink","text":"<pre><code>AnalyticsReportSink(*, base_path: str, file_stem: str = 'analytics_report', formats: Sequence[str] | None = None, include_metadata: bool = True, include_aggregates: bool = True, include_comparisons: bool = True, on_error: str = 'abort')\n</code></pre> <p>               Bases: <code>BasePlugin</code>, <code>ResultSink</code></p> <p>Generate a JSON analytics report (and optional Markdown) summarizing results and failures.</p> <p>Inherits from BasePlugin to provide security enforcement (ADR-004).</p> Source code in <code>elspeth/plugins/nodes/sinks/analytics_report.py</code> <pre><code>def __init__(\n    self,\n    *,\n    base_path: str,\n    file_stem: str = \"analytics_report\",\n    formats: Sequence[str] | None = None,\n    include_metadata: bool = True,\n    include_aggregates: bool = True,\n    include_comparisons: bool = True,\n    on_error: str = \"abort\",\n) -&gt; None:\n    # Initialize BasePlugin with security level and downgrade policy (ADR-002-B, ADR-005)\n    super().__init__(\n        security_level=SecurityLevel.UNOFFICIAL,  # ADR-002-B: Immutable policy\n        allow_downgrade=True,  # ADR-002-B: Immutable policy\n    )\n    self.base_path = Path(base_path)\n    self.file_stem = file_stem or \"analytics_report\"\n    selected = []\n    for fmt in formats or [\"json\", \"md\"]:\n        normalized = (fmt or \"\").strip().lower()\n        if normalized == \"markdown\":\n            normalized = \"md\"\n        if normalized:\n            selected.append(normalized)\n    self.formats = [fmt for fmt in selected if fmt in {\"json\", \"md\"}] or [\"json\"]\n    if on_error not in {\"abort\", \"skip\"}:\n        raise ValueError(\"on_error must be 'abort' or 'skip'\")\n    self.on_error = on_error\n    self.include_metadata = include_metadata\n    self.include_aggregates = include_aggregates\n    self.include_comparisons = include_comparisons\n    self._last_written_files: list[Path] = []\n    # Runtime data classification tracking (separate from sink's security clearance)\n    # security_level (from BasePlugin) = sink's clearance level\n    # _artifact_security_level = runtime classification of written data (for artifact metadata)\n    self._artifact_security_level: SecurityLevel | None = None\n    self._artifact_determinism_level: DeterminismLevel | None = None\n</code></pre>"},{"location":"api-reference/plugins/generated-sinks/#elspeth.plugins.nodes.sinks.analytics_report.AnalyticsReportSink-functions","title":"Functions","text":""},{"location":"api-reference/plugins/generated-sinks/#elspeth.plugins.nodes.sinks.analytics_report.AnalyticsReportSink.write","title":"write","text":"<pre><code>write(results: dict[str, Any], *, metadata: dict[str, Any] | None = None) -&gt; None\n</code></pre> Source code in <code>elspeth/plugins/nodes/sinks/analytics_report.py</code> <pre><code>def write(self, results: dict[str, Any], *, metadata: dict[str, Any] | None = None) -&gt; None:\n    try:\n        summary = self._build_summary(results, metadata or {})\n        self.base_path.mkdir(parents=True, exist_ok=True)\n        plugin_logger = getattr(self, \"plugin_logger\", None)\n        if plugin_logger:\n            plugin_logger.log_event(\n                \"sink_write_attempt\",\n                message=f\"Analytics report write attempt: {self.base_path}/{self.file_stem}\",\n                metrics={\"rows\": summary.get(\"rows\", 0)},\n                metadata={\"path\": str(self.base_path)},\n            )\n        written: list[Path] = []\n        if \"json\" in self.formats:\n            path = self.base_path / f\"{self.file_stem}.json\"\n            path.write_text(json.dumps(summary, indent=2, sort_keys=True), encoding=\"utf-8\")\n            written.append(path)\n        if \"md\" in self.formats:\n            path = self.base_path / f\"{self.file_stem}.md\"\n            path.write_text(self._render_markdown(summary), encoding=\"utf-8\")\n            written.append(path)\n        self._last_written_files = written\n        if metadata:\n            level = metadata.get(\"security_level\")\n            det = metadata.get(\"determinism_level\")\n            self._artifact_security_level = level if isinstance(level, SecurityLevel) else ensure_security_level(level)\n            self._artifact_determinism_level = det if isinstance(det, DeterminismLevel) else ensure_determinism_level(det)\n        if plugin_logger:\n            total_bytes = 0\n            for p in written:\n                try:\n                    total_bytes += p.stat().st_size\n                except (OSError, PermissionError):  # tolerate stat() errors to avoid blocking artifact write\n                    pass\n            plugin_logger.log_event(\n                \"sink_write\",\n                message=f\"Analytics report written under {self.base_path}\",\n                metrics={\"bytes\": total_bytes, \"files\": len(written)},\n                metadata={\"path\": str(self.base_path)},\n            )\n    except Exception as exc:  # pragma: no cover - error handling path (render/write)\n        if self.on_error == \"skip\":\n            logger.warning(\"Analytics report sink failed; skipping write: %s\", exc)\n            plugin_logger = getattr(self, \"plugin_logger\", None)\n            if plugin_logger:\n                plugin_logger.log_error(exc, context=\"analytics report sink write\", recoverable=True)\n            return\n        raise\n</code></pre>"},{"location":"api-reference/plugins/generated-sinks/#blobresultsink","title":"BlobResultSink","text":"<p>Configuration Type: <code>blob</code> Security: \u2705 Trusted Downgrade Source: <code>src/elspeth/plugins/nodes/sinks/blob.py</code></p> <p>Example Configuration: <pre><code>sinks:\n  - type: blob\n  security_level: OFFICIAL\n  allow_downgrade: true\n</code></pre></p>"},{"location":"api-reference/plugins/generated-sinks/#elspeth.plugins.nodes.sinks.blob.BlobResultSink","title":"BlobResultSink","text":"<pre><code>BlobResultSink(*, config_path: str | Path, profile: str = 'default', path_template: str | None = None, filename: str = 'results.json', manifest_template: str | None = None, manifest_suffix: str = '.manifest.json', include_manifest: bool = True, overwrite: bool = True, credential: Any | None = None, credential_env: str | None = None, content_type: str = 'application/json', metadata: Mapping[str, Any] | None = None, upload_chunk_size: int = 4 * 1024 * 1024, on_error: str = 'abort')\n</code></pre> <p>               Bases: <code>BasePlugin</code>, <code>ResultSink</code></p> <p>Upload artifacts to Azure Blob Storage with optional path constraints.</p> <p>Persist experiment payloads to Azure Blob Storage.</p> <p>The sink reuses the existing blob configuration files used by datasources so operators can target workspace datastores or ad-hoc storage accounts. The uploaded assets include a JSON payload of the experiment results and an optional manifest describing auxiliary metadata.</p> <p>Inherits from BasePlugin to provide security enforcement (ADR-004).</p> Source code in <code>elspeth/plugins/nodes/sinks/blob.py</code> <pre><code>def __init__(\n    self,\n    *,\n    config_path: str | Path,\n    profile: str = \"default\",\n    path_template: str | None = None,\n    filename: str = \"results.json\",\n    manifest_template: str | None = None,\n    manifest_suffix: str = \".manifest.json\",\n    include_manifest: bool = True,\n    overwrite: bool = True,\n    credential: Any | None = None,\n    credential_env: str | None = None,\n    content_type: str = \"application/json\",\n    metadata: Mapping[str, Any] | None = None,\n    upload_chunk_size: int = 4 * 1024 * 1024,\n    on_error: str = \"abort\",\n) -&gt; None:\n    # Initialize BasePlugin with security level and downgrade policy (ADR-002-B, ADR-005)\n    super().__init__(\n        security_level=SecurityLevel.UNOFFICIAL,  # ADR-002-B: Immutable policy\n        allow_downgrade=True,  # ADR-002-B: Immutable policy\n    )\n\n    self.config = load_blob_config(config_path, profile)\n    self.path_template = path_template\n    self.filename = filename\n    self.manifest_template = manifest_template\n    self.manifest_suffix = manifest_suffix\n    self.include_manifest = include_manifest\n    self.overwrite = overwrite\n    self.credential = credential\n    self.credential_env = credential_env\n    self.content_type = content_type\n    self._blob_metadata = self._normalize_metadata(metadata)\n    self.upload_chunk_size = max(int(upload_chunk_size), 0)\n    self._blob_service_client = None\n    if on_error not in {\"abort\", \"skip\"}:\n        raise ValueError(\"on_error must be 'abort' or 'skip'\")\n    self.on_error = on_error\n    self._artifact_inputs: list[Artifact] = []\n    # STRICT mode: enforce fail-closed policy (disallow skip-on-error)\n    try:\n        if get_secure_mode() == SecureMode.STRICT and self.on_error == \"skip\":\n            raise ValueError(\"BlobResultSink cannot use on_error='skip' in STRICT mode\")\n    except AttributeError:\n        # get_secure_mode or SecureMode may be unavailable in certain import contexts; ignore in that case\n        pass\n</code></pre>"},{"location":"api-reference/plugins/generated-sinks/#elspeth.plugins.nodes.sinks.blob.BlobResultSink-functions","title":"Functions","text":""},{"location":"api-reference/plugins/generated-sinks/#elspeth.plugins.nodes.sinks.blob.BlobResultSink.write","title":"write","text":"<pre><code>write(results: dict[str, Any], *, metadata: dict[str, Any] | None = None) -&gt; None\n</code></pre> Source code in <code>elspeth/plugins/nodes/sinks/blob.py</code> <pre><code>def write(self, results: dict[str, Any], *, metadata: dict[str, Any] | None = None) -&gt; None:\n    metadata = metadata or {}\n    timestamp = datetime.now(timezone.utc)\n    context = self._build_context(metadata, timestamp)\n\n    try:\n        blob_name = self._resolve_blob_name(context)\n        plugin_logger = getattr(self, \"plugin_logger\", None)\n        if plugin_logger:\n            plugin_logger.log_event(\n                \"sink_write_attempt\",\n                message=f\"Blob write attempt: {self.config.container_name}/{blob_name}\",\n                metadata={\"container\": self.config.container_name, \"blob\": blob_name},\n            )\n        if self._artifact_inputs:\n            for idx, artifact in enumerate(self._artifact_inputs, start=1):\n                target_name = blob_name if idx == 1 else self._append_suffix(blob_name, idx)\n                data = self._artifact_bytes(artifact)\n                content_type = None\n                if artifact.metadata:\n                    content_type = artifact.metadata.get(\"content_type\")\n                upload_metadata = self._build_upload_metadata(metadata, artifact)\n                self._upload_bytes(\n                    target_name,\n                    data,\n                    content_type=content_type or self.content_type,\n                    upload_metadata=upload_metadata,\n                )\n        else:\n            payload_bytes = self._serialize(results)\n            self._upload_bytes(\n                blob_name,\n                payload_bytes,\n                content_type=self.content_type,\n                upload_metadata=self._build_upload_metadata(metadata, None),\n            )\n\n            if self.include_manifest:\n                manifest_blob = self._resolve_manifest_name(blob_name, context)\n                manifest_payload = self._build_manifest(results, metadata, blob_name, timestamp)\n                manifest_bytes = json.dumps(manifest_payload, indent=2, sort_keys=True).encode(\"utf-8\")\n                self._upload_bytes(\n                    manifest_blob,\n                    manifest_bytes,\n                    content_type=\"application/json\",\n                    upload_metadata=self._build_upload_metadata(metadata, None),\n                )\n        if plugin_logger:\n            plugin_logger.log_event(\n                \"sink_write\",\n                message=f\"Blob write completed: {self.config.container_name}/{blob_name}\",\n                metadata={\"container\": self.config.container_name, \"blob\": blob_name},\n            )\n    except Exception as exc:\n        # Honor on_error='skip' in non-STRICT modes to maximize delivery; STRICT mode disallowed via __init__ guard\n        if self.on_error == \"skip\":\n            logger.warning(\"Blob sink failed; skipping upload: %s\", exc)\n            plugin_logger = getattr(self, \"plugin_logger\", None)\n            if plugin_logger:\n                plugin_logger.log_error(exc, context=\"blob sink write\", recoverable=True)\n            return\n        raise\n    finally:\n        self._artifact_inputs = []\n</code></pre>"},{"location":"api-reference/plugins/generated-sinks/#csvresultsink","title":"CsvResultSink","text":"<p>Configuration Type: <code>csv_file</code> Security: \u2705 Trusted Downgrade Source: <code>src/elspeth/plugins/nodes/sinks/csv_file.py</code></p> <p>Example Configuration: <pre><code>sinks:\n  - type: csv_file\n  security_level: OFFICIAL\n  allow_downgrade: true\n</code></pre></p>"},{"location":"api-reference/plugins/generated-sinks/#elspeth.plugins.nodes.sinks.csv_file.CsvResultSink","title":"CsvResultSink","text":"<pre><code>CsvResultSink(*, path: str, overwrite: bool = True, on_error: str = 'abort', sanitize_formulas: bool = True, sanitize_guard: str = \"'\", allowed_base_path: str | Path | None = None)\n</code></pre> <p>               Bases: <code>BasePlugin</code>, <code>ResultSink</code></p> <p>Result sink that writes experiment results to CSV files with formula sanitization.</p> <p>Converts experiment results into tabular CSV format with configurable sanitization to prevent formula injection attacks. Extracts row data, LLM responses, and named responses into a flat structure suitable for spreadsheet analysis.</p> <p>Inherits from BasePlugin to provide security enforcement (ADR-004).</p> <p>Features: - Automatic formula sanitization (prefix dangerous characters with guard) - Configurable overwrite behavior - Error handling with abort/skip strategies - Artifact tracking with security metadata</p> Source code in <code>elspeth/plugins/nodes/sinks/csv_file.py</code> <pre><code>def __init__(\n    self,\n    *,\n    path: str,\n    overwrite: bool = True,\n    on_error: str = \"abort\",\n    sanitize_formulas: bool = True,\n    sanitize_guard: str = \"'\",\n    allowed_base_path: str | Path | None = None,\n) -&gt; None:\n    # Initialize BasePlugin with security level and downgrade policy (ADR-002-B, ADR-005)\n    super().__init__(\n        security_level=SecurityLevel.UNOFFICIAL,  # ADR-002-B: Immutable policy\n        allow_downgrade=True,  # ADR-002-B: Immutable policy\n    )\n\n    self.path = Path(path)\n    self.overwrite = overwrite\n    if on_error not in {\"abort\", \"skip\"}:\n        raise ValueError(\"on_error must be 'abort' or 'skip'\")\n    self.on_error = on_error\n    if not sanitize_guard:\n        sanitize_guard = \"'\"\n    if len(sanitize_guard) != 1:\n        raise ValueError(\"sanitize_guard must be a single character\")\n    self.sanitize_formulas = sanitize_formulas\n    self.sanitize_guard = sanitize_guard\n    if not self.sanitize_formulas:\n        logger.warning(\"CSV sink sanitization disabled; outputs may trigger spreadsheet formulas.\")\n    self._last_written_path: str | None = None\n    # Runtime data classification tracking (separate from sink's security clearance)\n    # security_level (from BasePlugin) = sink's clearance level\n    # _artifact_security_level = runtime classification of written data (for artifact metadata)\n    self._artifact_security_level: SecurityLevel | None = None\n    self._sanitization = {\n        \"enabled\": self.sanitize_formulas,\n        \"guard\": self.sanitize_guard,\n    }\n    # Allowed base directory for writes; default to ./outputs\n    try:\n        default_base = self.path.parent.resolve()\n        self._allowed_base = Path(allowed_base_path).resolve() if allowed_base_path is not None else default_base\n    except (OSError, ValueError, TypeError):  # pragma: no cover - defensive\n        self._allowed_base = Path.cwd().resolve()\n</code></pre>"},{"location":"api-reference/plugins/generated-sinks/#elspeth.plugins.nodes.sinks.csv_file.CsvResultSink-functions","title":"Functions","text":""},{"location":"api-reference/plugins/generated-sinks/#elspeth.plugins.nodes.sinks.csv_file.CsvResultSink.write","title":"write","text":"<pre><code>write(results: dict[str, Any], *, metadata: dict[str, Any] | None = None) -&gt; None\n</code></pre> <p>Write experiment results to CSV file with sanitization.</p> Source code in <code>elspeth/plugins/nodes/sinks/csv_file.py</code> <pre><code>def write(self, results: dict[str, Any], *, metadata: dict[str, Any] | None = None) -&gt; None:\n    \"\"\"Write experiment results to CSV file with sanitization.\"\"\"\n    try:\n        df = self._results_to_dataframe(results)\n\n        if self.path.exists() and not self.overwrite:\n            raise FileExistsError(f\"CSV sink destination exists: {self.path}\")\n\n        # Resolve and enforce write under allowed base; then atomic replace\n        target = resolve_under_base(self.path, self._allowed_base)\n\n        # Emit attempt event if plugin_logger available\n        plugin_logger = getattr(self, \"plugin_logger\", None)\n        if plugin_logger:\n            plugin_logger.log_event(\n                \"sink_write_attempt\",\n                message=f\"CSV write attempt: {target}\",\n                metrics={\"rows\": len(df)},\n                metadata={\"path\": str(target)},\n            )\n\n        def _writer(tmp_path: Path) -&gt; None:\n            df.to_csv(tmp_path, index=False)\n\n        safe_atomic_write(target, _writer)\n        self._last_written_path = str(target)\n\n        if metadata:\n            level = metadata.get(\"security_level\")\n            self._artifact_security_level = level if isinstance(level, SecurityLevel) else ensure_security_level(level)\n        # Emit success event\n        if plugin_logger:\n            try:\n                size = Path(self._last_written_path).stat().st_size if self._last_written_path else 0\n            except (OSError, PermissionError):\n                size = 0\n            plugin_logger.log_event(\n                \"sink_write\",\n                message=f\"CSV written to {target}\",\n                metrics={\"rows\": len(df), \"bytes\": size},\n                metadata={\"path\": str(target)},\n            )\n    except Exception as exc:\n        if self.on_error == \"skip\":\n            logger.warning(\"CSV sink failed; skipping write to '%s': %s\", self.path, exc)\n            # Emit error event (recoverable)\n            plugin_logger = getattr(self, \"plugin_logger\", None)\n            if plugin_logger:\n                plugin_logger.log_error(exc, context=\"csv sink write\", recoverable=True)\n            return\n        # Emit error event (fatal)\n        plugin_logger = getattr(self, \"plugin_logger\", None)\n        if plugin_logger:\n            plugin_logger.log_error(exc, context=\"csv sink write\", recoverable=False)\n        raise\n</code></pre>"},{"location":"api-reference/plugins/generated-sinks/#embeddingsstoresink","title":"EmbeddingsStoreSink","text":"<p>Configuration Type: <code>embeddings_store</code> Security: \u2705 Trusted Downgrade Source: <code>src/elspeth/plugins/nodes/sinks/embeddings_store.py</code></p> <p>Example Configuration: <pre><code>sinks:\n  - type: embeddings_store\n  security_level: OFFICIAL\n  allow_downgrade: true\n</code></pre></p>"},{"location":"api-reference/plugins/generated-sinks/#elspeth.plugins.nodes.sinks.embeddings_store.EmbeddingsStoreSink","title":"EmbeddingsStoreSink","text":"<pre><code>EmbeddingsStoreSink(*, provider: str, namespace: str | None = None, dsn: str | None = None, table: str = 'elspeth_rag', text_field: str = DEFAULT_TEXT_FIELD, embedding_source: str = DEFAULT_EMBEDDING_FIELD, embed_model: Mapping[str, Any] | None = None, metadata_fields: Sequence[str] | None = None, id_field: str = DEFAULT_ID_FIELD, batch_size: int = 50, upsert_conflict: str = 'replace', provider_factory: Callable[[str, Mapping[str, Any]], VectorStoreClient] | None = None, embedder_factory: Callable[[Mapping[str, Any]], Embedder] | None = None, provider_options: Mapping[str, Any] | None = None)\n</code></pre> <p>               Bases: <code>BasePlugin</code>, <code>ResultSink</code></p> <p>Persist embeddings into a vector store backend (pgvector/Azure Search).</p> <p>Inherits from BasePlugin to provide security enforcement (ADR-004).</p> <p>Persist experiment outputs into a vector store for RAG workflows.</p> Source code in <code>elspeth/plugins/nodes/sinks/embeddings_store.py</code> <pre><code>def __init__(\n    self,\n    *,\n    provider: str,\n    namespace: str | None = None,\n    dsn: str | None = None,\n    table: str = \"elspeth_rag\",\n    text_field: str = DEFAULT_TEXT_FIELD,\n    embedding_source: str = DEFAULT_EMBEDDING_FIELD,\n    embed_model: Mapping[str, Any] | None = None,\n    metadata_fields: Sequence[str] | None = None,\n    id_field: str = DEFAULT_ID_FIELD,\n    batch_size: int = 50,\n    upsert_conflict: str = \"replace\",\n    provider_factory: Callable[[str, Mapping[str, Any]], VectorStoreClient] | None = None,\n    embedder_factory: Callable[[Mapping[str, Any]], Embedder] | None = None,\n    provider_options: Mapping[str, Any] | None = None,\n) -&gt; None:\n    # Initialize BasePlugin with security level and downgrade policy (ADR-002-B)\n    super().__init__(\n        security_level=SecurityLevel.UNOFFICIAL,  # ADR-002-B: Immutable policy\n        allow_downgrade=True,  # ADR-002-B: Immutable policy\n    )\n\n    self.provider_name = provider\n    self._namespace_override = namespace\n    self._dsn = dsn\n    self._table = table\n    self._text_field = text_field\n    self._embedding_field = embedding_source\n    self._embed_model = dict(embed_model or {})\n    self._metadata_fields = list(metadata_fields or DEFAULT_METADATA_FIELDS)\n    self._id_field = id_field\n    self._batch_size = max(int(batch_size), 1)\n    self._upsert_conflict = upsert_conflict\n    self._provider_factory = provider_factory or self._default_provider_factory\n    self._embedder_factory = embedder_factory or self._default_embedder_factory\n    provider_payload = {\n        \"dsn\": dsn,\n        \"table\": table,\n        \"upsert_conflict\": upsert_conflict,\n    }\n    if provider_options:\n        provider_payload.update(provider_options)\n    self._client = self._provider_factory(\n        provider,\n        provider_payload,\n    )\n    self._embedder: Embedder | None = None\n    self._last_manifest: dict[str, Any] | None = None\n</code></pre>"},{"location":"api-reference/plugins/generated-sinks/#elspeth.plugins.nodes.sinks.embeddings_store.EmbeddingsStoreSink-functions","title":"Functions","text":""},{"location":"api-reference/plugins/generated-sinks/#elspeth.plugins.nodes.sinks.embeddings_store.EmbeddingsStoreSink.write","title":"write","text":"<pre><code>write(results: dict[str, Any], *, metadata: dict[str, Any] | None = None) -&gt; None\n</code></pre> Source code in <code>elspeth/plugins/nodes/sinks/embeddings_store.py</code> <pre><code>def write(self, results: dict[str, Any], *, metadata: dict[str, Any] | None = None) -&gt; None:\n    metadata = metadata or {}\n    namespace = self._resolve_namespace(metadata)\n    context: PluginContext | None = getattr(self, \"plugin_context\", None)\n    security_level = metadata.get(\"security_level\", getattr(context, \"security_level\", \"unofficial\"))\n    determinism_level = metadata.get(\"determinism_level\", getattr(context, \"determinism_level\", \"none\"))\n    embeddings: list[VectorRecord] = []\n\n    for index, record in enumerate(results.get(\"results\") or []):\n        vector = self._extract_embedding(record)\n        text_value = self._extract_value(record, self._text_field) or \"\"\n        if vector is None:\n            if not self._embed_model:\n                raise ConfigurationError(\"embeddings_store requires embed_model configuration when payload lacks embeddings\")\n            vector = self._embed_text(str(text_value))\n\n        document_id = self._extract_value(record, self._id_field) or f\"{metadata.get('run_id', 'run')}-{index}\"\n        record_metadata = self._extract_metadata(record, metadata)\n        # Persist classification as canonical text for storage\n        if isinstance(security_level, str):\n            sec_text = security_level\n        else:\n            try:\n                from elspeth.core.base.types import SecurityLevel as _SL\n\n                sec_text = security_level.value if isinstance(security_level, _SL) else str(security_level)\n            except ImportError:\n                sec_text = str(security_level)\n\n        embeddings.append(\n            VectorRecord(\n                document_id=str(document_id),\n                vector=vector,\n                text=str(text_value),\n                metadata=record_metadata,\n                security_level=sec_text,\n            )\n        )\n\n    plugin_logger = getattr(self, \"plugin_logger\", None)\n    if plugin_logger:\n        plugin_logger.log_event(\n            \"sink_write_attempt\",\n            message=f\"Embeddings upsert attempt: provider={self.provider_name}, namespace={namespace}\",\n            metrics={\"rows\": len(embeddings)},\n            metadata={\"namespace\": namespace, \"provider\": self.provider_name},\n        )\n\n    batches = [embeddings[i : i + self._batch_size] for i in range(0, len(embeddings), self._batch_size)]\n    upsert_total = 0\n    took_total = 0.0\n    for batch in batches:\n        response = self._client.upsert_many(namespace=namespace, records=batch)\n        upsert_total += response.count\n        took_total += response.took\n\n    if upsert_total:\n        self._last_manifest = {\n            \"namespace\": namespace,\n            \"count\": upsert_total,\n            \"batch_count\": len(batches),\n            \"duration_seconds\": took_total,\n            \"security_level\": security_level,\n            \"determinism_level\": determinism_level,\n            \"provider\": self.provider_name,\n        }\n    else:\n        self._last_manifest = None\n    if plugin_logger:\n        plugin_logger.log_event(\n            \"sink_write\",\n            message=f\"Embeddings upserted: provider={self.provider_name}, namespace={namespace}\",\n            metrics={\"count\": upsert_total, \"duration_seconds\": took_total, \"batches\": len(batches)},\n            metadata={\"namespace\": namespace, \"provider\": self.provider_name},\n        )\n</code></pre>"},{"location":"api-reference/plugins/generated-sinks/#excelresultsink","title":"ExcelResultSink","text":"<p>Configuration Type: <code>excel</code> Security: \u2705 Trusted Downgrade Source: <code>src/elspeth/plugins/nodes/sinks/excel.py</code></p> <p>Example Configuration: <pre><code>sinks:\n  - type: excel\n  security_level: OFFICIAL\n  allow_downgrade: true\n</code></pre></p>"},{"location":"api-reference/plugins/generated-sinks/#elspeth.plugins.nodes.sinks.excel.ExcelResultSink","title":"ExcelResultSink","text":"<pre><code>ExcelResultSink(*, base_path: str | Path, workbook_name: str | None = None, timestamped: bool = True, results_sheet: str = 'Results', manifest_sheet: str = 'Manifest', aggregates_sheet: str = 'Aggregates', include_manifest: bool = True, include_aggregates: bool = True, on_error: str = 'abort', sanitize_formulas: bool = True, sanitize_guard: str = \"'\", config: ExcelSinkConfig | None = None, allowed_base_path: str | Path | None = None)\n</code></pre> <p>               Bases: <code>BasePlugin</code>, <code>ResultSink</code></p> <p>Persist experiment payloads into a timestamped Excel workbook.</p> <p>Inherits from BasePlugin to provide security enforcement (ADR-004).</p> <p>Initialize Excel sink.</p> <p>Args can be provided either directly or via config object. If config is provided, it takes precedence over individual args. This supports both legacy and new usage patterns.</p> Source code in <code>elspeth/plugins/nodes/sinks/excel.py</code> <pre><code>def __init__(\n    self,\n    *,\n    base_path: str | Path,\n    workbook_name: str | None = None,\n    timestamped: bool = True,\n    results_sheet: str = \"Results\",\n    manifest_sheet: str = \"Manifest\",\n    aggregates_sheet: str = \"Aggregates\",\n    include_manifest: bool = True,\n    include_aggregates: bool = True,\n    on_error: str = \"abort\",\n    sanitize_formulas: bool = True,\n    sanitize_guard: str = \"'\",\n    config: ExcelSinkConfig | None = None,\n    allowed_base_path: str | Path | None = None,\n) -&gt; None:\n    \"\"\"Initialize Excel sink.\n\n    Args can be provided either directly or via config object.\n    If config is provided, it takes precedence over individual args.\n    This supports both legacy and new usage patterns.\n    \"\"\"\n    # Initialize BasePlugin with security level and downgrade policy (ADR-002-B, ADR-005)\n    super().__init__(\n        security_level=SecurityLevel.UNOFFICIAL,  # ADR-002-B: Immutable policy\n        allow_downgrade=True,  # ADR-002-B: Immutable policy\n    )\n\n    # Use config if provided, otherwise use individual args\n    if config is not None:\n        base_path = config.base_path\n        workbook_name = config.workbook_name\n        timestamped = config.timestamped\n        results_sheet = config.results_sheet\n        manifest_sheet = config.manifest_sheet\n        aggregates_sheet = config.aggregates_sheet\n        include_manifest = config.include_manifest\n        include_aggregates = config.include_aggregates\n        on_error = config.on_error\n        sanitize_formulas = config.sanitize_formulas\n        sanitize_guard = config.sanitize_guard\n    self.base_path = Path(base_path)\n    self.workbook_name = workbook_name\n    self.timestamped = timestamped\n    self.results_sheet = results_sheet\n    self.manifest_sheet = manifest_sheet\n    self.aggregates_sheet = aggregates_sheet\n    self.include_manifest = include_manifest\n    self.include_aggregates = include_aggregates\n    if on_error not in {\"abort\", \"skip\"}:\n        raise ValueError(\"on_error must be 'abort' or 'skip'\")\n    self.on_error = on_error\n    if not sanitize_guard:\n        sanitize_guard = \"'\"\n    if len(sanitize_guard) != 1:\n        raise ValueError(\"sanitize_guard must be a single character\")\n    self.sanitize_formulas = sanitize_formulas\n    self.sanitize_guard = sanitize_guard\n    if not self.sanitize_formulas:\n        logger.warning(\"Excel sink sanitization disabled; outputs may trigger spreadsheet formulas.\")\n    # Ensure dependency availability early for fast failure when configured incorrectly.\n    self._workbook_factory = _load_workbook_dependencies()\n    self._last_workbook_path: str | None = None\n    # Runtime data classification tracking (separate from sink's security clearance)\n    # security_level (from BasePlugin) = sink's clearance level\n    # _artifact_security_level = runtime classification of written data (for artifact metadata)\n    self._artifact_security_level: SecurityLevel | None = None\n    self._artifact_determinism_level: DeterminismLevel | None = None\n    self._sanitization = {\n        \"enabled\": self.sanitize_formulas,\n        \"guard\": self.sanitize_guard,\n    }\n    # Allowed base directory for writes; default to ./outputs\n    try:\n        default_base = Path(base_path).resolve()\n        self._allowed_base = Path(allowed_base_path).resolve() if allowed_base_path is not None else default_base\n    except Exception:  # pragma: no cover - defensive\n        self._allowed_base = Path.cwd().resolve()\n</code></pre>"},{"location":"api-reference/plugins/generated-sinks/#elspeth.plugins.nodes.sinks.excel.ExcelResultSink-functions","title":"Functions","text":""},{"location":"api-reference/plugins/generated-sinks/#elspeth.plugins.nodes.sinks.excel.ExcelResultSink.write","title":"write","text":"<pre><code>write(results: dict[str, Any], *, metadata: dict[str, Any] | None = None) -&gt; None\n</code></pre> Source code in <code>elspeth/plugins/nodes/sinks/excel.py</code> <pre><code>def write(self, results: dict[str, Any], *, metadata: dict[str, Any] | None = None) -&gt; None:\n    metadata = metadata or {}\n    timestamp = datetime.now(timezone.utc)\n    try:\n        path = self._resolve_path(metadata, timestamp)\n        target = resolve_under_base(path, self._allowed_base)\n\n        plugin_logger = getattr(self, \"plugin_logger\", None)\n        if plugin_logger:\n            plugin_logger.log_event(\n                \"sink_write_attempt\",\n                message=f\"Excel write attempt: {target}\",\n                metadata={\"path\": str(target)},\n            )\n\n        workbook = self._workbook_factory()\n        self._populate_results_sheet(workbook, results.get(\"results\", []))\n\n        if self.include_manifest:\n            self._populate_manifest_sheet(workbook, results, metadata, timestamp)\n\n        if self.include_aggregates and results.get(\"aggregates\"):\n            aggregates = results.get(\"aggregates\")\n            if isinstance(aggregates, Mapping):\n                self._populate_aggregates_sheet(workbook, aggregates)\n\n        def _writer(tmp_path: Path) -&gt; None:\n            workbook.save(tmp_path)\n\n        safe_atomic_write(target, _writer)\n        self._last_workbook_path = str(target)\n        if metadata:\n            level = metadata.get(\"security_level\")\n            det = metadata.get(\"determinism_level\")\n            self._artifact_security_level = level if isinstance(level, SecurityLevel) else ensure_security_level(level)\n            self._artifact_determinism_level = det if isinstance(det, DeterminismLevel) else ensure_determinism_level(det)\n        if plugin_logger:\n            try:\n                size = Path(self._last_workbook_path).stat().st_size if self._last_workbook_path else 0\n            except Exception:\n                size = 0\n            plugin_logger.log_event(\n                \"sink_write\",\n                message=f\"Excel written to {target}\",\n                metrics={\"bytes\": size},\n                metadata={\"path\": str(target)},\n            )\n    except Exception as exc:\n        if self.on_error == \"skip\":\n            logger.warning(\"Excel sink failed; skipping workbook creation: %s\", exc)\n            plugin_logger = getattr(self, \"plugin_logger\", None)\n            if plugin_logger:\n                plugin_logger.log_error(exc, context=\"excel sink write\", recoverable=True)\n            return\n        raise\n</code></pre>"},{"location":"api-reference/plugins/generated-sinks/#filecopysink","title":"FileCopySink","text":"<p>Configuration Type: <code>file_copy</code> Security: \u2705 Trusted Downgrade Source: <code>src/elspeth/plugins/nodes/sinks/file_copy.py</code></p> <p>Example Configuration: <pre><code>sinks:\n  - type: file_copy\n  security_level: OFFICIAL\n  allow_downgrade: true\n</code></pre></p>"},{"location":"api-reference/plugins/generated-sinks/#elspeth.plugins.nodes.sinks.file_copy.FileCopySink","title":"FileCopySink","text":"<pre><code>FileCopySink(*, destination: str, overwrite: bool = True, on_error: str = 'abort', allowed_base_path: str | None = None)\n</code></pre> <p>               Bases: <code>BasePlugin</code>, <code>ResultSink</code></p> <p>Copy a single input artifact to a destination path.</p> <p>Honors overwrite and on_error semantics and enforces base-path containment via <code>allowed_base_path</code> when provided.</p> <p>Inherits from BasePlugin to provide security enforcement (ADR-004).</p> Source code in <code>elspeth/plugins/nodes/sinks/file_copy.py</code> <pre><code>def __init__(\n    self,\n    *,\n    destination: str,\n    overwrite: bool = True,\n    on_error: str = \"abort\",\n    allowed_base_path: str | None = None,\n) -&gt; None:\n    # Initialize BasePlugin with security level and downgrade policy (ADR-002-B, ADR-005)\n    super().__init__(\n        security_level=SecurityLevel.UNOFFICIAL,  # ADR-002-B: Immutable policy\n        allow_downgrade=True,  # ADR-002-B: Immutable policy\n    )\n    self.destination = Path(destination)\n    self.overwrite = overwrite\n    if on_error not in {\"abort\", \"skip\"}:\n        raise ValueError(\"on_error must be 'abort' or 'skip'\")\n    self.on_error = on_error\n    self._source_artifact: Artifact | None = None\n    self._written_path: Path | None = None\n    self._output_type: str | None = None\n    # Runtime data classification tracking (separate from sink's security clearance)\n    # security_level (from BasePlugin) = sink's clearance level\n    # _artifact_security_level = runtime classification of written data (for artifact metadata)\n    self._artifact_security_level: SecurityLevel | None = None\n    self._artifact_determinism_level: DeterminismLevel | None = None\n    # Configure allowed base path for containment checks\n    try:\n        default_base = self.destination.parent.resolve()\n        self._allowed_base = Path(allowed_base_path).resolve() if allowed_base_path is not None else default_base\n    except (OSError, ValueError, TypeError):  # pragma: no cover - defensive\n        self._allowed_base = self.destination.parent.resolve()\n</code></pre>"},{"location":"api-reference/plugins/generated-sinks/#elspeth.plugins.nodes.sinks.file_copy.FileCopySink-functions","title":"Functions","text":""},{"location":"api-reference/plugins/generated-sinks/#elspeth.plugins.nodes.sinks.file_copy.FileCopySink.write","title":"write","text":"<pre><code>write(results: dict, *, metadata: dict | None = None) -&gt; None\n</code></pre> Source code in <code>elspeth/plugins/nodes/sinks/file_copy.py</code> <pre><code>def write(self, results: dict, *, metadata: dict | None = None) -&gt; None:\n    if not self._source_artifact or not self._source_artifact.path:\n        message = \"FileCopySink requires an input artifact; configure artifacts.consumes\"\n        if self.on_error == \"skip\":\n            logger.warning(message)\n            return\n        raise ValueError(message)\n\n    src_path = Path(self._source_artifact.path)\n    if not src_path.exists():\n        message = f\"Source artifact path not found: {src_path}\"\n        if self.on_error == \"skip\":\n            logger.warning(message)\n            return\n        raise FileNotFoundError(message)\n\n    if self.destination.exists() and not self.overwrite:\n        raise FileExistsError(f\"Destination exists: {self.destination}\")\n\n    # Resolve destination under allowed base (default to destination parent)\n    default_base = self.destination.parent.resolve()\n    allowed_base = getattr(self, \"_allowed_base\", None)\n    base_to_use = allowed_base if allowed_base is not None else default_base\n    target = resolve_under_base(self.destination, Path(base_to_use))\n    plugin_logger = getattr(self, \"plugin_logger\", None)\n    if plugin_logger:\n        plugin_logger.log_event(\n            \"sink_write_attempt\",\n            message=f\"File copy attempt: {src_path} -&gt; {target}\",\n            metadata={\"source\": str(src_path), \"dest\": str(target)},\n        )\n\n    def _copy_to_tmp(tmp: Path) -&gt; None:\n        shutil.copyfile(src_path, tmp)\n\n    safe_atomic_write(target, _copy_to_tmp)\n    self._written_path = target\n    if plugin_logger:\n        try:\n            size = target.stat().st_size\n        except (OSError, PermissionError):\n            size = 0\n        plugin_logger.log_event(\n            \"sink_write\",\n            message=f\"File copied to {target}\",\n            metrics={\"bytes\": size},\n            metadata={\"source\": str(src_path), \"dest\": str(target)},\n        )\n    if metadata and metadata.get(\"security_level\"):\n        level = metadata.get(\"security_level\")\n        det = metadata.get(\"determinism_level\")\n        self._artifact_security_level = level if isinstance(level, SecurityLevel) else ensure_security_level(level)\n        self._artifact_determinism_level = det if isinstance(det, DeterminismLevel) else ensure_determinism_level(det)\n</code></pre>"},{"location":"api-reference/plugins/generated-sinks/#localbundlesink","title":"LocalBundleSink","text":"<p>Configuration Type: <code>local_bundle</code> Security: \u2753 Unknown Source: <code>src/elspeth/plugins/nodes/sinks/local_bundle.py</code></p> <p>Example Configuration: <pre><code>sinks:\n  - type: local_bundle\n  security_level: OFFICIAL\n</code></pre></p>"},{"location":"api-reference/plugins/generated-sinks/#elspeth.plugins.nodes.sinks.local_bundle.LocalBundleSink","title":"LocalBundleSink  <code>dataclass</code>","text":"<pre><code>LocalBundleSink(base_path: str | Path, bundle_name: str | None = None, timestamped: bool = True, write_json: bool = True, write_csv: bool = False, manifest_name: str = 'manifest.json', results_name: str = 'results.json', csv_name: str = 'results.csv', on_error: str = 'abort', sanitize_formulas: bool = True, sanitize_guard: str = \"'\", allowed_base_path: str | None = None)\n</code></pre> <p>               Bases: <code>BasePlugin</code>, <code>ResultSink</code></p> <p>Create a local folder bundle with payload, metadata, and artifacts.</p> <p>Inherits from BasePlugin to provide security enforcement (ADR-004).</p>"},{"location":"api-reference/plugins/generated-sinks/#elspeth.plugins.nodes.sinks.local_bundle.LocalBundleSink-functions","title":"Functions","text":""},{"location":"api-reference/plugins/generated-sinks/#elspeth.plugins.nodes.sinks.local_bundle.LocalBundleSink.write","title":"write","text":"<pre><code>write(results: dict[str, Any], *, metadata: dict[str, Any] | None = None) -&gt; None\n</code></pre> Source code in <code>elspeth/plugins/nodes/sinks/local_bundle.py</code> <pre><code>def write(self, results: dict[str, Any], *, metadata: dict[str, Any] | None = None) -&gt; None:\n    metadata = metadata or {}\n    timestamp = datetime.now(timezone.utc)\n    try:\n        plugin_logger = getattr(self, \"plugin_logger\", None)\n        bundle_dir = self._resolve_bundle_dir(metadata, timestamp)\n        # Enforce allowed base for directory; use placeholder technique to leverage resolver\n        target_dir = resolve_under_base(bundle_dir / \".dir\", self._allowed_base).parent\n\n        if plugin_logger:\n            plugin_logger.log_event(\n                \"sink_write_attempt\",\n                message=f\"Bundle write attempt: {target_dir}\",\n                metadata={\"path\": str(target_dir)},\n            )\n\n        manifest = self._build_manifest(results, metadata, timestamp)\n        manifest_path = target_dir / self.manifest_name\n\n        def _write_manifest(tmp: Path) -&gt; None:\n            tmp.write_text(json.dumps(manifest, indent=2, sort_keys=True), encoding=\"utf-8\")\n\n        safe_atomic_write(manifest_path, _write_manifest)\n\n        if self.write_json:\n            results_path = target_dir / self.results_name\n\n            def _write_results(tmp: Path) -&gt; None:\n                tmp.write_text(json.dumps(results, indent=2, sort_keys=True), encoding=\"utf-8\")\n\n            safe_atomic_write(results_path, _write_results)\n\n        if self.write_csv:\n            csv_path = target_dir / self.csv_name\n            csv_sink = CsvResultSink(\n                path=str(csv_path),\n                overwrite=True,\n                sanitize_formulas=self.sanitize_formulas,\n                sanitize_guard=self.sanitize_guard,\n            )\n            # Propagate allowed base to nested sink (best-effort)\n            try:\n                csv_sink._allowed_base = self._allowed_base  # noqa: SLF001 - internal attribute for path guard\n            except Exception:  # nosec B110 - optional optimization only\n                logger.debug(\"Failed to set CSV sink allowed base; continuing\", exc_info=True)\n            csv_sink.write(results, metadata=metadata)\n\n        if plugin_logger:\n            total_bytes = 0\n            paths: list[Path | None] = [\n                manifest_path,\n                (target_dir / self.results_name) if self.write_json else None,\n                (target_dir / self.csv_name) if self.write_csv else None,\n            ]\n            for p in paths:\n                if p and p.exists():\n                    try:\n                        total_bytes += p.stat().st_size\n                    except Exception:  # nosec B110 - tolerate stat() errors; do not block artifact write\n                        logger.debug(\"Failed to stat file during size aggregation: %s\", p, exc_info=True)\n            plugin_logger.log_event(\n                \"sink_write\",\n                message=f\"Bundle written under {target_dir}\",\n                metrics={\"bytes\": total_bytes},\n                metadata={\"path\": str(target_dir)},\n            )\n    except Exception as exc:\n        if self.on_error == \"skip\":\n            logger.warning(\"Local bundle sink failed; skipping bundle creation: %s\", exc)\n            plugin_logger = getattr(self, \"plugin_logger\", None)\n            if plugin_logger:\n                plugin_logger.log_error(exc, context=\"local bundle sink write\", recoverable=True)\n            return\n        raise\n</code></pre>"},{"location":"api-reference/plugins/generated-sinks/#_reposinkbase","title":"_RepoSinkBase","text":"<p>Configuration Type: <code>repository</code> Security: \u2753 Unknown Source: <code>src/elspeth/plugins/nodes/sinks/repository.py</code></p> <p>Example Configuration: <pre><code>sinks:\n  - type: repository\n  security_level: OFFICIAL\n</code></pre></p>"},{"location":"api-reference/plugins/generated-sinks/#elspeth.plugins.nodes.sinks.repository._RepoSinkBase","title":"_RepoSinkBase  <code>dataclass</code>","text":"<pre><code>_RepoSinkBase(path_template: str = 'experiments/{experiment}/{timestamp}', commit_message_template: str = 'Add experiment results for {experiment}', include_manifest: bool = True, dry_run: bool = True, session: Session | None = None, request_timeout: int = DEFAULT_REQUEST_TIMEOUT, on_error: str = 'abort', _dry_run_warned_once: bool = False, _strict_dry_run_warned_once: bool = False)\n</code></pre> <p>               Bases: <code>BasePlugin</code>, <code>ResultSink</code></p> <p>Common repository sink behavior (auth, retries, dry-run, error handling).</p> <p>Inherits from BasePlugin to provide security enforcement (ADR-004).</p>"},{"location":"api-reference/plugins/generated-sinks/#elspeth.plugins.nodes.sinks.repository._RepoSinkBase-functions","title":"Functions","text":""},{"location":"api-reference/plugins/generated-sinks/#elspeth.plugins.nodes.sinks.repository._RepoSinkBase.write","title":"write","text":"<pre><code>write(results: dict[str, Any], *, metadata: dict[str, Any] | None = None) -&gt; None\n</code></pre> Source code in <code>elspeth/plugins/nodes/sinks/repository.py</code> <pre><code>def write(self, results: dict[str, Any], *, metadata: dict[str, Any] | None = None) -&gt; None:\n    metadata = metadata or {}\n    timestamp = datetime.now(timezone.utc)\n    context = _default_context(metadata, timestamp)\n    plugin_logger = getattr(self, \"plugin_logger\", None)\n\n    try:\n        prefix = self._resolve_prefix(context)\n        files = self._prepare_files(results, metadata, prefix, timestamp)\n        commit_message = self.commit_message_template.format(**context)\n        payload = self._build_payload(context, commit_message, files)\n\n        self._log_attempt(plugin_logger, prefix, commit_message, len(files))\n\n        if self.dry_run:\n            self._handle_dry_run(payload, prefix, len(files), plugin_logger)\n            return\n\n        self._upload(files, commit_message, metadata, context, timestamp)\n        self._log_success(plugin_logger, prefix, len(files))\n    except _RepoRequestError as exc:\n        if not self._handle_repo_exception(exc, plugin_logger):\n            raise\n    except (OSError, RuntimeError) as exc:\n        if not self._handle_generic_exception(exc, plugin_logger):\n            raise\n</code></pre>"},{"location":"api-reference/plugins/generated-sinks/#reproducibilitybundlesink","title":"ReproducibilityBundleSink","text":"<p>Configuration Type: <code>reproducibility_bundle</code> Security: \u2753 Unknown Source: <code>src/elspeth/plugins/nodes/sinks/reproducibility_bundle.py</code></p> <p>Example Configuration: <pre><code>sinks:\n  - type: reproducibility_bundle\n  security_level: OFFICIAL\n</code></pre></p>"},{"location":"api-reference/plugins/generated-sinks/#elspeth.plugins.nodes.sinks.reproducibility_bundle.ReproducibilityBundleSink","title":"ReproducibilityBundleSink  <code>dataclass</code>","text":"<pre><code>ReproducibilityBundleSink(base_path: str | Path, bundle_name: str | None = None, timestamped: bool = True, include_results_json: bool = True, include_results_csv: bool = True, include_source_data: bool = True, include_config: bool = True, include_prompts: bool = True, include_plugins: bool = True, include_framework_code: bool = False, results_json_name: str = 'results.json', results_csv_name: str = 'results.csv', source_data_name: str = 'source_data.csv', config_name: str = 'config.yaml', prompts_name: str = 'prompts.json', manifest_name: str = 'MANIFEST.json', signature_name: str = 'SIGNATURE.json', algorithm: str = 'hmac-sha256', key: str | None = None, key_env: str | None = 'ELSPETH_SIGNING_KEY', on_error: str = 'abort', sanitize_formulas: bool = True, sanitize_guard: str = \"'\", compression: str = 'gz')\n</code></pre> <p>               Bases: <code>BasePlugin</code>, <code>ResultSink</code></p> <p>Create a tamper-evident reproducibility bundle with results, metadata, and code.</p> <p>Inherits from BasePlugin to provide security enforcement (ADR-004).</p>"},{"location":"api-reference/plugins/generated-sinks/#elspeth.plugins.nodes.sinks.reproducibility_bundle.ReproducibilityBundleSink-functions","title":"Functions","text":""},{"location":"api-reference/plugins/generated-sinks/#elspeth.plugins.nodes.sinks.reproducibility_bundle.ReproducibilityBundleSink.write","title":"write","text":"<pre><code>write(results: dict[str, Any], *, metadata: dict[str, Any] | None = None) -&gt; None\n</code></pre> <p>Create reproducibility bundle with cryptographic signing.</p> Source code in <code>elspeth/plugins/nodes/sinks/reproducibility_bundle.py</code> <pre><code>def write(self, results: dict[str, Any], *, metadata: dict[str, Any] | None = None) -&gt; None:\n    \"\"\"Create reproducibility bundle with cryptographic signing.\"\"\"\n    metadata = metadata or {}\n    timestamp = datetime.now(timezone.utc)\n\n    try:\n        plugin_logger = getattr(self, \"plugin_logger\", None)\n        if plugin_logger:\n            plugin_logger.log_event(\n                \"sink_write_attempt\",\n                message=f\"Reproducibility bundle attempt: {self.base_path}/{self.bundle_name or 'bundle'}\",\n                metadata={\"path\": str(self.base_path)},\n            )\n        # Create temporary directory for bundle contents\n        self._temp_dir = Path(tempfile.mkdtemp(prefix=\"elspeth_repro_\"))\n\n        # Write all bundle components\n        if self.include_results_json:\n            self._write_results_json(results)\n\n        if self.include_results_csv:\n            self._write_results_csv(results, metadata)\n\n        if self.include_source_data:\n            self._write_source_data(metadata)\n\n        if self.include_config:\n            self._write_config(metadata)\n\n        if self.include_prompts:\n            self._write_prompts(results, metadata)\n\n        if self.include_plugins:\n            self._write_plugins(metadata)\n\n        if self.include_framework_code:\n            self._write_framework_code()\n\n        # Write consumed artifacts from other sinks\n        self._write_consumed_artifacts()\n\n        # Generate manifest with file hashes\n        manifest = self._build_manifest(results, metadata, timestamp)\n        temp_dir = self._ensure_temp_dir()\n        manifest_path = temp_dir / self.manifest_name\n        manifest_path.write_text(json.dumps(manifest, indent=2, sort_keys=True), encoding=\"utf-8\")\n        self._file_hashes[self.manifest_name] = self._hash_file(manifest_path)\n\n        # Sign the manifest\n        signature_data = self._sign_manifest(manifest, timestamp)\n        temp_dir = self._ensure_temp_dir()\n        signature_path = temp_dir / self.signature_name\n        signature_path.write_text(json.dumps(signature_data, indent=2, sort_keys=True), encoding=\"utf-8\")\n\n        # Create final tarball\n        archive_path = self._create_archive(metadata, timestamp)\n        self._last_archive_path = str(archive_path)\n\n        if plugin_logger:\n            try:\n                size = archive_path.stat().st_size\n            except OSError:\n                size = 0\n            plugin_logger.log_event(\n                \"sink_write\",\n                message=f\"Reproducibility bundle created: {archive_path.name}\",\n                metrics={\"files\": len(self._file_hashes), \"bytes\": size},\n                metadata={\"path\": str(archive_path)},\n            )\n\n    except Exception as exc:\n        if self.on_error == \"skip\":\n            logger.warning(\"Reproducibility bundle creation failed; skipping: %s\", exc)\n            plugin_logger = getattr(self, \"plugin_logger\", None)\n            if plugin_logger:\n                plugin_logger.log_error(exc, context=\"reproducibility bundle write\", recoverable=True)\n            return\n        raise\n    finally:\n        # Cleanup temp directory\n        if self._temp_dir and self._temp_dir.exists():\n            shutil.rmtree(self._temp_dir, ignore_errors=True)\n        self._temp_dir = None\n        self._file_hashes = {}\n        self._consumed_artifacts = {}\n</code></pre>"},{"location":"api-reference/plugins/generated-sinks/#signedartifactsink","title":"SignedArtifactSink","text":"<p>Configuration Type: <code>signed</code> Security: \u2753 Unknown Source: <code>src/elspeth/plugins/nodes/sinks/signed.py</code></p> <p>Example Configuration: <pre><code>sinks:\n  - type: signed\n  security_level: OFFICIAL\n</code></pre></p>"},{"location":"api-reference/plugins/generated-sinks/#elspeth.plugins.nodes.sinks.signed.SignedArtifactSink","title":"SignedArtifactSink  <code>dataclass</code>","text":"<pre><code>SignedArtifactSink(base_path: str | Path, bundle_name: str | None = None, timestamped: bool = True, results_name: str = 'results.json', signature_name: str = 'signature.json', manifest_name: str = 'manifest.json', algorithm: Literal['hmac-sha256', 'hmac-sha512', 'rsa-pss-sha256', 'ecdsa-p256-sha256'] = 'hmac-sha256', key: str | bytes | None = None, key_env: str | None = 'ELSPETH_SIGNING_KEY', public_key_env: str | None = None, key_vault_secret_uri: str | None = None, on_error: str = 'abort')\n</code></pre> <p>               Bases: <code>BasePlugin</code>, <code>ResultSink</code></p> <p>Write a signed artifact bundle (e.g., SBOM + signature) to disk.</p> <p>Inherits from BasePlugin to provide security enforcement (ADR-004).</p>"},{"location":"api-reference/plugins/generated-sinks/#elspeth.plugins.nodes.sinks.signed.SignedArtifactSink-functions","title":"Functions","text":""},{"location":"api-reference/plugins/generated-sinks/#elspeth.plugins.nodes.sinks.signed.SignedArtifactSink.write","title":"write","text":"<pre><code>write(results: dict[str, Any], *, metadata: dict[str, Any] | None = None) -&gt; None\n</code></pre> Source code in <code>elspeth/plugins/nodes/sinks/signed.py</code> <pre><code>def write(self, results: dict[str, Any], *, metadata: dict[str, Any] | None = None) -&gt; None:\n    metadata = metadata or {}\n    timestamp = datetime.now(timezone.utc)\n    try:\n        plugin_logger = getattr(self, \"plugin_logger\", None)\n        if plugin_logger:\n            plugin_logger.log_event(\n                \"sink_write_attempt\",\n                message=f\"Signed artifact write attempt: {self.base_path}/{self.bundle_name or 'signed'}\",\n                metadata={\"path\": str(self.base_path)},\n            )\n        bundle_dir = self._resolve_bundle_dir(metadata, timestamp)\n        bundle_dir.mkdir(parents=True, exist_ok=True)\n\n        results_path = bundle_dir / self.results_name\n        results_bytes = json.dumps(results, indent=2, sort_keys=True).encode(\"utf-8\")\n        results_path.write_bytes(results_bytes)\n\n        key = self._resolve_key()\n        signature_value = generate_signature(results_bytes, key, algorithm=self.algorithm)\n        key_fp = None\n        # For asymmetric signing, compute a public key fingerprint if possible\n        if self.algorithm.startswith(\"rsa-\") or self.algorithm.startswith(\"ecdsa-\"):\n            pub_pem: str | bytes | None = os.getenv(self.public_key_env) if self.public_key_env else None\n            # If public key not provided, attempt to derive from private PEM (best-effort)\n            if not pub_pem:\n                if isinstance(key, (bytes, bytearray)):\n                    if b\"BEGIN PUBLIC KEY\" in key:\n                        pub_pem = key  # bytes OK\n                elif isinstance(key, str):\n                    if \"BEGIN PUBLIC KEY\" in key:\n                        pub_pem = key  # str OK\n            if pub_pem:\n                try:\n                    key_fp = public_key_fingerprint(pub_pem)\n                except Exception as exc:  # nosec - optional\n                    logger.debug(\"Failed to compute public key fingerprint: %s\", exc, exc_info=False)\n                    key_fp = None\n        signature_payload = {\n            \"algorithm\": self.algorithm,\n            \"signature\": signature_value,\n            \"generated_at\": timestamp.isoformat(),\n            \"target\": self.results_name,\n        }\n        if key_fp:\n            signature_payload[\"key_fingerprint\"] = key_fp\n        signature_path = bundle_dir / self.signature_name\n        signature_path.write_text(json.dumps(signature_payload, indent=2, sort_keys=True), encoding=\"utf-8\")\n\n        manifest = self._build_manifest(results, metadata, timestamp, signature_value)\n        manifest_path = bundle_dir / self.manifest_name\n        manifest_path.write_text(json.dumps(manifest, indent=2, sort_keys=True), encoding=\"utf-8\")\n        if plugin_logger:\n            total_bytes = 0\n            for p in (results_path, signature_path, manifest_path):\n                try:\n                    total_bytes += p.stat().st_size\n                except (OSError, PermissionError):  # tolerate stat() errors; do not block artifact write\n                    pass\n            plugin_logger.log_event(\n                \"sink_write\",\n                message=f\"Signed artifact written under {bundle_dir}\",\n                metrics={\"bytes\": total_bytes, \"files\": 3},\n                metadata={\"path\": str(bundle_dir)},\n            )\n    except Exception as exc:\n        if self.on_error == \"skip\":\n            logger.warning(\"Signed artifact sink failed; skipping bundle creation: %s\", exc)\n            plugin_logger = getattr(self, \"plugin_logger\", None)\n            if plugin_logger:\n                plugin_logger.log_error(exc, context=\"signed artifact sink write\", recoverable=True)\n            return\n        raise\n</code></pre>"},{"location":"api-reference/plugins/generated-sinks/#zipresultsink","title":"ZipResultSink","text":"<p>Configuration Type: <code>zip_bundle</code> Security: \u2705 Trusted Downgrade Source: <code>src/elspeth/plugins/nodes/sinks/zip_bundle.py</code></p> <p>Example Configuration: <pre><code>sinks:\n  - type: zip_bundle\n  security_level: OFFICIAL\n  allow_downgrade: true\n</code></pre></p>"},{"location":"api-reference/plugins/generated-sinks/#elspeth.plugins.nodes.sinks.zip_bundle.ZipResultSink","title":"ZipResultSink","text":"<pre><code>ZipResultSink(*, base_path: str | Path, bundle_name: str | None = None, timestamped: bool = True, include_manifest: bool = True, include_results: bool = True, include_csv: bool = False, manifest_name: str = 'manifest.json', results_name: str = 'results.json', csv_name: str = 'results.csv', on_error: str = 'abort', sanitize_formulas: bool = True, sanitize_guard: str = \"'\", allowed_base_path: str | Path | None = None)\n</code></pre> <p>               Bases: <code>BasePlugin</code>, <code>ResultSink</code></p> <p>Bundle results, manifest, and optional CSV into a compressed ZIP archive at the configured path.</p> <p>Inherits from BasePlugin to provide security enforcement (ADR-004).</p> Source code in <code>elspeth/plugins/nodes/sinks/zip_bundle.py</code> <pre><code>def __init__(\n    self,\n    *,\n    base_path: str | Path,\n    bundle_name: str | None = None,\n    timestamped: bool = True,\n    include_manifest: bool = True,\n    include_results: bool = True,\n    include_csv: bool = False,\n    manifest_name: str = \"manifest.json\",\n    results_name: str = \"results.json\",\n    csv_name: str = \"results.csv\",\n    on_error: str = \"abort\",\n    sanitize_formulas: bool = True,\n    sanitize_guard: str = \"'\",\n    allowed_base_path: str | Path | None = None,\n) -&gt; None:\n    # Initialize BasePlugin with security level and downgrade policy (ADR-002-B: Immutable security policy)\n    super().__init__(\n        security_level=SecurityLevel.UNOFFICIAL,  # ADR-002-B: Immutable\n        allow_downgrade=True\n    )\n\n    self.base_path = Path(base_path)\n    self.bundle_name = bundle_name\n    self.timestamped = timestamped\n    self.include_manifest = include_manifest\n    self.include_results = include_results\n    self.include_csv = include_csv\n    self.manifest_name = manifest_name\n    self.results_name = results_name\n    self.csv_name = csv_name\n    if on_error not in {\"abort\", \"skip\"}:\n        raise ValueError(\"on_error must be 'abort' or 'skip'\")\n    self.on_error = on_error\n    if not sanitize_guard:\n        sanitize_guard = \"'\"\n    if len(sanitize_guard) != 1:\n        raise ValueError(\"sanitize_guard must be a single character\")\n    self.sanitize_formulas = sanitize_formulas\n    self.sanitize_guard = sanitize_guard\n    if not self.sanitize_formulas:\n        logger.warning(\"ZIP sink sanitization disabled; CSV artifacts may trigger formulas.\")\n    self._sanitization = {\n        \"enabled\": self.sanitize_formulas,\n        \"guard\": self.sanitize_guard,\n    }\n    self._last_archive_path: str | None = None\n    self._last_artifacts: dict[str, Any] = {}\n    self._additional_inputs: dict[str, list[Artifact]] = {}\n    # Runtime data classification tracking (separate from sink's security clearance)\n    # security_level (from BasePlugin) = sink's clearance level\n    # _artifact_security_level = runtime classification of written data (for artifact metadata)\n    self._artifact_security_level: SecurityLevel | None = None\n    self._artifact_determinism_level: DeterminismLevel | None = None\n    # Allowed base directory for writes; default to ./outputs\n    try:\n        default_base = Path(base_path).resolve()\n        self._allowed_base = Path(allowed_base_path).resolve() if allowed_base_path is not None else default_base\n    except Exception:  # pragma: no cover - defensive\n        self._allowed_base = Path.cwd().resolve()\n</code></pre>"},{"location":"api-reference/plugins/generated-sinks/#elspeth.plugins.nodes.sinks.zip_bundle.ZipResultSink-functions","title":"Functions","text":""},{"location":"api-reference/plugins/generated-sinks/#elspeth.plugins.nodes.sinks.zip_bundle.ZipResultSink.write","title":"write","text":"<pre><code>write(results: dict[str, Any], *, metadata: dict[str, Any] | None = None) -&gt; None\n</code></pre> Source code in <code>elspeth/plugins/nodes/sinks/zip_bundle.py</code> <pre><code>def write(self, results: dict[str, Any], *, metadata: dict[str, Any] | None = None) -&gt; None:\n    metadata = metadata or {}\n    timestamp = datetime.now(timezone.utc)\n    try:\n        plugin_logger = getattr(self, \"plugin_logger\", None)\n        archive_path = self._resolve_path(metadata, timestamp)\n        target = resolve_under_base(archive_path, self._allowed_base)\n\n        if plugin_logger:\n            plugin_logger.log_event(\n                \"sink_write_attempt\",\n                message=f\"ZIP write attempt: {target}\",\n                metadata={\"path\": str(target)},\n            )\n\n        def _safe_name(name: str) -&gt; str:\n            r\"\"\"Return a sanitized ZIP entry name.\n\n            - Drops any directory components\n            - Rejects NUL bytes (\\x00)\n            - Replaces characters outside [A-Za-z0-9._-] with '_'\n            - Avoids empty/bad names by falling back to 'artifact'\n            \"\"\"\n\n            if \"\\x00\" in name:\n                raise ValueError(\"ZIP entry name contains NUL byte\")\n            base = Path(name).name\n            if not base or base in {\".\", \"..\"}:\n                base = \"artifact\"\n            allowed = set(\"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789._-\")\n            sanitized = \"\".join(c if c in allowed else \"_\" for c in base)\n            # Ensure we didn't strip the entire name\n            return sanitized or \"artifact\"\n\n        def _writer(tmp_path: Path) -&gt; None:\n            with ZipFile(tmp_path, mode=\"w\", compression=ZIP_DEFLATED) as bundle:\n                if self.include_results:\n                    payload = json.dumps(results, indent=2, sort_keys=True)\n                    bundle.writestr(_safe_name(self.results_name), payload)\n\n                if self.include_manifest:\n                    manifest = self._build_manifest(results, metadata, timestamp)\n                    bundle.writestr(\n                        _safe_name(self.manifest_name),\n                        json.dumps(manifest, indent=2, sort_keys=True),\n                    )\n\n                if self.include_csv:\n                    csv_data = self._render_csv(results)\n                    bundle.writestr(_safe_name(self.csv_name), csv_data)\n\n                # Include upstream artifacts\n                counter = 0\n                for _, artifacts in self._additional_inputs.items():\n                    for artifact in artifacts:\n                        counter += 1\n                        name = None\n                        if artifact.metadata:\n                            name = artifact.metadata.get(\"filename\") or artifact.metadata.get(\"path\")\n                            if name:\n                                name = _safe_name(str(name))\n                        if not name and artifact.path:\n                            name = _safe_name(str(artifact.path))\n                        if not name:\n                            name = f\"artifact_{counter}\"\n                        data = self._read_artifact(artifact)\n                        bundle.writestr(name, data)\n\n        safe_atomic_write(target, _writer)\n        self._last_archive_path = str(target)\n        self._last_artifacts = {\n            \"results\": self.results_name if self.include_results else None,\n            \"manifest\": self.manifest_name if self.include_manifest else None,\n            \"csv\": self.csv_name if self.include_csv else None,\n            \"sanitization\": self._sanitization,\n        }\n        if metadata:\n            level = metadata.get(\"security_level\")\n            det = metadata.get(\"determinism_level\")\n            self._artifact_security_level = level if isinstance(level, SecurityLevel) else ensure_security_level(level)\n            self._artifact_determinism_level = det if isinstance(det, DeterminismLevel) else ensure_determinism_level(det)\n    except Exception as exc:\n        if self.on_error == \"skip\":\n            logger.warning(\"ZIP sink failed; skipping archive creation: %s\", exc)\n            plugin_logger = getattr(self, \"plugin_logger\", None)\n            if plugin_logger:\n                plugin_logger.log_error(exc, context=\"zip sink write\", recoverable=True)\n            return\n        raise\n    finally:\n        self._additional_inputs = {}\n</code></pre>"},{"location":"api-reference/plugins/generated-transforms/","title":"Transforms API (Auto-Generated)","text":"<p>Auto-Generated Documentation</p> <p>This file is automatically generated from source code. Do not edit manually.</p> <p>Generated from: <code>scripts/generate_plugin_docs.py</code> Last generated: Run <code>make docs-generate</code> to update</p> <p>API documentation for 4 transforms that process and transform data.</p>"},{"location":"api-reference/plugins/generated-transforms/#azureopenaiclient","title":"AzureOpenAIClient","text":"<p>Configuration Type: <code>azure_openai</code> Security: \u2705 Trusted Downgrade Source: <code>src/elspeth/plugins/nodes/transforms/llm/azure_openai.py</code></p> <p>Example Configuration: <pre><code>transform:\n  type: azure_openai\n  security_level: OFFICIAL\n  allow_downgrade: true\n</code></pre></p>"},{"location":"api-reference/plugins/generated-transforms/#elspeth.plugins.nodes.transforms.llm.azure_openai.AzureOpenAIClient","title":"AzureOpenAIClient","text":"<pre><code>AzureOpenAIClient(*, deployment: str | None = None, config: dict[str, Any], client: Any | None = None)\n</code></pre> <p>               Bases: <code>BasePlugin</code>, <code>LLMClientProtocol</code></p> <p>Thin wrapper around OpenAI's Azure client implementing LLMClientProtocol.</p> <p>Security policy: Enterprise Azure OpenAI operates at PROTECTED level (ADR-002-B).</p> <p>Parameters:</p> Name Type Description Default <code>deployment</code> <code>str | None</code> <p>Azure deployment name (optional, can be resolved from config or env).</p> <code>None</code> <code>config</code> <code>dict[str, Any]</code> <p>Azure OpenAI configuration dict (endpoint, API keys, model parameters).</p> required <code>client</code> <code>Any | None</code> <p>Pre-configured OpenAI client (optional, for dependency injection).</p> <code>None</code> <p>Initialize Azure OpenAI client with hard-coded security policy.</p> <p>ADR-002-B: Security policy is immutable. Azure OpenAI operates at PROTECTED level (maximum clearance for enterprise-managed endpoints) and can be trusted to downgrade.</p> Source code in <code>elspeth/plugins/nodes/transforms/llm/azure_openai.py</code> <pre><code>def __init__(\n    self,\n    *,\n    deployment: str | None = None,\n    config: dict[str, Any],\n    client: Any | None = None,\n):\n    \"\"\"Initialize Azure OpenAI client with hard-coded security policy.\n\n    ADR-002-B: Security policy is immutable. Azure OpenAI operates at PROTECTED level\n    (maximum clearance for enterprise-managed endpoints) and can be trusted to downgrade.\n    \"\"\"\n    super().__init__(\n        security_level=SecurityLevel.PROTECTED,  # ADR-002-B: Immutable policy\n        allow_downgrade=True,  # ADR-002-B: Immutable policy\n    )\n    self.config = config\n    self.temperature = config.get(\"temperature\")\n    self.max_tokens = config.get(\"max_tokens\")\n    # Bounded request timeouts for operational resilience\n    try:\n        self.request_timeout = float(config.get(\"timeout\", 30.0))\n    except (ValueError, TypeError):\n        self.request_timeout = 30.0\n    self.deployment = self._resolve_deployment(deployment)\n    self._client = client or self._create_client()\n</code></pre>"},{"location":"api-reference/plugins/generated-transforms/#elspeth.plugins.nodes.transforms.llm.azure_openai.AzureOpenAIClient-functions","title":"Functions","text":""},{"location":"api-reference/plugins/generated-transforms/#mockllmclient","title":"MockLLMClient","text":"<p>Configuration Type: <code>mock</code> Security: \u2705 Trusted Downgrade Source: <code>src/elspeth/plugins/nodes/transforms/llm/mock.py</code></p> <p>Example Configuration: <pre><code>transform:\n  type: mock\n  security_level: OFFICIAL\n  allow_downgrade: true\n</code></pre></p>"},{"location":"api-reference/plugins/generated-transforms/#elspeth.plugins.nodes.transforms.llm.mock.MockLLMClient","title":"MockLLMClient","text":"<pre><code>MockLLMClient(*, seed: int | None = None)\n</code></pre> <p>               Bases: <code>BasePlugin</code>, <code>LLMClientProtocol</code></p> <p>Deterministic mock client for tests and offline runs.</p> <p>Security policy: Test-only transform operates at UNOFFICIAL level (ADR-002-B).</p> <p>Parameters:</p> Name Type Description Default <code>seed</code> <code>int | None</code> <p>Optional seed for deterministic response generation (default: 0).</p> <code>None</code> <p>Initialize mock LLM client with hard-coded security policy.</p> <p>ADR-002-B: Security policy is immutable. Mock LLMs operate at UNOFFICIAL level and can be trusted to downgrade (test-only transform).</p> Source code in <code>elspeth/plugins/nodes/transforms/llm/mock.py</code> <pre><code>def __init__(\n    self,\n    *,\n    seed: int | None = None\n):\n    \"\"\"Initialize mock LLM client with hard-coded security policy.\n\n    ADR-002-B: Security policy is immutable. Mock LLMs operate at UNOFFICIAL level\n    and can be trusted to downgrade (test-only transform).\n    \"\"\"\n    super().__init__(\n        security_level=SecurityLevel.UNOFFICIAL,  # ADR-002-B: Immutable policy\n        allow_downgrade=True,  # ADR-002-B: Immutable policy\n    )\n    self.seed = seed or 0\n</code></pre>"},{"location":"api-reference/plugins/generated-transforms/#elspeth.plugins.nodes.transforms.llm.mock.MockLLMClient-functions","title":"Functions","text":""},{"location":"api-reference/plugins/generated-transforms/#httpopenaiclient","title":"HttpOpenAIClient","text":"<p>Configuration Type: <code>openai_http</code> Security: \u2705 Trusted Downgrade Source: <code>src/elspeth/plugins/nodes/transforms/llm/openai_http.py</code></p> <p>Example Configuration: <pre><code>transform:\n  type: openai_http\n  security_level: OFFICIAL\n  allow_downgrade: true\n</code></pre></p>"},{"location":"api-reference/plugins/generated-transforms/#elspeth.plugins.nodes.transforms.llm.openai_http.HttpOpenAIClient","title":"HttpOpenAIClient","text":"<pre><code>HttpOpenAIClient(*, api_base: str, api_key: str | None = None, api_key_env: str | None = None, model: str = 'gpt-4o-mini', temperature: float | None = None, max_tokens: int | None = None, timeout: float = 30.0, retry_total: int = 3, backoff_factor: float = 0.5, status_forcelist: tuple[int, ...] | None = None)\n</code></pre> <p>               Bases: <code>BasePlugin</code>, <code>LLMClientProtocol</code></p> <p>Minimal client for standard /v1/chat/completions endpoints.</p> <p>Security policy: Public HTTP OpenAI operates at OFFICIAL level (ADR-002-B).</p> <p>Parameters:</p> Name Type Description Default <code>api_base</code> <code>str</code> <p>Base URL for OpenAI-compatible API endpoint.</p> required <code>api_key</code> <code>str | None</code> <p>API key for authentication (optional if using api_key_env).</p> <code>None</code> <code>api_key_env</code> <code>str | None</code> <p>Environment variable name containing API key.</p> <code>None</code> <code>model</code> <code>str</code> <p>Model identifier to use.</p> <code>'gpt-4o-mini'</code> <code>temperature</code> <code>float | None</code> <p>Sampling temperature (0-2).</p> <code>None</code> <code>max_tokens</code> <code>int | None</code> <p>Maximum tokens in response.</p> <code>None</code> <code>timeout</code> <code>float</code> <p>Request timeout in seconds.</p> <code>30.0</code> <code>retry_total</code> <code>int</code> <p>Total number of retry attempts.</p> <code>3</code> <code>backoff_factor</code> <code>float</code> <p>Backoff factor for retries.</p> <code>0.5</code> <code>status_forcelist</code> <code>tuple[int, ...] | None</code> <p>HTTP status codes to retry on.</p> <code>None</code> <p>Initialize HTTP OpenAI client with hard-coded security policy.</p> <p>ADR-002-B: Security policy is immutable. HTTP OpenAI operates at OFFICIAL level (public API endpoint) and can be trusted to downgrade.</p> Source code in <code>elspeth/plugins/nodes/transforms/llm/openai_http.py</code> <pre><code>def __init__(\n    self,\n    *,\n    api_base: str,\n    api_key: str | None = None,\n    api_key_env: str | None = None,\n    model: str = \"gpt-4o-mini\",\n    temperature: float | None = None,\n    max_tokens: int | None = None,\n    timeout: float = 30.0,\n    retry_total: int = 3,\n    backoff_factor: float = 0.5,\n    status_forcelist: tuple[int, ...] | None = None,\n) -&gt; None:\n    \"\"\"Initialize HTTP OpenAI client with hard-coded security policy.\n\n    ADR-002-B: Security policy is immutable. HTTP OpenAI operates at OFFICIAL level\n    (public API endpoint) and can be trusted to downgrade.\n    \"\"\"\n    super().__init__(\n        security_level=SecurityLevel.OFFICIAL,  # ADR-002-B: Immutable policy\n        allow_downgrade=True,  # ADR-002-B: Immutable policy\n    )\n    self.api_base = api_base.rstrip(\"/\")\n    # Defense-in-depth: validate endpoint even when instantiated directly\n    try:\n        from elspeth.core.security import validate_http_api_endpoint\n\n        validate_http_api_endpoint(endpoint=self.api_base, security_level=SecurityLevel.OFFICIAL)\n    except ValueError as exc:  # pragma: no cover - validation path exercised via registry tests\n        # Surface endpoint validation issues as ValueError\n        raise ValueError(f\"HTTP API endpoint validation failed for '{self.api_base}': {exc}\") from exc\n    if not api_key and api_key_env:\n        api_key = os.getenv(api_key_env)\n    self.api_key = api_key\n    self.model = model\n    self.temperature = temperature\n    self.max_tokens = max_tokens\n    self.timeout = timeout\n    # Configure a session with bounded retries for transient errors\n    self.session = requests.Session()\n    try:  # pragma: no cover - adapter internals vary by env\n        if status_forcelist is None:\n            status_forcelist = (429, 500, 502, 503, 504)\n        retry = Retry(\n            total=retry_total,\n            backoff_factor=backoff_factor,\n            status_forcelist=list(status_forcelist),\n            allowed_methods=[\"GET\", \"POST\"],\n            raise_on_status=False,\n        )\n        adapter = HTTPAdapter(max_retries=retry)\n        self.session.mount(\"https://\", adapter)\n        # Mount HTTP only when explicitly using localhost endpoints; never for external traffic.\n        if self.api_base.startswith(\"http://\"):  # NOSONAR\n            # Endpoint validation already restricts HTTP to localhost/loopback only.\n            # Enforce with an explicit runtime check for defense-in-depth.\n            from urllib.parse import urlparse\n\n            host = (urlparse(self.api_base).hostname or \"\").lower()\n            if not (host == \"localhost\" or host.startswith(\"127.\") or host == \"::1\"):\n                raise RuntimeError(f\"HTTP endpoints must be localhost/loopback only, got: {self.api_base}\")\n            self.session.mount(\"http://\", adapter)  # NOSONAR - localhost-only by policy\n    except (ValueError, TypeError, AttributeError) as exc:\n        # If retry adapter isn't available, proceed without retries\n        logger.debug(\"HTTP retry adapter not mounted; proceeding without retries: %s\", exc, exc_info=False)\n</code></pre>"},{"location":"api-reference/plugins/generated-transforms/#elspeth.plugins.nodes.transforms.llm.openai_http.HttpOpenAIClient-functions","title":"Functions","text":""},{"location":"api-reference/plugins/generated-transforms/#staticllmclient","title":"StaticLLMClient","text":"<p>Configuration Type: <code>static</code> Security: \u2705 Trusted Downgrade Source: <code>src/elspeth/plugins/nodes/transforms/llm/static.py</code></p> <p>Example Configuration: <pre><code>transform:\n  type: static\n  security_level: OFFICIAL\n  allow_downgrade: true\n</code></pre></p>"},{"location":"api-reference/plugins/generated-transforms/#elspeth.plugins.nodes.transforms.llm.static.StaticLLMClient","title":"StaticLLMClient","text":"<pre><code>StaticLLMClient(*, content: str, score: float | None = None, metrics: Mapping[str, Any] | None = None)\n</code></pre> <p>               Bases: <code>BasePlugin</code>, <code>LLMClientProtocol</code></p> <p>Return predefined content and metrics for every request.</p> <p>Security policy: Test-only transform operates at UNOFFICIAL level (ADR-002-B).</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>Static response content to return for all requests (REQUIRED).</p> required <code>score</code> <code>float | None</code> <p>Optional score metric to include in response.</p> <code>None</code> <code>metrics</code> <code>Mapping[str, Any] | None</code> <p>Optional additional metrics to include in response.</p> <code>None</code> <p>Initialize static LLM client with hard-coded security policy.</p> <p>ADR-002-B: Security policy is immutable. Static LLMs operate at UNOFFICIAL level and can be trusted to downgrade (test-only transform).</p> Source code in <code>elspeth/plugins/nodes/transforms/llm/static.py</code> <pre><code>def __init__(\n    self,\n    *,\n    content: str,  # Required - no default allowed\n    score: float | None = None,\n    metrics: Mapping[str, Any] | None = None,\n) -&gt; None:\n    \"\"\"Initialize static LLM client with hard-coded security policy.\n\n    ADR-002-B: Security policy is immutable. Static LLMs operate at UNOFFICIAL level\n    and can be trusted to downgrade (test-only transform).\n    \"\"\"\n    super().__init__(\n        security_level=SecurityLevel.UNOFFICIAL,  # ADR-002-B: Immutable policy\n        allow_downgrade=True,  # ADR-002-B: Immutable policy\n    )\n    self.content = content\n    self.score = score\n    self.extra_metrics = dict(metrics or {})\n</code></pre>"},{"location":"api-reference/plugins/generated-transforms/#elspeth.plugins.nodes.transforms.llm.static.StaticLLMClient-functions","title":"Functions","text":""},{"location":"api-reference/registries/base/","title":"Plugin Registry","text":"<p>Factory pattern for plugin registration and instantiation with schema validation.</p>"},{"location":"api-reference/registries/base/#overview","title":"Overview","text":"<p>Elspeth uses a registry pattern to manage plugin lifecycle:</p> <ol> <li>Register plugin factories with schemas</li> <li>Validate configuration against schemas</li> <li>Create plugin instances with context propagation</li> <li>Enumerate available plugins</li> </ol> <p>All registries inherit from <code>BasePluginRegistry[T]</code> for consistency.</p>"},{"location":"api-reference/registries/base/#class-documentation","title":"Class Documentation","text":""},{"location":"api-reference/registries/base/#elspeth.core.registries.base.BasePluginRegistry","title":"BasePluginRegistry","text":"<pre><code>BasePluginRegistry(plugin_type: str)\n</code></pre> <p>               Bases: <code>Generic[T]</code></p> <p>Base class for plugin registries.</p> <p>Provides common functionality for registering, validating, and creating plugins with consistent security context handling. This class consolidates the registry pattern previously duplicated across 5 implementations.</p> <p>Attributes:</p> Name Type Description <code>plugin_type</code> <p>Human-readable plugin type name</p> <code>_plugins</code> <code>PluginFactoryMap[T]</code> <p>Internal registry of factories</p> Example <p>registry = BasePluginRegistryDataSource registry.register( ...     \"csv\", ...     lambda opts, ctx: CSVDataSource(**opts), ...     schema={...} ... ) plugin = registry.create( ...     name=\"csv\", ...     options={\"path\": \"data.csv\"}, ...     require_security=True ... )</p> <p>Initialize a plugin registry.</p> <p>Parameters:</p> Name Type Description Default <code>plugin_type</code> <code>str</code> <p>Human-readable type name (e.g., \"datasource\", \"llm\")</p> required Source code in <code>elspeth/core/registries/base.py</code> <pre><code>def __init__(self, plugin_type: str):\n    \"\"\"\n    Initialize a plugin registry.\n\n    Args:\n        plugin_type: Human-readable type name (e.g., \"datasource\", \"llm\")\n    \"\"\"\n    self.plugin_type = plugin_type\n    self._plugins: PluginFactoryMap[T] = {}\n</code></pre>"},{"location":"api-reference/registries/base/#elspeth.core.registries.base.BasePluginRegistry-functions","title":"Functions","text":""},{"location":"api-reference/registries/base/#elspeth.core.registries.base.BasePluginRegistry.register","title":"register","text":"<pre><code>register(name: str, factory: Callable[[dict[str, Any], PluginContext], T], *, schema: Mapping[str, Any] | None = None, requires_input_schema: bool = False, capabilities: Iterable[str] | None = None, declared_security_level: str | None = None) -&gt; None\n</code></pre> <p>Register a plugin factory with its immutable security policy.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Plugin name (used for lookup)</p> required <code>factory</code> <code>Callable[[dict[str, Any], PluginContext], T]</code> <p>Factory callable that creates plugin instances</p> required <code>schema</code> <code>Mapping[str, Any] | None</code> <p>Optional JSON schema for validation</p> <code>None</code> <code>capabilities</code> <code>Iterable[str] | None</code> <p>Optional iterable of capability flags exposed to callers</p> <code>None</code> <code>declared_security_level</code> <code>str | None</code> <p>Plugin's hard-coded security level (ADR-002-B). This is the security level declared by the plugin author, not configurable by users.</p> <code>None</code> Source code in <code>elspeth/core/registries/base.py</code> <pre><code>def register(\n    self,\n    name: str,\n    factory: Callable[[dict[str, Any], PluginContext], T],\n    *,\n    schema: Mapping[str, Any] | None = None,\n    requires_input_schema: bool = False,\n    capabilities: Iterable[str] | None = None,\n    declared_security_level: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Register a plugin factory with its immutable security policy.\n\n    Args:\n        name: Plugin name (used for lookup)\n        factory: Factory callable that creates plugin instances\n        schema: Optional JSON schema for validation\n        capabilities: Optional iterable of capability flags exposed to callers\n        declared_security_level: Plugin's hard-coded security level (ADR-002-B).\n            This is the security level declared by the plugin author, not configurable by users.\n    \"\"\"\n    plugin_factory = BasePluginFactory(\n        create=factory,\n        schema=schema,\n        plugin_type=self.plugin_type,\n        requires_input_schema=requires_input_schema,\n        capabilities=frozenset(capabilities or ()),\n        declared_security_level=declared_security_level,\n    )\n    # Pre-compile JSON schema validator once at registration to avoid first-call latency\n    if schema is not None:\n        try:\n            import importlib  # pylint: disable=import-outside-toplevel\n\n            validator_cls = importlib.import_module(\"jsonschema\").Draft202012Validator\n            plugin_factory._compiled_validator = validator_cls(schema)\n            # Warm up validator by running a trivial check to pre-initialize internals\n            if plugin_factory._compiled_validator is not None:\n                try:\n                    _ = list(plugin_factory._compiled_validator.iter_errors({}))  # noqa: F841\n                except Exception:\n                    import logging  # pylint: disable=import-outside-toplevel\n\n                    logging.getLogger(__name__).debug(\"Warm-up validation error ignored for %s\", name, exc_info=True)\n        except Exception:\n            plugin_factory._compiled_validator = None  # Fallback; validate() will handle\n    self._plugins[name] = plugin_factory\n</code></pre>"},{"location":"api-reference/registries/base/#elspeth.core.registries.base.BasePluginRegistry.create","title":"create","text":"<pre><code>create(name: str, options: dict[str, Any], *, provenance: Iterable[str] | None = None, parent_context: PluginContext | None = None, require_determinism: bool = True) -&gt; T\n</code></pre> <p>Create a plugin instance with full context handling.</p> <p>This method handles the complete plugin lifecycle: 1. Extracts and normalizes security/determinism levels 2. Creates or derives plugin context 3. Validates options against schema 4. Instantiates the plugin 5. Applies context to the plugin</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Plugin name</p> required <code>options</code> <code>dict[str, Any]</code> <p>Plugin configuration options</p> required <code>provenance</code> <code>Iterable[str] | None</code> <p>Optional provenance source identifiers</p> <code>None</code> <code>parent_context</code> <code>PluginContext | None</code> <p>Optional parent context for inheritance</p> <code>None</code> <code>require_determinism</code> <code>bool</code> <p>Whether determinism_level is required</p> <code>True</code> <p>Returns:</p> Type Description <code>T</code> <p>Instantiated plugin of type T</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If plugin not found</p> <code>ConfigurationError</code> <p>If validation or creation fails</p> Source code in <code>elspeth/core/registries/base.py</code> <pre><code>def create(\n    self,\n    name: str,\n    options: dict[str, Any],\n    *,\n    provenance: Iterable[str] | None = None,\n    parent_context: PluginContext | None = None,\n    require_determinism: bool = True,\n) -&gt; T:\n    \"\"\"\n    Create a plugin instance with full context handling.\n\n    This method handles the complete plugin lifecycle:\n    1. Extracts and normalizes security/determinism levels\n    2. Creates or derives plugin context\n    3. Validates options against schema\n    4. Instantiates the plugin\n    5. Applies context to the plugin\n\n    Args:\n        name: Plugin name\n        options: Plugin configuration options\n        provenance: Optional provenance source identifiers\n        parent_context: Optional parent context for inheritance\n        require_determinism: Whether determinism_level is required\n\n    Returns:\n        Instantiated plugin of type T\n\n    Raises:\n        ValueError: If plugin not found\n        ConfigurationError: If validation or creation fails\n    \"\"\"\n    factory = self._get_factory(name)\n\n    # ADR-002-B: ALWAYS include plugin's declared_security_level in coalescing\n    # SECURITY: Factory default MUST participate regardless of parent_context\n    # to enforce weakest-link model (prevents parent SECRET from bypassing child UNOFFICIAL)\n    definition_for_extraction = dict(options)\n    if (factory.declared_security_level is not None\n        and \"security_level\" not in options):\n        # ALWAYS add declared level - coalesce_security_level will enforce weakest-link\n        definition_for_extraction[\"security_level\"] = factory.declared_security_level\n\n    # Extract and normalize security levels (ADR-001: always required, no backdoors)\n    security_level, determinism_level, sources = extract_security_levels(\n        definition=definition_for_extraction,\n        options=options,\n        plugin_type=self.plugin_type,\n        plugin_name=name,\n        parent_context=parent_context,\n        require_determinism=require_determinism,\n    )\n\n    # Add provenance if provided\n    if provenance:\n        sources.extend(provenance)\n\n    # Create plugin context\n    context = create_plugin_context(\n        plugin_name=name,\n        plugin_kind=self.plugin_type,\n        security_level=security_level,\n        determinism_level=determinism_level,\n        provenance=sources,\n        parent_context=parent_context,\n    )\n\n    # Prepare payload (strip framework keys)\n    payload = prepare_plugin_payload(options)\n\n    # Instantiate plugin using BasePluginFactory\n    return factory.instantiate(\n        payload,\n        plugin_context=context,\n        schema_context=f\"{self.plugin_type}:{name}\",\n    )\n</code></pre>"},{"location":"api-reference/registries/base/#elspeth.core.registries.base.BasePluginRegistry.list_plugins","title":"list_plugins","text":"<pre><code>list_plugins() -&gt; list[str]\n</code></pre> <p>Return list of registered plugin names.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>Sorted list of registered plugin names</p> Source code in <code>elspeth/core/registries/base.py</code> <pre><code>def list_plugins(self) -&gt; list[str]:\n    \"\"\"\n    Return list of registered plugin names.\n\n    Returns:\n        Sorted list of registered plugin names\n    \"\"\"\n    return sorted(self._plugins.keys())\n</code></pre>"},{"location":"api-reference/registries/base/#elspeth.core.registries.base.BasePluginFactory","title":"BasePluginFactory  <code>dataclass</code>","text":"<pre><code>BasePluginFactory(create: Callable[[dict[str, Any], PluginContext], T], schema: Mapping[str, Any] | None = None, plugin_type: str = 'plugin', requires_input_schema: bool = False, capabilities: frozenset[str] = frozenset(), declared_security_level: str | None = None)\n</code></pre> <p>               Bases: <code>Generic[T]</code></p> <p>Base factory for creating and validating plugin instances.</p> <p>Consolidates the factory pattern repeated across 5 registries. Provides schema validation and consistent plugin instantiation.</p> <p>Attributes:</p> Name Type Description <code>create</code> <code>Callable[[dict[str, Any], PluginContext], T]</code> <p>Factory callable that creates plugin instances</p> <code>schema</code> <code>Mapping[str, Any] | None</code> <p>Optional JSON schema for validation</p> <code>plugin_type</code> <code>str</code> <p>Human-readable plugin type (e.g., \"datasource\", \"llm\")</p> <code>capabilities</code> <code>frozenset[str]</code> <p>Declared capability flags exposed to registry consumers</p> Example <p>def create_my_plugin(opts: Dict, ctx: PluginContext) -&gt; MyPlugin: ...     return MyPlugin(**opts) factory = BasePluginFactory( ...     create=create_my_plugin, ...     schema={\"type\": \"object\", \"properties\": {...}}, ...     plugin_type=\"my_plugin\" ... ) plugin = factory.instantiate( ...     options={\"key\": \"value\"}, ...     plugin_context=context, ...     schema_context=\"my_plugin:my_name\" ... )</p>"},{"location":"api-reference/registries/base/#elspeth.core.registries.base.BasePluginFactory-attributes","title":"Attributes","text":""},{"location":"api-reference/registries/base/#elspeth.core.registries.base.BasePluginFactory.create","title":"create  <code>instance-attribute</code>","text":"<pre><code>create: Callable[[dict[str, Any], PluginContext], T]\n</code></pre>"},{"location":"api-reference/registries/base/#elspeth.core.registries.base.BasePluginFactory-functions","title":"Functions","text":""},{"location":"api-reference/registries/base/#elspeth.core.registries.base.BasePluginFactory.validate","title":"validate","text":"<pre><code>validate(options: dict[str, Any], *, context: str) -&gt; None\n</code></pre> <p>Validate options against the schema.</p> <p>Parameters:</p> Name Type Description Default <code>options</code> <code>dict[str, Any]</code> <p>Plugin options dictionary to validate</p> required <code>context</code> <code>str</code> <p>Context string for error messages (e.g., \"datasource:csv\")</p> required <p>Raises:</p> Type Description <code>ConfigurationError</code> <p>If validation fails</p> Source code in <code>elspeth/core/registries/base.py</code> <pre><code>def validate(self, options: dict[str, Any], *, context: str) -&gt; None:\n    \"\"\"\n    Validate options against the schema.\n\n    Args:\n        options: Plugin options dictionary to validate\n        context: Context string for error messages (e.g., \"datasource:csv\")\n\n    Raises:\n        ConfigurationError: If validation fails\n    \"\"\"\n    if self.schema is None:\n        return\n\n    # Fast path: cache compiled JSON Schema validator per factory to avoid repeated parse/compile cost\n    try:\n        if self._compiled_validator is None:\n            # Lazy import to avoid cost when unused, without type stubs dependency\n            import importlib  # pylint: disable=import-outside-toplevel\n\n            validator_cls = importlib.import_module(\"jsonschema\").Draft202012Validator\n            self._compiled_validator = validator_cls(self.schema)\n        if self._compiled_validator is None:\n            raise RuntimeError(f\"Failed to compile JSON schema validator for {context}. Ensure jsonschema package is installed.\")\n        # Validate options\n        errors = list(self._compiled_validator.iter_errors(options or {}))\n        if errors:\n            # Build a concise error message similar to validate_schema\n            formatted = []\n            for err in errors:\n                path = \".\".join(str(p) for p in err.path) if err.path else context\n                formatted.append(f\"{context}: {err.message} (at {path})\")\n            raise ConfigurationError(\"\\n\".join(formatted))\n    except Exception:  # Fallback to original path for compatibility/error formatting\n        errors = list(validate_schema(options or {}, self.schema, context=context))\n        if errors:\n            message = \"\\n\".join(msg.format() for msg in errors)\n            raise ConfigurationError(message)\n</code></pre>"},{"location":"api-reference/registries/base/#elspeth.core.registries.base.BasePluginFactory.instantiate","title":"instantiate","text":"<pre><code>instantiate(options: dict[str, Any], *, plugin_context: PluginContext, schema_context: str) -&gt; T\n</code></pre> <p>Validate and create a plugin instance.</p> <p>This method: 1. Validates options against the schema 2. Calls the factory function to create the plugin 3. Applies the plugin context to the instance</p> <p>Parameters:</p> Name Type Description Default <code>options</code> <code>dict[str, Any]</code> <p>Plugin configuration options</p> required <code>plugin_context</code> <code>PluginContext</code> <p>Security and provenance context</p> required <code>schema_context</code> <code>str</code> <p>Context string for validation errors</p> required <p>Returns:</p> Type Description <code>T</code> <p>Instantiated plugin of type T</p> <p>Raises:</p> Type Description <code>ConfigurationError</code> <p>If validation fails</p> Source code in <code>elspeth/core/registries/base.py</code> <pre><code>def instantiate(\n    self,\n    options: dict[str, Any],\n    *,\n    plugin_context: PluginContext,\n    schema_context: str,\n) -&gt; T:\n    \"\"\"\n    Validate and create a plugin instance.\n\n    This method:\n    1. Validates options against the schema\n    2. Calls the factory function to create the plugin\n    3. Applies the plugin context to the instance\n\n    Args:\n        options: Plugin configuration options\n        plugin_context: Security and provenance context\n        schema_context: Context string for validation errors\n\n    Returns:\n        Instantiated plugin of type T\n\n    Raises:\n        ConfigurationError: If validation fails\n    \"\"\"\n    self.validate(options, context=schema_context)\n    plugin = self.create(options, plugin_context)\n\n    # Layer 3: Verify declared security_level matches actual (ADR-002-B, VULN-004)\n    if self.declared_security_level is not None:\n        # Only verify if plugin has security_level attribute\n        if hasattr(plugin, \"security_level\"):\n            actual_security_level = plugin.security_level\n\n            # SECURITY VALIDATION: Enforce SecurityLevel enum (ADR-002-B immutable policy)\n            # SecurityLevel is defined as class SecurityLevel(str, Enum), so isinstance(SecurityLevel.X, str) is True\n            # We check for SecurityLevel enum specifically to reject plain strings or other types\n            from elspeth.core.base.types import SecurityLevel  # pylint: disable=import-outside-toplevel\n\n            if not isinstance(actual_security_level, SecurityLevel):\n                # Plain string or other type - not a SecurityLevel enum\n                # This prevents regressions where plugins return strings instead of enums\n                raise ConfigurationError(\n                    f\"CRITICAL SECURITY POLICY VIOLATION: Plugin {type(plugin).__name__} \"\n                    f\"returns {type(actual_security_level).__name__} security_level='{actual_security_level}'. \"\n                    f\"ALL plugins MUST return SecurityLevel enum instance. \"\n                    f\"Update plugin to use SecurityLevel.{str(actual_security_level).upper()} instead.\"\n                )\n\n            # SecurityLevel enum comparison: compare enum.value (string) against declared (string)\n            if actual_security_level.value != self.declared_security_level:\n                raise ConfigurationError(\n                    f\"{schema_context}: Plugin declares security_level={self.declared_security_level} \"\n                    f\"but has actual security_level={actual_security_level.value}. \"\n                    \"Plugin implementation must match registry declaration (ADR-002-B).\"\n                )\n\n    # Attach factory metadata for downstream enforcement (e.g., input_schema requirement)\n    try:\n        setattr(plugin, \"_elspeth_requires_input_schema\", bool(self.requires_input_schema))\n    except Exception:  # pragma: no cover - best effort\n        # Best-effort; proceed even if plugin does not allow attribute assignment\n        import logging  # pylint: disable=import-outside-toplevel\n\n        logging.getLogger(__name__).debug(\"Failed to set _elspeth_requires_input_schema on %s\", type(plugin).__name__, exc_info=True)\n    apply_plugin_context(plugin, plugin_context)\n    return plugin\n</code></pre>"},{"location":"api-reference/registries/base/#usage-examples","title":"Usage Examples","text":""},{"location":"api-reference/registries/base/#creating-a-registry","title":"Creating a Registry","text":"<pre><code>from elspeth.core.registries.base import BasePluginRegistry, BasePluginFactory\nfrom elspeth.core.base.plugin_context import PluginContext\nfrom elspeth.core.base.types import SecurityLevel\n\n# Define plugin type\nclass MyPlugin:\n    def __init__(self, *, name: str, value: int):\n        self.name = name\n        self.value = value\n\n# Create registry\nregistry = BasePluginRegistry[MyPlugin]()\n\n# Define factory function\ndef create_my_plugin(opts: dict, ctx: PluginContext) -&gt; MyPlugin:\n    return MyPlugin(name=opts['name'], value=opts['value'])\n\n# Define schema\nmy_plugin_schema = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"name\": {\"type\": \"string\"},\n        \"value\": {\"type\": \"integer\", \"minimum\": 0}\n    },\n    \"required\": [\"name\", \"value\"]\n}\n\n# Register plugin\nregistry.register(\n    \"my_plugin\",\n    factory=BasePluginFactory(\n        create=create_my_plugin,\n        schema=my_plugin_schema,\n        plugin_type=\"my_plugin\"\n    )\n)\n</code></pre>"},{"location":"api-reference/registries/base/#creating-plugin-instances","title":"Creating Plugin Instances","text":"<pre><code>from elspeth.core.base.plugin_context import PluginContext\n\n# Create context\ncontext = PluginContext(\n    security_level=SecurityLevel.OFFICIAL,\n    run_id=\"test-run-001\",\n    plugin_kind=\"my_plugin\"\n)\n\n# Create plugin instance\nplugin = registry.create(\n    \"my_plugin\",\n    options={\"name\": \"example\", \"value\": 42},\n    plugin_context=context\n)\n\nprint(plugin.name)   # \"example\"\nprint(plugin.value)  # 42\n</code></pre>"},{"location":"api-reference/registries/base/#schema-validation","title":"Schema Validation","text":"<pre><code>from elspeth.core.validation.base import ConfigurationError\n\n# Valid configuration\nvalid_opts = {\"name\": \"test\", \"value\": 10}\nplugin = registry.create(\"my_plugin\", valid_opts, context)  # \u2705 OK\n\n# Invalid configuration (missing required field)\ninvalid_opts = {\"name\": \"test\"}  # Missing 'value'\ntry:\n    plugin = registry.create(\"my_plugin\", invalid_opts, context)\nexcept ConfigurationError as e:\n    print(f\"Validation failed: {e}\")  # \u274c Missing required field 'value'\n\n# Invalid configuration (wrong type)\ninvalid_opts = {\"name\": \"test\", \"value\": \"not a number\"}\ntry:\n    plugin = registry.create(\"my_plugin\", invalid_opts, context)\nexcept ConfigurationError as e:\n    print(f\"Validation failed: {e}\")  # \u274c 'value' must be integer\n</code></pre>"},{"location":"api-reference/registries/base/#listing-available-plugins","title":"Listing Available Plugins","text":"<pre><code># Get all registered plugin names\nplugins = registry.list_plugins()\nprint(plugins)  # ['my_plugin']\n\n# Get schema for specific plugin\nschema = registry.get_schema(\"my_plugin\")\nprint(schema)  # {...}\n</code></pre>"},{"location":"api-reference/registries/base/#built-in-registries","title":"Built-In Registries","text":"<p>Elspeth provides several specialized registries:</p> Registry Module Purpose DatasourceRegistry <code>elspeth.core.registries.datasources</code> CSV, Azure Blob datasources TransformRegistry <code>elspeth.core.registries.transforms</code> LLM clients and middleware SinkRegistry <code>elspeth.core.registries.sinks</code> CSV, Excel, signed artifacts ExperimentPluginRegistry <code>elspeth.core.experiments.plugin_registry</code> Row, aggregation, validation plugins"},{"location":"api-reference/registries/base/#example-using-datasource-registry","title":"Example: Using Datasource Registry","text":"<pre><code>from elspeth.core.registries.datasources import datasource_registry\nfrom elspeth.core.base.plugin_context import PluginContext\n\n# List available datasources\ndatasources = datasource_registry.list_plugins()\nprint(datasources)  # ['csv_local', 'csv_blob', 'azure_blob']\n\n# Create CSV datasource\ncontext = PluginContext(\n    security_level=SecurityLevel.OFFICIAL,\n    run_id=\"exp-001\"\n)\n\ndatasource = datasource_registry.create(\n    \"csv_local\",\n    options={\"path\": \"data/input.csv\"},\n    plugin_context=context\n)\n</code></pre>"},{"location":"api-reference/registries/base/#advanced-custom-plugin-registration","title":"Advanced: Custom Plugin Registration","text":""},{"location":"api-reference/registries/base/#with-security-level-inheritance","title":"With Security Level Inheritance","text":"<pre><code>from elspeth.core.base.plugin import BasePlugin\n\nclass SecurePlugin(BasePlugin):\n    \"\"\"Plugin with security level enforcement.\"\"\"\n\n    def __init__(self, *, security_level: SecurityLevel, config: dict):\n        super().__init__(security_level=security_level)\n        self.config = config\n\ndef create_secure_plugin(opts: dict, ctx: PluginContext) -&gt; SecurePlugin:\n    \"\"\"Factory with context-aware security level.\"\"\"\n    return SecurePlugin(\n        security_level=ctx.security_level,\n        config=opts\n    )\n\n# Register with security awareness\nregistry.register(\n    \"secure_plugin\",\n    factory=BasePluginFactory(\n        create=create_secure_plugin,\n        schema={\n            \"type\": \"object\",\n            \"properties\": {\n                \"api_key\": {\"type\": \"string\"},\n                \"endpoint\": {\"type\": \"string\"}\n            },\n            \"required\": [\"endpoint\"]\n        },\n        plugin_type=\"secure_plugin\"\n    )\n)\n\n# Create with context\nplugin = registry.create(\n    \"secure_plugin\",\n    options={\"endpoint\": \"https://api.example.com\"},\n    plugin_context=PluginContext(\n        security_level=SecurityLevel.PROTECTED,\n        run_id=\"run-001\"\n    )\n)\n\nprint(plugin.get_security_level())  # SecurityLevel.PROTECTED\n</code></pre>"},{"location":"api-reference/registries/base/#with-capabilities","title":"With Capabilities","text":"<pre><code># Register plugin with declared capabilities\nregistry.register(\n    \"advanced_plugin\",\n    factory=BasePluginFactory(\n        create=create_advanced_plugin,\n        schema=advanced_schema,\n        plugin_type=\"advanced\",\n        capabilities=frozenset([\"streaming\", \"caching\", \"retry\"])\n    )\n)\n\n# Check capabilities\nfactory = registry._plugins[\"advanced_plugin\"]\nprint(\"streaming\" in factory.capabilities)  # True\n</code></pre>"},{"location":"api-reference/registries/base/#error-handling","title":"Error Handling","text":""},{"location":"api-reference/registries/base/#configurationerror","title":"ConfigurationError","text":"<p>Raised when plugin configuration is invalid:</p> <pre><code>from elspeth.core.validation.base import ConfigurationError\n\ntry:\n    plugin = registry.create(\n        \"my_plugin\",\n        options={\"invalid\": \"config\"},\n        plugin_context=context\n    )\nexcept ConfigurationError as e:\n    print(f\"Configuration error: {e}\")\n    print(f\"Context: {e.context}\")  # Plugin name and location\n</code></pre>"},{"location":"api-reference/registries/base/#unknown-plugin","title":"Unknown Plugin","text":"<p>Attempting to create unregistered plugin:</p> <pre><code>try:\n    plugin = registry.create(\n        \"nonexistent_plugin\",\n        options={},\n        plugin_context=context\n    )\nexcept KeyError as e:\n    print(f\"Plugin not found: {e}\")\n</code></pre>"},{"location":"api-reference/registries/base/#schema-validation-details","title":"Schema Validation Details","text":""},{"location":"api-reference/registries/base/#json-schema-support","title":"JSON Schema Support","text":"<p>Registries use JSON Schema Draft 7 for validation:</p> <pre><code>schema = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"name\": {\n            \"type\": \"string\",\n            \"minLength\": 1,\n            \"maxLength\": 100\n        },\n        \"age\": {\n            \"type\": \"integer\",\n            \"minimum\": 0,\n            \"maximum\": 120\n        },\n        \"email\": {\n            \"type\": \"string\",\n            \"format\": \"email\"\n        },\n        \"tags\": {\n            \"type\": \"array\",\n            \"items\": {\"type\": \"string\"},\n            \"uniqueItems\": true\n        }\n    },\n    \"required\": [\"name\", \"age\"],\n    \"additionalProperties\": false\n}\n</code></pre>"},{"location":"api-reference/registries/base/#schema-compilation-caching","title":"Schema Compilation Caching","text":"<p>Schemas are compiled once and cached for performance:</p> <pre><code># First call: compile schema\nplugin1 = registry.create(\"my_plugin\", opts1, context)  # Compile + validate\n\n# Subsequent calls: use cached validator\nplugin2 = registry.create(\"my_plugin\", opts2, context)  # Validate only (fast)\n</code></pre>"},{"location":"api-reference/registries/base/#related-documentation","title":"Related Documentation","text":"<ul> <li>BasePlugin - Plugin base class</li> <li>Plugin Catalogue - User-facing plugin documentation</li> <li>Configuration - YAML configuration reference</li> </ul>"},{"location":"api-reference/registries/base/#adr-cross-references","title":"ADR Cross-References","text":"<ul> <li>ADR-004: Mandatory BasePlugin Inheritance - Registry enforces BasePlugin subclassing</li> <li>ADR-002: Multi-Level Security - PluginContext propagates security_level</li> </ul>"},{"location":"architecture/adrs/","title":"Architecture Decision Records (ADRs)","text":"<p>Elspeth's design is guided by documented architecture decisions. This catalog provides an overview of all ADRs.</p> <p>What are ADRs?</p> <p>Architecture Decision Records (ADRs) document significant architectural choices, their context, rationale, and consequences. They provide a historical record of \"why\" decisions were made, helping future maintainers understand the system.</p>"},{"location":"architecture/adrs/#adr-summary","title":"ADR Summary","text":"ID Title Status Category Impact 001 Design Philosophy \u2705 Accepted Foundation \ud83d\udd34 Critical 002 Multi-Level Security Enforcement \u2705 Accepted Security \ud83d\udd34 Critical 002a Trusted Container Model (ClassifiedDataFrame) \u2705 Accepted Security \ud83d\udd34 Critical 002b Immutable Security Policy Metadata \ud83d\udfe1 Proposed Security \ud83d\udd34 Critical 003 Plugin Type Registry \u2705 Accepted Architecture \ud83d\udfe1 High 004 Mandatory BasePlugin Inheritance \u2705 Accepted Security \ud83d\udd34 Critical 005 Frozen Plugin Protection \u2705 Accepted Security \ud83d\udfe1 High 006 Security-Critical Exception Policy \u2705 Accepted Security \ud83d\udfe1 High 007 Universal Dual-Output Protocol \u2705 Accepted Architecture \ud83d\udfe2 Medium 008 Unified Registry Pattern \u2705 Accepted Architecture \ud83d\udfe1 High 009 Configuration Composition \u2705 Accepted Architecture \ud83d\udfe1 High 010 Pass-Through Lifecycle and Routing \u2705 Accepted Architecture \ud83d\udfe2 Medium 011 Error Classification and Recovery \u2705 Accepted Reliability \ud83d\udfe2 Medium 012 Testing Strategy and Quality Gates \u2705 Accepted Quality \ud83d\udfe1 High 013 Global Observability Policy \u2705 Accepted Operations \ud83d\udfe2 Medium 014 Tamper-Evident Reproducibility Bundle \u2705 Accepted Compliance \ud83d\udd34 Critical"},{"location":"architecture/adrs/#core-philosophy-security","title":"Core Philosophy &amp; Security","text":""},{"location":"architecture/adrs/#adr-001-design-philosophy","title":"ADR-001: Design Philosophy","text":"<p>Status: \u2705 Accepted (2025-10-23)</p> <p>Summary: Establishes security-first priority hierarchy for all architectural decisions.</p> <p>Priority Order: 1. Security - Prevent unauthorized access, leakage, or downgrade 2. Data Integrity - Ensure reproducibility and tamper-evident audit trails 3. Availability - Keep system reliable and recoverable 4. Usability/Functionality - Developer ergonomics without compromising higher priorities</p> <p>Key Principle: Fail-closed - Security controls deny operations when unavailable (never fail-open).</p> <p>Impact: Guides all subsequent architectural decisions. When priorities conflict, higher-ranked wins.</p> <p>Full ADR: docs/architecture/decisions/001-design-philosophy.md</p>"},{"location":"architecture/adrs/#adr-002-multi-level-security-enforcement","title":"ADR-002: Multi-Level Security Enforcement","text":"<p>Status: \u2705 Accepted (2025-10-23)</p> <p>Summary: Implements Bell-LaPadula Multi-Level Security (MLS) with fail-fast validation.</p> <p>Key Concepts: - Security Levels: UNOFFICIAL \u2192 OFFICIAL \u2192 OFFICIAL_SENSITIVE \u2192 PROTECTED \u2192 SECRET - Operating Level: MIN of all component security levels (weakest link) - \"No Read Up\": Components can only access data at or below their clearance - Trusted Downgrade: High-clearance components can operate at lower levels (filtering data)</p> <p>Validation Rules: - \u2705 SECRET datasource + OFFICIAL sink \u2192 Pipeline operates at OFFICIAL (datasource filters) - \u274c UNOFFICIAL datasource + SECRET sink \u2192 Pipeline aborts (datasource has insufficient clearance)</p> <p>Implementation: - <code>SecurityLevel</code> enum (UNOFFICIAL=0 \u2192 SECRET=4) - <code>BasePlugin.validate_can_operate_at_level()</code> enforcement - Pipeline computes operating level before data retrieval</p> <p>Impact: Foundation of Elspeth's security model. All plugins must declare security levels.</p> <p>See Also: Security Model Guide, ADR-002a</p> <p>Full ADR: docs/architecture/decisions/002-security-architecture.md</p>"},{"location":"architecture/adrs/#adr-002a-trusted-container-model","title":"ADR-002a: Trusted Container Model","text":"<p>Status: \u2705 Accepted (2025-10-23)</p> <p>Summary: <code>ClassifiedDataFrame</code> implements immutable classification with constructor protection.</p> <p>Security Properties: - Immutable: Classification cannot be modified after creation (frozen dataclass) - Uplifting Only: Classification can only increase via <code>max()</code> operation - Constructor Protection: Only datasources can create instances (prevents laundering) - Access Validation: Runtime failsafe via <code>validate_access_by()</code></p> <p>Threat Prevention: - T3 (Runtime Bypass): <code>validate_access_by()</code> catches start-time validation bypass - T4 (Classification Mislabeling): Constructor protection prevents laundering attacks</p> <p>API: <pre><code># \u2705 Datasource creation (trusted)\nframe = ClassifiedDataFrame.create_from_datasource(data, SecurityLevel.OFFICIAL)\n\n# \u2705 Plugin uplifting (automatic max)\nuplifted = frame.with_uplifted_classification(plugin.get_security_level())\n\n# \u274c Direct construction (blocked)\nframe = ClassifiedDataFrame(data, SecurityLevel.OFFICIAL)  # SecurityValidationError\n</code></pre></p> <p>Impact: Prevents data laundering and downgrade attacks. All data flows through ClassifiedDataFrame.</p> <p>See Also: ClassifiedDataFrame API, ADR-002</p> <p>Full ADR: docs/architecture/decisions/002-a-trusted-container-model.md</p>"},{"location":"architecture/adrs/#adr-002b-immutable-security-policy-metadata","title":"ADR-002b: Immutable Security Policy Metadata","text":"<p>Status: \ud83d\udfe1 Proposed (2025-10-26)</p> <p>Summary: Security policy metadata (<code>security_level</code>, <code>allow_downgrade</code>) is immutable, author-owned, and cannot be overridden via configuration.</p> <p>Problem Prevented: <pre><code># \u274c REJECTED: Configuration overrides security policy\ndatasource:\n  type: azure_blob_secret\n  security_level: UNOFFICIAL  # \u2190 Override from SECRET to UNOFFICIAL\n  allow_downgrade: true       # \u2190 Enable downgrade for frozen plugin\n</code></pre></p> <p>Policy Field Classification: - Immutable (plugin-author-owned): <code>security_level</code>, <code>allow_downgrade</code>, <code>max_operating_level</code> - Mutable (operator-configurable): <code>path</code>, <code>container</code>, <code>timeout</code>, <code>batch_size</code>, etc.</p> <p>Registry Enforcement: - Configuration exposing forbidden policy fields \u2192 <code>RegistrationError</code> - Security policy hardcoded in plugin implementation, certified with code - Aligns with ADR-005 (frozen plugins) and ADR-014 (reproducibility bundles)</p> <p>Impact: Prevents silent security bypass via configuration. Security policy is code-certified, not user-configurable.</p> <p>See Also: ADR-002, ADR-005</p> <p>Full ADR: docs/architecture/decisions/002-b-security-policy-metadata.md</p>"},{"location":"architecture/adrs/#adr-004-mandatory-baseplugin-inheritance","title":"ADR-004: Mandatory BasePlugin Inheritance","text":"<p>Status: \u2705 Accepted (2025-10-23)</p> <p>Summary: All plugins must explicitly inherit from <code>BasePlugin</code> ABC (nominal typing, not duck typing).</p> <p>Why ABC over Protocol? - Nominal typing: <code>isinstance(plugin, BasePlugin)</code> requires explicit inheritance - Security enforcement: Prevents bypass via duck-typed classes - \"Security Bones\": Concrete <code>@final</code> methods prevent override</p> <p>Security Methods (non-overridable): - <code>get_security_level()</code> - Returns plugin clearance - <code>validate_can_operate_at_level()</code> - Enforces Bell-LaPadula rules</p> <p>Runtime Enforcement: - <code>__init_subclass__</code> hook prevents override attempts - Attempting to override raises <code>TypeError</code> at class definition time</p> <p>Impact: Foundation of ADR-002 security validation. All plugins use consistent security logic.</p> <p>See Also: BasePlugin API, ADR-002</p> <p>Full ADR: docs/architecture/decisions/004-mandatory-baseplugin-inheritance.md</p>"},{"location":"architecture/adrs/#adr-005-frozen-plugin-protection","title":"ADR-005: Frozen Plugin Protection","text":"<p>Status: \u2705 Accepted (2025-10-23)</p> <p>Summary: Plugins can opt-out of trusted downgrade via <code>allow_downgrade=False</code>.</p> <p>Use Case: Dedicated infrastructure that should NEVER serve lower-classified pipelines.</p> <p>Example: <pre><code>class FrozenSecretDataSource(BasePlugin):\n    def __init__(self):\n        super().__init__(\n            security_level=SecurityLevel.SECRET,\n            allow_downgrade=False  # \u2190 Frozen at SECRET only\n        )\n\n# \u2705 Can operate at SECRET (exact match)\nfrozen.validate_can_operate_at_level(SecurityLevel.SECRET)\n\n# \u274c Cannot operate at OFFICIAL (rejects downgrade)\nfrozen.validate_can_operate_at_level(SecurityLevel.OFFICIAL)  # Raises SecurityValidationError\n</code></pre></p> <p>Bell-LaPadula Directionality: - Data classification: Can only INCREASE (UNOFFICIAL \u2192 SECRET) - Plugin operations: Can only DECREASE (SECRET \u2192 UNOFFICIAL), unless frozen</p> <p>Impact: Allows dedicated SECRET infrastructure that refuses to handle lower-classified data.</p> <p>See Also: BasePlugin API, ADR-002</p> <p>Full ADR: docs/architecture/decisions/005-frozen-plugin-capability.md</p>"},{"location":"architecture/adrs/#adr-006-security-critical-exception-policy","title":"ADR-006: Security-Critical Exception Policy","text":"<p>Status: \u2705 Accepted (2025-10-23)</p> <p>Summary: Defines exception hierarchy for security-critical vs. operational errors.</p> <p>Exception Taxonomy:</p> Exception When to Raise Recovery <code>SecurityCriticalError</code> Security control failure (PII detected, insufficient clearance) \u274c Never catch (fail-closed) <code>SecurityValidationError</code> Invalid security configuration \u274c Never catch (fail-fast) <code>ConfigurationError</code> Invalid YAML/schema \u2705 Catch at CLI (show user-friendly message) <code>OperationalError</code> Network timeout, file not found \u2705 Catch and retry or log <p>Key Principle: Security exceptions are never caught - they propagate to top-level and abort execution.</p> <p>Example: <pre><code># \u274c NEVER catch security exceptions\ntry:\n    plugin.validate_can_operate_at_level(level)\nexcept SecurityValidationError:\n    logging.warning(\"Insufficient clearance, continuing anyway\")  # FORBIDDEN\n\n# \u2705 Let security exceptions propagate\nplugin.validate_can_operate_at_level(level)  # Raises SecurityValidationError \u2192 abort\n</code></pre></p> <p>Impact: Ensures fail-closed behavior for all security controls.</p> <p>See Also: ADR-001 (Fail-Closed Principle)</p> <p>Full ADR: docs/architecture/decisions/006-security-critical-exception-policy.md</p>"},{"location":"architecture/adrs/#architecture-design-patterns","title":"Architecture &amp; Design Patterns","text":""},{"location":"architecture/adrs/#adr-003-plugin-type-registry","title":"ADR-003: Plugin Type Registry","text":"<p>Status: \u2705 Accepted (2025-10-23)</p> <p>Summary: Plugin registries use factory pattern with JSON schema validation.</p> <p>Registry Types: - <code>DatasourceRegistry</code> - CSV, Azure Blob - <code>TransformRegistry</code> - LLM clients, middleware - <code>SinkRegistry</code> - CSV, Excel, signed artifacts - <code>ExperimentPluginRegistry</code> - Row, aggregation, validation plugins</p> <p>Benefits: - Schema validation before instantiation - Context-aware plugin creation - Centralized plugin discovery</p> <p>Impact: Foundation of plugin architecture. All plugins register via factories.</p> <p>See Also: Plugin Registry API</p> <p>Full ADR: docs/architecture/decisions/003-plugin-type-registry.md</p>"},{"location":"architecture/adrs/#adr-007-universal-dual-output-protocol","title":"ADR-007: Universal Dual-Output Protocol","text":"<p>Status: \u2705 Accepted (2025-10-23)</p> <p>Summary: Sinks return metadata for downstream consumers via dual-output pattern.</p> <p>Pattern: <pre><code>def write(self, frame: ClassifiedDataFrame, metadata: dict) -&gt; dict:\n    \"\"\"Write data and return metadata for downstream.\"\"\"\n    # Write data\n    path = write_csv(frame.data)\n\n    # Return metadata for downstream sinks\n    return {\"output_path\": path, \"row_count\": len(frame.data)}\n</code></pre></p> <p>Benefits: - Sink chaining (signed bundles consume CSV outputs) - Metadata propagation (costs, retries, paths) - Backward compatible (return value optional)</p> <p>Impact: Enables artifact pipeline dependency resolution.</p> <p>See Also: Artifact Pipeline API</p> <p>Full ADR: docs/architecture/decisions/007-universal-dual-output-protocol.md</p>"},{"location":"architecture/adrs/#adr-008-unified-registry-pattern","title":"ADR-008: Unified Registry Pattern","text":"<p>Status: \u2705 Accepted (2025-10-23)</p> <p>Summary: Consolidates registry implementations under <code>BasePluginRegistry[T]</code>.</p> <p>Before: 5 duplicate registry implementations After: Single <code>BasePluginRegistry[T]</code> with generic type parameter</p> <p>Benefits: - Code reuse (1 implementation, not 5) - Consistency (same validation logic everywhere) - Type safety (generic <code>T</code> for plugin type)</p> <p>Impact: Simplified plugin architecture with consistent validation.</p> <p>See Also: Plugin Registry API</p> <p>Full ADR: docs/architecture/decisions/008-unified-registry-pattern.md</p>"},{"location":"architecture/adrs/#adr-009-configuration-composition","title":"ADR-009: Configuration Composition","text":"<p>Status: \u2705 Accepted (2025-10-23)</p> <p>Summary: Configuration merges in predictable order: defaults \u2192 prompt packs \u2192 experiment overrides.</p> <p>Merge Order: <pre><code>1. Suite defaults (settings.yaml)\n        \u2193\n2. Prompt packs (optional)\n        \u2193\n3. Experiment overrides (experiments/*.yaml)\n</code></pre></p> <p>Merge Rules: - Simple values: Later overwrites earlier - Lists (middleware, sinks): Later appends to earlier (unless <code>inherit: false</code>) - Nested objects: Deep merge (only specified keys overwrite)</p> <p>Benefits: - Define security middleware once (suite defaults) - Share prompts across experiments (prompt packs) - Override per-experiment as needed</p> <p>Impact: Foundation of configuration system. Enables DRY configuration.</p> <p>See Also: Configuration Guide</p> <p>Full ADR: docs/architecture/decisions/009-configuration-composition.md</p>"},{"location":"architecture/adrs/#adr-010-pass-through-lifecycle-and-routing","title":"ADR-010: Pass-Through Lifecycle and Routing","text":"<p>Status: \u2705 Accepted (2025-10-23)</p> <p>Summary: Middleware lifecycle events (suite loaded, retry exhausted) propagate to all middleware instances.</p> <p>Lifecycle Hooks: - <code>on_suite_loaded()</code> - Suite starts (share state across experiments) - <code>on_retry_exhausted()</code> - Row retry failed (publish failure telemetry)</p> <p>Benefits: - Middleware can track suite-level state - Telemetry captures retry context - Azure ML run integration</p> <p>Impact: Enables suite-aware middleware (health monitoring, cost aggregation).</p> <p>Full ADR: docs/architecture/decisions/010-pass-through-lifecycle-and-routing.md</p>"},{"location":"architecture/adrs/#adr-011-error-classification-and-recovery","title":"ADR-011: Error Classification and Recovery","text":"<p>Status: \u2705 Accepted (2025-10-23)</p> <p>Summary: Defines error taxonomy and recovery strategies for operational errors.</p> <p>Error Categories: - Transient (network timeout) \u2192 Retry with exponential backoff - Invalid Input (malformed data) \u2192 Skip row or abort - Resource Exhaustion (rate limit) \u2192 Wait and retry - Security (insufficient clearance) \u2192 Abort immediately (fail-closed)</p> <p>Recovery Strategies: - <code>on_error: abort</code> - Stop pipeline immediately - <code>on_error: skip</code> - Log error and continue - <code>on_error: log</code> - Log warning and continue</p> <p>Impact: Enables resilient pipelines with configurable error handling.</p> <p>See Also: ADR-006</p> <p>Full ADR: docs/architecture/decisions/011-error-classification-and-recovery.md</p>"},{"location":"architecture/adrs/#quality-operations","title":"Quality &amp; Operations","text":""},{"location":"architecture/adrs/#adr-012-testing-strategy-and-quality-gates","title":"ADR-012: Testing Strategy and Quality Gates","text":"<p>Status: \u2705 Accepted (2025-10-23)</p> <p>Summary: Defines testing requirements and quality gates for CI/CD.</p> <p>Test Categories: - Unit Tests: \u226580% coverage on core modules - Integration Tests: End-to-end experiment runs - Security Tests: Validation bypass attempts, classification laundering - Performance Tests: Baseline latency and throughput</p> <p>Quality Gates: - \u2705 All tests passing (100%) - \u2705 MyPy clean (type checking) - \u2705 Ruff clean (linting) - \u2705 Coverage \u226580% on security-critical paths - \u2705 No known vulnerabilities (audit)</p> <p>Impact: Ensures code quality and security before deployment.</p> <p>See Also: Testing documentation in developer docs</p> <p>Full ADR: docs/architecture/decisions/012-testing-strategy-and-quality-gates.md</p>"},{"location":"architecture/adrs/#adr-013-global-observability-policy","title":"ADR-013: Global Observability Policy","text":"<p>Status: \u2705 Accepted (2025-10-23)</p> <p>Summary: Defines logging, telemetry, and audit trail requirements.</p> <p>Logging Requirements: - Structured Logging: JSON format (JSONL) - Audit Trail: All security decisions logged - Correlation IDs: Track requests across components - Security Classification: Log metadata includes security level</p> <p>Log Levels: - ERROR: Security violations, unrecoverable errors - WARNING: Operational issues, retry attempts - INFO: Normal execution (datasource loaded, sinks written) - DEBUG: Detailed execution trace</p> <p>Audit Events: - Security validation decisions - Classification uplifts - Retry attempts and exhaustion - Cost and token usage</p> <p>Impact: Enables compliance audits and troubleshooting.</p> <p>See Also: Audit logging documentation in operations docs</p> <p>Full ADR: docs/architecture/decisions/013-global-observability-policy.md</p>"},{"location":"architecture/adrs/#adr-014-tamper-evident-reproducibility-bundle","title":"ADR-014: Tamper-Evident Reproducibility Bundle","text":"<p>Status: \u2705 Accepted (2025-10-26)</p> <p>Summary: Elspeth emits a single, cryptographically signed, tamper-evident reproducibility bundle for every experiment suite execution.</p> <p>Compliance Requirements: - Government PSPF, HIPAA, PCI-DSS, defence export control - Auditors must verify: what data/config/prompts/code produced results - Detect post-run modifications to artifacts</p> <p>Core Requirements:</p> <ol> <li>Mandatory Reproducibility Sink:</li> <li><code>ReproducibilityBundleSink</code> enabled by default in production templates</li> <li> <p>Explicit opt-out required with formal risk acceptance</p> </li> <li> <p>Comprehensive Contents:</p> </li> <li>Experiment results (JSON + sanitized CSV)</li> <li>Source data snapshot + datasource config</li> <li>Full merged configuration + rendered prompts</li> <li>Plugin source code used during run</li> <li>Optional framework source code</li> <li>All artifacts from other sinks (logs, analytics)</li> <li> <p>Sanitization metadata</p> </li> <li> <p>Cryptographic Integrity:</p> </li> <li>Every file hashed (SHA-256) and recorded in <code>MANIFEST.json</code></li> <li>Manifest signed using configured algorithm: <code>hmac-sha256</code>, <code>hmac-sha512</code>, <code>rsa-pss-sha256</code>, <code>ecdsa-p256-sha256</code></li> <li>Signature stored in <code>SIGNATURE.json</code></li> <li> <p>Final archive: <code>.tar</code> or <code>.tar.gz</code> with signed manifest</p> </li> <li> <p>Immutable Policy Metadata (ADR-002-B alignment):</p> </li> <li>Sink has hard-coded <code>security_level=SecurityLevel.UNOFFICIAL</code> and <code>allow_downgrade=True</code></li> <li>Operators cannot lower signing requirements via configuration</li> </ol> <p>Verification: <pre><code>python -m elspeth.cli verify-bundle \\\n  --bundle-path outputs/bundle_2025-10-26_experiment.tar.gz \\\n  --public-key /path/to/signing.pub\n</code></pre></p> <p>Impact: Every run is independently auditable with tamper detection. Meets compliance requirements for reproducibility.</p> <p>See Also: ADR-002-B, Artifact signing documentation</p> <p>Full ADR: docs/architecture/decisions/014-reproducibility-bundle.md</p>"},{"location":"architecture/adrs/#reading-guide","title":"Reading Guide","text":""},{"location":"architecture/adrs/#by-role","title":"By Role","text":"<p>Security Architect: - ADR-001 - Fail-closed principle - ADR-002 - Bell-LaPadula MLS - ADR-002a - Immutable classification - ADR-002b - Immutable security policy - ADR-004 - Security bones - ADR-005 - Dedicated infrastructure - ADR-006 - Security exceptions - ADR-014 - Tamper-evident bundles</p> <p>Plugin Developer: - ADR-003 - Registry pattern - ADR-004 - BasePlugin requirements - ADR-007 - Sink metadata return - ADR-008 - Registry API</p> <p>Operations/SRE: - ADR-001 - Priority hierarchy - ADR-011 - Error recovery strategies - ADR-013 - Logging and telemetry</p> <p>Configuration Manager: - ADR-009 - Merge order - ADR-010 - Middleware lifecycle</p>"},{"location":"architecture/adrs/#by-topic","title":"By Topic","text":"<p>Security: - ADR-001, ADR-002, ADR-002a, ADR-002b, ADR-004, ADR-005, ADR-006</p> <p>Architecture: - ADR-003, ADR-007, ADR-008, ADR-009, ADR-010</p> <p>Reliability: - ADR-011, ADR-012</p> <p>Operations: - ADR-013</p> <p>Compliance: - ADR-014</p>"},{"location":"architecture/adrs/#adr-process","title":"ADR Process","text":""},{"location":"architecture/adrs/#when-to-write-an-adr","title":"When to Write an ADR","text":"<p>Create an ADR when making decisions about: - \u2705 Security model changes - \u2705 Plugin architecture changes - \u2705 Configuration structure changes - \u2705 Error handling strategy changes - \u2705 Testing or quality gate changes - \u2705 Breaking changes to public APIs</p>"},{"location":"architecture/adrs/#adr-template","title":"ADR Template","text":"<p>Use the template at docs/architecture/decisions/000-template.md.</p>"},{"location":"architecture/adrs/#related-documentation","title":"Related Documentation","text":"<ul> <li>Architecture Overview - System architecture guide</li> <li>Security Model - Bell-LaPadula MLS user guide</li> <li>API Reference - Plugin development APIs</li> </ul> <p>Understanding Elspeth</p> <p>ADRs provide the \"why\" behind Elspeth's design. Read them to understand:</p> <ul> <li>Why security-first priority hierarchy? \u2192 ADR-001</li> <li>Why Bell-LaPadula MLS? \u2192 ADR-002</li> <li>Why immutable classification? \u2192 ADR-002a</li> <li>Why ABC not Protocol? \u2192 ADR-004</li> <li>Why fail-closed exceptions? \u2192 ADR-006</li> </ul> <p>Start with ADR-001 and ADR-002 for foundational understanding.</p>"},{"location":"architecture/execution-flow/","title":"Execution Flow","text":"<p>Complete trace of Elspeth's execution from CLI invocation to artifact generation.</p> <p>Purpose</p> <p>This document provides both logical flow (high-level stages) and detailed trace (function-by-function execution) to help developers understand the system and debug issues.</p>"},{"location":"architecture/execution-flow/#quick-navigation","title":"Quick Navigation","text":"<ul> <li>Logical Flow Overview - High-level stages with decision points</li> <li>Detailed Trace - Function-by-function execution with code references</li> <li>Security Checkpoints - Where/when/what security validation happens</li> <li>Debugging Guide - What to check at each stage</li> </ul>"},{"location":"architecture/execution-flow/#logical-flow-overview","title":"Logical Flow Overview","text":"<p>Elspeth execution follows a seven-stage pipeline with validation at each boundary:</p> <pre><code>graph TD\n    START([python -m elspeth.cli]) --&gt; STAGE1[1. CLI Entry &amp; Argument Parsing]\n    STAGE1 --&gt; STAGE2[2. Configuration Loading &amp; Validation]\n    STAGE2 --&gt; STAGE3[3. Suite/Workflow Initialization]\n    STAGE3 --&gt; STAGE4[4. Security Level Computation]\n    STAGE4 --&gt; STAGE5[5. Workflow Execution Row by Row]\n    STAGE5 --&gt; STAGE6[6. Aggregation &amp; Post-Processing]\n    STAGE6 --&gt; STAGE7[7. Artifact Pipeline &amp; Sink Execution]\n    STAGE7 --&gt; END([Artifacts Written, Exit])\n\n    STAGE2 -.-&gt;|Validation Fails| ERROR1[\u274c Exit: Config Invalid]\n    STAGE4 -.-&gt;|Security Violation| ERROR2[\u274c Exit: Insufficient Clearance]\n    STAGE5 -.-&gt;|Retry Exhausted| ERROR3[\u274c Exit: Processing Failed]\n    STAGE7 -.-&gt;|Sink Fails| ERROR4[\u274c Exit: Artifact Write Failed]\n\n    style START fill:#E6F3FF\n    style END fill:#90EE90\n    style ERROR1 fill:#FFB6C1\n    style ERROR2 fill:#FFB6C1\n    style ERROR3 fill:#FFB6C1\n    style ERROR4 fill:#FFB6C1</code></pre>"},{"location":"architecture/execution-flow/#stage-breakdown","title":"Stage Breakdown","text":"Stage Responsible Component Key Actions Exit on Failure? 1. CLI Entry <code>src/elspeth/core/cli/suite.py</code> Parse arguments, setup logging \u2705 Yes (invalid args) 2. Config Loading <code>src/elspeth/core/cli/config_utils.py</code> Load YAML, merge configs, validate schemas \u2705 Yes (invalid config) 3. Suite Init <code>src/elspeth/core/experiments/suite_runner.py</code> Instantiate plugins, bind components \u2705 Yes (plugin errors) 4. Security Computation <code>ExperimentOrchestrator</code> Compute operating level (min of all), validate clearances \u2705 Yes (security violation) 5. Workflow Execution <code>src/elspeth/core/experiments/runner.py</code> Process rows with transforms + middleware \u26a0\ufe0f Configurable (retry/skip) 6. Aggregation Aggregator plugins Compute statistics, cost summaries \ud83d\udd39 Optional (may skip) 7. Artifact Pipeline <code>src/elspeth/core/pipeline/artifact_pipeline.py</code> Write sinks in dependency order \u2705 Yes (sink failures)"},{"location":"architecture/execution-flow/#detailed-trace","title":"Detailed Trace","text":""},{"location":"architecture/execution-flow/#stage-1-cli-entry-argument-parsing","title":"Stage 1: CLI Entry &amp; Argument Parsing","text":"<p>File: <code>src/elspeth/core/cli/suite.py</code></p> <p>Entry Point: <code>python -m elspeth.cli --settings &lt;path&gt; --suite-root &lt;path&gt;</code></p> <pre><code># Execution path:\n1. __main__.py \u2192 dispatch to suite.main()\n2. suite.main()\n   \u251c\u2500 Parse CLI arguments (argparse)\n   \u251c\u2500 Setup logging configuration\n   \u251c\u2500 Validate required arguments (settings, suite-root)\n   \u2514\u2500 Call run_suite()\n</code></pre> <p>Arguments Parsed: - <code>--settings</code> (required) \u2192 Path to settings.yaml - <code>--suite-root</code> (required) \u2192 Directory containing experiments/ - <code>--reports-dir</code> (optional) \u2192 Output directory for reports - <code>--head</code> (optional) \u2192 Preview N rows before execution - <code>--live-outputs</code> (optional) \u2192 Enable real-time output display</p> <p>Debug Checkpoint: If CLI fails, check: - \u2705 File paths exist (<code>settings.yaml</code>, suite root directory) - \u2705 Correct argument names (check <code>--help</code> output) - \u2705 Logging configured (should see log initialization messages)</p>"},{"location":"architecture/execution-flow/#stage-2-configuration-loading-validation","title":"Stage 2: Configuration Loading &amp; Validation","text":"<p>File: <code>src/elspeth/core/cli/config_utils.py</code></p> <pre><code># Execution sequence:\n1. load_suite_config(settings_path, suite_root)\n   \u251c\u2500 Load settings.yaml (YAML parse)\n   \u251c\u2500 Discover experiments in suite_root/experiments/\n   \u251c\u2500 For each experiment:\n   \u2502  \u251c\u2500 Load experiment YAML\n   \u2502  \u251c\u2500 Merge: suite defaults \u2192 prompt packs \u2192 experiment overrides\n   \u2502  \u2514\u2500 Validate merged config against JSON schemas\n   \u2514\u2500 Return: SuiteConfig object\n\n2. validate_schemas(config)\n   \u251c\u2500 Validate datasource schema\n   \u251c\u2500 Validate transform schema (LLM/custom)\n   \u251c\u2500 Validate sink schemas (each sink)\n   \u2514\u2500 Validate middleware schemas\n</code></pre> <p>Merge Order (per ADR-009): <pre><code>1. Suite defaults (settings.yaml \u2192 defaults:)\n        \u2193 (deep merge)\n2. Prompt packs (if prompt_pack: specified)\n        \u2193 (deep merge)\n3. Experiment overrides (experiments/&lt;name&gt;.yaml)\n        \u2193 (final result)\nMerged Configuration\n</code></pre></p> <p>Environment Variable Resolution: <pre><code># Before resolution:\nllm:\n  api_key: ${AZURE_OPENAI_KEY}\n\n# After resolution:\nllm:\n  api_key: \"sk-actual-key-value\"  # From environment\n</code></pre></p> <p>Debug Checkpoint: If config loading fails: - \u2705 Valid YAML syntax (<code>yamllint settings.yaml</code>) - \u2705 All required fields present (datasource, transform, sinks) - \u2705 Environment variables set (check <code>${VAR}</code> references) - \u2705 Schema validation errors \u2192 Check plugin-specific required fields</p>"},{"location":"architecture/execution-flow/#stage-3-suiteworkflow-initialization","title":"Stage 3: Suite/Workflow Initialization","text":"<p>File: <code>src/elspeth/core/experiments/suite_runner.py</code></p> <pre><code># Execution sequence:\n1. SuiteRunner.__init__(suite_config)\n   \u251c\u2500 Store suite configuration\n   \u251c\u2500 Initialize shared middleware instances\n   \u2514\u2500 Create experiment orchestrators (one per experiment)\n\n2. For each experiment in suite:\n   ExperimentOrchestrator.__init__(experiment_config)\n   \u251c\u2500 Instantiate datasource plugin\n   \u2502  \u2514\u2500 DatasourceRegistry.create_from_config(config.datasource)\n   \u251c\u2500 Instantiate transform plugin (LLM/custom)\n   \u2502  \u2514\u2500 TransformRegistry.create_from_config(config.llm)\n   \u251c\u2500 Instantiate sink plugins (list)\n   \u2502  \u2514\u2500 SinkRegistry.create_from_config(sink_config) for each\n   \u251c\u2500 Instantiate middleware plugins\n   \u2502  \u2514\u2500 MiddlewareRegistry.create_from_config(mw_config) for each\n   \u2514\u2500 Store component references\n</code></pre> <p>Plugin Instantiation Pattern: <pre><code># All plugins follow this pattern:\n1. Registry.create_from_config(config_dict)\n2. Registry validates config against plugin schema\n3. Registry calls PluginClass(**config)\n4. Plugin.__init__() validates security_level\n5. Plugin stores configuration\n6. Return initialized plugin instance\n</code></pre></p> <p>Debug Checkpoint: If suite initialization fails: - \u2705 Plugin type exists in registry (check available plugins) - \u2705 Plugin configuration matches schema (required fields) - \u2705 Security level is valid enum value (UNOFFICIAL, OFFICIAL, etc.) - \u2705 Plugin dependencies available (e.g., azure-storage-blob for azure_blob)</p>"},{"location":"architecture/execution-flow/#stage-4-security-level-computation-validation","title":"Stage 4: Security Level Computation &amp; Validation","text":"<p>Execution sequence:</p> <pre><code>1. orchestrator.compute_operating_level()\n   \u251c\u2500 Collect security levels:\n   \u2502  \u251c\u2500 datasource_level = datasource.get_security_level()\n   \u2502  \u251c\u2500 transform_level = transform.get_security_level()\n   \u2502  \u2514\u2500 sink_levels = [sink.get_security_level() for sink in sinks]\n   \u2502\n   \u251c\u2500 Compute minimum:\n   \u2502  operating_level = min(datasource_level, transform_level, *sink_levels)\n   \u2502\n   \u2514\u2500 Return operating_level (e.g., SecurityLevel.OFFICIAL)\n\n2. orchestrator.validate_security()\n   For each component:\n   \u251c\u2500 component.validate_can_operate_at_level(operating_level)\n   \u2502  \u251c\u2500 If component.security_level &lt; operating_level:\n   \u2502  \u2502  \u2514\u2500 Raise SecurityValidationError(\"Insufficient clearance\")\n   \u2502  \u2514\u2500 Else: Pass (component can downgrade or exact match)\n   \u2514\u2500 Continue to next component\n</code></pre> <p>Security Validation Logic (per ADR-002): <pre><code>Component Clearance: SECRET\nOperating Level: OFFICIAL\nValidation: SECRET \u2265 OFFICIAL \u2192 \u2705 PASS (trusted downgrade)\n\nComponent Clearance: UNOFFICIAL\nOperating Level: OFFICIAL\nValidation: UNOFFICIAL &lt; OFFICIAL \u2192 \u274c FAIL (insufficient clearance)\n</code></pre></p> <p>Mermaid Flow: <pre><code>graph TD\n    START[Collect Security Levels] --&gt; COMPUTE[Compute min of all components]\n    COMPUTE --&gt; VALIDATE{For each component}\n    VALIDATE --&gt;|Clearance \u2265 Operating Level| PASS[\u2705 Component Valid]\n    VALIDATE --&gt;|Clearance &lt; Operating Level| FAIL[\u274c SecurityValidationError]\n    PASS --&gt; NEXT{More components?}\n    NEXT --&gt;|Yes| VALIDATE\n    NEXT --&gt;|No| SUCCESS[\u2705 All Validated]\n    FAIL --&gt; ERROR[Exit: Security Violation]\n\n    style SUCCESS fill:#90EE90\n    style ERROR fill:#FFB6C1</code></pre></p> <p>Debug Checkpoint: If security validation fails: - \u2705 Check <code>security_level</code> on failing component (should be \u2265 operating level) - \u2705 Verify operating level computation (may be lower than expected due to min()) - \u2705 Review Bell-LaPadula rules: Security Model - \u2705 Common fix: Increase component's <code>security_level</code> to match operating level</p>"},{"location":"architecture/execution-flow/#stage-5-workflow-execution-row-by-row-processing","title":"Stage 5: Workflow Execution (Row-by-Row Processing)","text":"<p>File: <code>src/elspeth/core/experiments/runner.py</code></p> <p>This is the core execution loop where actual data processing happens.</p> <pre><code># High-level execution:\n1. runner.run(datasource, transform, sinks, middleware)\n   \u251c\u2500 datasource.load_data() \u2192 ClassifiedDataFrame\n   \u251c\u2500 For each row in frame:\n   \u2502  \u251c\u2500 Apply middleware.before_request(row)\n   \u2502  \u251c\u2500 transform.transform(row) \u2192 result\n   \u2502  \u251c\u2500 Apply middleware.after_response(row, result)\n   \u2502  \u251c\u2500 Validate result (JSON schema, regex, etc.)\n   \u2502  \u2514\u2500 Store result or retry on error\n   \u2514\u2500 Return: results DataFrame\n</code></pre> <p>Detailed Row Processing:</p> <pre><code># For each row (sequential or concurrent):\n1. Extract row data \u2192 dict\n\n2. Middleware.before_request(row_data)\n   \u251c\u2500 pii_shield: Check for PII patterns\n   \u251c\u2500 classified_material: Check for classified markings\n   \u251c\u2500 prompt_shield: Check for banned terms\n   \u2514\u2500 If any middleware blocks \u2192 skip row or abort\n\n3. Transform.transform(row_data)\n   \u251c\u2500 Render prompt template (Jinja2 strict)\n   \u251c\u2500 Call LLM API / custom transform logic\n   \u251c\u2500 Parse response\n   \u2514\u2500 Return transformed data\n\n4. Middleware.after_response(row_data, response)\n   \u251c\u2500 audit_logger: Log request/response\n   \u251c\u2500 health_monitor: Track latency, success rate\n   \u251c\u2500 azure_content_safety: Check response safety\n   \u2514\u2500 If any middleware blocks \u2192 retry or skip\n\n5. Validate response\n   \u251c\u2500 JSON schema validation (if specified)\n   \u251c\u2500 Regex pattern validation (if specified)\n   \u2514\u2500 If validation fails \u2192 retry or skip\n\n6. Handle result:\n   \u251c\u2500 Success \u2192 Store result, increment row counter\n   \u251c\u2500 Transient error \u2192 Retry with exponential backoff\n   \u2514\u2500 Permanent error \u2192 Log failure, continue or abort\n</code></pre> <p>Retry Logic:</p> <pre><code># Retry with exponential backoff:\nmax_retries = 3\nbase_delay = 1.0  # seconds\n\nfor attempt in range(1, max_retries + 1):\n    try:\n        result = transform.transform(row)\n        return result  # Success\n    except TransientError as e:\n        if attempt &lt; max_retries:\n            delay = base_delay * (2 ** (attempt - 1))  # 1s, 2s, 4s\n            sleep(delay)\n            continue\n        else:\n            # Retry exhausted\n            handle_exhaustion(row, e)\n</code></pre> <p>Concurrency Model (if enabled):</p> <pre><code># Concurrent execution with ThreadPoolExecutor:\nif concurrency.enabled and not rate_limiter.saturated():\n    with ThreadPoolExecutor(max_workers=concurrency.max_workers) as executor:\n        futures = [executor.submit(process_row, row) for row in rows]\n        results = [future.result() for future in futures]\nelse:\n    # Sequential fallback\n    results = [process_row(row) for row in rows]\n</code></pre> <p>Debug Checkpoint: If row processing fails: - \u2705 Check middleware logs (which middleware blocked?) - \u2705 Inspect transform errors (API failures, timeout, rate limit?) - \u2705 Review retry count (exhausted all retries?) - \u2705 Validation errors (response doesn't match expected schema?) - \u2705 Check <code>logs/run_*.jsonl</code> for audit trail</p>"},{"location":"architecture/execution-flow/#stage-6-aggregation-post-processing","title":"Stage 6: Aggregation &amp; Post-Processing","text":"<p>Execution sequence:</p> <pre><code>1. runner.run_aggregation_plugins(results_df, metadata)\n   For each aggregation plugin:\n   \u251c\u2500 plugin.aggregate(results_df, metadata)\n   \u251c\u2500 Compute statistics (mean, std, percentiles)\n   \u251c\u2500 Generate cost summaries (token usage, API cost)\n   \u251c\u2500 Baseline comparison (if baseline specified)\n   \u2514\u2500 Store aggregation results in metadata\n\n2. Examples of aggregators:\n   \u251c\u2500 score_stats: Calculate mean/median/std of numeric columns\n   \u251c\u2500 cost_summary: Sum prompt_tokens, completion_tokens, total_cost\n   \u251c\u2500 score_significance: Compare to baseline with t-test\n   \u2514\u2500 rationale_analysis: Extract common patterns from text\n</code></pre> <p>Baseline Comparison Flow:</p> <pre><code># If baseline specified:\n1. Load baseline results from previous experiment\n2. For each comparison criterion (e.g., accuracy):\n   \u251c\u2500 Extract current values: current_scores\n   \u251c\u2500 Extract baseline values: baseline_scores\n   \u251c\u2500 Compute delta: mean(current) - mean(baseline)\n   \u251c\u2500 Statistical test: t_test(current, baseline)\n   \u251c\u2500 Effect size: cohens_d(current, baseline)\n   \u2514\u2500 Store in metadata: {\"delta\": 0.12, \"p_value\": 0.03, \"significant\": true}\n</code></pre> <p>Debug Checkpoint: If aggregation fails: - \u2705 Check aggregator plugin configuration - \u2705 Baseline experiment exists (if baseline comparison requested) - \u2705 Required columns present in results (e.g., \"accuracy\" for score_significance) - \u2705 Data types match expected (numeric for statistical aggregators)</p>"},{"location":"architecture/execution-flow/#stage-7-artifact-pipeline-sink-execution","title":"Stage 7: Artifact Pipeline &amp; Sink Execution","text":"<p>File: <code>src/elspeth/core/pipeline/artifact_pipeline.py</code></p> <p>Final stage: write all outputs in dependency order.</p> <pre><code>1. ArtifactPipeline.execute(results_df, metadata)\n   \u251c\u2500 Topological sort sinks by dependencies\n   \u2502  \u251c\u2500 Independent sinks first (no consumes=[])\n   \u2502  \u2514\u2500 Dependent sinks after their dependencies\n   \u2502\n   \u251c\u2500 For each sink (in dependency order):\n   \u2502  \u251c\u2500 Validate sink can operate at operating_level\n   \u2502  \u251c\u2500 sink.write(results_df, metadata)\n   \u2502  \u251c\u2500 Capture sink output metadata\n   \u2502  \u2514\u2500 Merge into global metadata\n   \u2502\n   \u2514\u2500 Return aggregated metadata\n</code></pre> <p>Dependency Resolution Example:</p> <pre><code># Configuration:\nsinks:\n  - name: csv_output\n    type: csv\n    path: results.csv\n\n  - name: excel_report\n    type: excel_workbook\n    base_path: reports/\n\n  - name: signed_bundle\n    type: signed_artifact\n    consumes: [csv_output]  # Depends on CSV\n\n# Execution order:\n1. csv_output, excel_report (parallel - no dependencies)\n2. signed_bundle (after csv_output completes)\n</code></pre> <p>Sink Execution Detail:</p> <pre><code># For each sink:\n1. sink.validate_can_operate_at_level(operating_level)\n   \u2514\u2500 If sink.security_level &lt; operating_level:\n      Raise SecurityValidationError\n\n2. sink.write(results_df, metadata)\n   \u251c\u2500 Sanitize output (formula injection prevention)\n   \u251c\u2500 Write to destination (file, blob, API)\n   \u251c\u2500 Generate metadata (file paths, checksums, timestamps)\n   \u2514\u2500 Return sink-specific metadata\n\n3. Merge sink metadata:\n   metadata[sink_name] = sink.write(...) return value\n</code></pre> <p>Metadata Chaining:</p> <pre><code># Metadata flows between sinks:\nmetadata = {}\n\n# First: csv_output writes\nmetadata[\"csv_output\"] = {\"path\": \"results.csv\", \"rows\": 100}\n\n# Later: signed_bundle accesses CSV path\ncsv_path = metadata[\"csv_output\"][\"path\"]\ncreate_signed_bundle(csv_path, ...)\n</code></pre> <p>Debug Checkpoint: If sink execution fails: - \u2705 Check sink security level (\u2265 operating level) - \u2705 Destination writable (file permissions, network access) - \u2705 Dependency available (if consumes=[] specified) - \u2705 Sink-specific errors (blob auth, API keys, disk space)</p>"},{"location":"architecture/execution-flow/#security-checkpoints","title":"Security Checkpoints","text":"<p>Security validation happens at five critical points:</p> Checkpoint Location What's Validated Failure Mode 1. Plugin Init Plugin <code>__init__()</code> Security level is valid enum Raises <code>ValueError</code> 2. Operating Level Computation Orchestrator Min of all components computed correctly N/A (computation always succeeds) 3. Component Clearance Orchestrator Each component \u2265 operating level Raises <code>SecurityValidationError</code> 4. Data Classification Transform execution Data classification \u2264 component clearance Raises <code>SecurityValidationError</code> 5. Sink Write Each sink Sink \u2265 data classification Raises <code>SecurityValidationError</code> <p>Security Enforcement Sequence:</p> <pre><code>sequenceDiagram\n    participant Config\n    participant Orchestrator\n    participant Plugin\n    participant Runner\n    participant Sink\n\n    Config-&gt;&gt;Plugin: Instantiate with security_level\n    Plugin-&gt;&gt;Plugin: Validate security_level enum \u2705\n\n    Orchestrator-&gt;&gt;Orchestrator: Compute operating_level = min(all)\n    Orchestrator-&gt;&gt;Plugin: validate_can_operate_at_level(operating_level)\n    Plugin--&gt;&gt;Orchestrator: \u2705 or \u274c SecurityValidationError\n\n    Runner-&gt;&gt;Plugin: transform(row)\n    Plugin-&gt;&gt;Plugin: Check data.classification \u2264 self.security_level \u2705\n    Plugin--&gt;&gt;Runner: result\n\n    Runner-&gt;&gt;Sink: write(data, metadata)\n    Sink-&gt;&gt;Sink: Check data.classification \u2264 self.security_level \u2705\n    Sink--&gt;&gt;Runner: metadata</code></pre>"},{"location":"architecture/execution-flow/#debugging-guide","title":"Debugging Guide","text":""},{"location":"architecture/execution-flow/#common-issues-diagnosis","title":"Common Issues &amp; Diagnosis","text":""},{"location":"architecture/execution-flow/#issue-securityvalidationerror-insufficient-clearance","title":"Issue: \"SecurityValidationError: Insufficient clearance\"","text":"<p>Where: Stage 4 (Security Validation)</p> <p>Diagnosis: <pre><code># Find the failing component:\n1. Check error message for component name\n2. Check component's security_level\n3. Check computed operating_level\n4. Compare: component.security_level \u2265 operating_level?\n</code></pre></p> <p>Solution: Increase component's <code>security_level</code> to at least <code>operating_level</code></p>"},{"location":"architecture/execution-flow/#issue-config-validation-failed-missing-required-field","title":"Issue: \"Config validation failed: Missing required field\"","text":"<p>Where: Stage 2 (Config Loading)</p> <p>Diagnosis: <pre><code># Check which plugin/field failed:\n1. Error message shows: \"datasource.path\" or \"llm.api_key\"\n2. Check that field exists in YAML config\n3. Check field name spelling (case-sensitive)\n</code></pre></p> <p>Solution: Add missing field to configuration YAML</p>"},{"location":"architecture/execution-flow/#issue-retry-exhausted-transform-keeps-failing","title":"Issue: \"Retry exhausted\" / Transform keeps failing","text":"<p>Where: Stage 5 (Row Processing)</p> <p>Diagnosis: <pre><code># Check audit logs:\ntail -f logs/run_*.jsonl | grep ERROR\n\n# Look for:\n1. API errors (rate limit, auth failure, timeout)\n2. Middleware blocks (PII detected, content safety)\n3. Validation failures (response doesn't match schema)\n</code></pre></p> <p>Solution: - API errors \u2192 Check credentials, rate limits, network - Middleware blocks \u2192 Adjust middleware thresholds or disable - Validation \u2192 Fix response schema or transform output</p>"},{"location":"architecture/execution-flow/#issue-sink-fails-to-write","title":"Issue: Sink fails to write","text":"<p>Where: Stage 7 (Artifact Pipeline)</p> <p>Diagnosis: <pre><code># Check:\n1. File permissions (can write to output directory?)\n2. Disk space (df -h)\n3. Network access (if cloud sink like azure_blob)\n4. Credentials (API keys, connection strings)\n</code></pre></p> <p>Solution: Fix infrastructure issue (permissions, network, auth)</p>"},{"location":"architecture/execution-flow/#issue-slow-execution-hangs","title":"Issue: Slow execution / Hangs","text":"<p>Where: Stage 5 (Row Processing)</p> <p>Diagnosis: <pre><code># Check:\n1. Concurrency disabled? (sequential processing is slow)\n2. Rate limiter saturated? (waiting for quota)\n3. Network latency? (slow API responses)\n4. Large dataset? (how many rows?)\n</code></pre></p> <p>Solution: - Enable concurrency (if not enabled) - Increase rate limit quota - Use checkpoint recovery for large datasets - Monitor with <code>--live-outputs</code> flag</p>"},{"location":"architecture/execution-flow/#related-documentation","title":"Related Documentation","text":"<ul> <li>Architecture Overview - System components</li> <li>Security Model - Bell-LaPadula MLS details</li> <li>Configuration Guide - Config structure and merge order</li> <li>API Reference - Plugin interfaces</li> </ul>"},{"location":"architecture/execution-flow/#code-references","title":"Code References","text":"<p>Key files for deep diving:</p> Component File Path Lines of Interest CLI Entry <code>src/elspeth/core/cli/suite.py</code> CLI argument parsing Config Loading <code>src/elspeth/core/cli/config_utils.py</code> Merge logic, validation Suite Runner <code>src/elspeth/core/experiments/suite_runner.py</code> Orchestration Row Processing <code>src/elspeth/core/experiments/runner.py</code> Core execution loop Artifact Pipeline <code>src/elspeth/core/pipeline/artifact_pipeline.py</code> Dependency resolution Security Validation <code>src/elspeth/core/base/plugin.py</code> <code>validate_can_operate_at_level()</code> <p>Execution Tracing</p> <p>For detailed execution traces, enable debug logging: <pre><code>export ELSPETH_LOG_LEVEL=DEBUG\npython -m elspeth.cli --settings ...\n</code></pre></p> <p>Check <code>logs/run_*.jsonl</code> for complete audit trail with correlation IDs.</p>"},{"location":"architecture/overview/","title":"Architecture Overview","text":"<p>Understand Elspeth's component-based architecture and data flow.</p> <p>Design Philosophy</p> <p>Elspeth follows security-first design principles with defense-in-depth, fail-fast validation, and explicit security level propagation throughout the pipeline.</p>"},{"location":"architecture/overview/#system-architecture","title":"System Architecture","text":"<p>Elspeth is organized into six core layers:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  CLI &amp; Configuration                                        \u2502\n\u2502  \u2514\u2500 YAML validation, environment resolution, suite loading  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Datasources (Ingress)                                      \u2502\n\u2502  \u2514\u2500 CSV local/blob, Azure Blob \u2192 ClassifiedDataFrame       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Workflow Orchestrator                                       \u2502\n\u2502  \u2514\u2500 Bind datasource + transforms + sinks + middleware       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Workflow Runner                                             \u2502\n\u2502  \u2514\u2500 Concurrency, retries, middleware chain, validation      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Transforms (with Middleware)                                \u2502\n\u2502  \u2514\u2500 LLM (Azure OpenAI, Mock), ETL, Analytics, Rules        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Artifact Pipeline (Egress)                                 \u2502\n\u2502  \u2514\u2500 Dependency-ordered sinks with security enforcement      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <pre><code>graph TD\n    CLI[CLI &amp; Configuration&lt;br/&gt;YAML validation, environment resolution]\n    DS[Datasources Ingress&lt;br/&gt;CSV local/blob, Azure Blob]\n    ORCH[Workflow Orchestrator&lt;br/&gt;Bind datasource + transforms + sinks]\n    RUNNER[Workflow Runner&lt;br/&gt;Concurrency, retries, middleware]\n    TRANSFORM[Transforms&lt;br/&gt;LLM, ETL, Analytics, Rules]\n    PIPE[Artifact Pipeline Egress&lt;br/&gt;Dependency-ordered sinks]\n\n    CLI --&gt; DS\n    DS --&gt; ORCH\n    ORCH --&gt; RUNNER\n    RUNNER --&gt; TRANSFORM\n    TRANSFORM --&gt; PIPE\n\n    style CLI fill:#E6F3FF\n    style DS fill:#FFE6E6\n    style ORCH fill:#E6FFE6\n    style RUNNER fill:#FFF3E6\n    style TRANSFORM fill:#F3E6FF\n    style PIPE fill:#FFFFE6</code></pre>"},{"location":"architecture/overview/#core-principles","title":"Core Principles","text":""},{"location":"architecture/overview/#1-defense-by-design","title":"1. Defense by Design","text":"<p>All external integrations use typed protocols so untrusted components can be swapped without touching orchestration logic.</p> <pre><code># Protocols define contracts\nclass DataSource(Protocol):\n    def load_data(self) -&gt; ClassifiedDataFrame: ...\n\nclass Transform(Protocol):\n    def transform(self, frame: ClassifiedDataFrame) -&gt; ClassifiedDataFrame: ...\n\nclass Sink(Protocol):\n    def write(self, frame: ClassifiedDataFrame, metadata: dict) -&gt; None: ...\n</code></pre> <p>Benefits: - \u2705 Swap implementations without changing core code - \u2705 Plugin boundaries prevent contamination - \u2705 Security enforcement isolated from business logic</p>"},{"location":"architecture/overview/#2-configuration-as-code","title":"2. Configuration as Code","text":"<p>Profiles are validated YAML merged with prompt packs, preventing runtime surprises and enabling fail-fast feedback.</p> <pre><code># settings.yaml (validated at load time)\nllm:\n  type: azure_openai\n  endpoint: ${AZURE_OPENAI_ENDPOINT}  # \u2190 Validated before run\n  middleware:\n    - type: pii_shield              # \u2190 Schema validated\n</code></pre> <p>Benefits: - \u2705 Configuration errors caught before data retrieval - \u2705 No runtime surprises from malformed config - \u2705 Environment variable resolution with validation</p>"},{"location":"architecture/overview/#3-traceable-execution","title":"3. Traceable Execution","text":"<p>Every run records retries, aggregates, costs, and security classifications for consistent audit trails.</p> <pre><code>{\n  \"run_id\": \"exp-2025-10-26-001\",\n  \"security_level\": \"OFFICIAL\",\n  \"retries\": {\"total\": 5, \"successful\": 4, \"exhausted\": 1},\n  \"cost_summary\": {\"total_cost\": 0.45, \"prompt_tokens\": 1250, \"completion_tokens\": 380},\n  \"baseline_comparison\": {\"delta\": 0.12, \"p_value\": 0.03}\n}\n</code></pre> <p>Benefits: - \u2705 Full audit trail for compliance - \u2705 Cost tracking per experiment - \u2705 Retry history for debugging</p>"},{"location":"architecture/overview/#4-least-privilege-propagation","title":"4. Least Privilege Propagation","text":"<p>Data, middleware, and artifacts carry explicit security levels allowing downstream sinks to enforce clearance.</p> <pre><code># Data carries classification\nframe = ClassifiedDataFrame.create_from_datasource(data, SecurityLevel.OFFICIAL)\n\n# Sink validates clearance before writing\nsink.validate_can_operate_at_level(frame.classification)  # \u2705 or \u274c\n</code></pre> <p>Benefits: - \u2705 No implicit security assumptions - \u2705 Fail-fast on clearance violations - \u2705 Defense-in-depth (multiple validation points)</p>"},{"location":"architecture/overview/#component-layers","title":"Component Layers","text":""},{"location":"architecture/overview/#ingress-datasources","title":"Ingress: Datasources","text":"<p>Datasources load tabular data and tag each frame with its classification.</p> <p>Built-in Datasources: - <code>csv_local</code> - Local filesystem CSV - <code>csv_blob</code> - Azure Blob Storage (direct URI) - <code>azure_blob</code> - Azure Blob with profile-based auth</p> <p>Key Features: - Security level tagging - <code>on_error</code> policies (abort/skip/log) - Type coercion via <code>dtype</code> hints</p> <p>Example: <pre><code>frame = datasource.load_data()\n# \u2192 ClassifiedDataFrame(data=DataFrame, classification=OFFICIAL)\n</code></pre></p>"},{"location":"architecture/overview/#configuration-loader","title":"Configuration Loader","text":"<p>Merges configuration from multiple sources in priority order:</p> <pre><code>1. Suite defaults (settings.yaml)\n        \u2193\n2. Prompt packs (optional)\n        \u2193\n3. Experiment overrides (experiments/*.yaml)\n        \u2193\n4. Runtime plugins instantiated\n</code></pre> <p>Validation stages: - \u2705 YAML syntax validation - \u2705 JSON schema validation per plugin - \u2705 Security level consistency checks - \u2705 Environment variable resolution</p>"},{"location":"architecture/overview/#orchestrator","title":"Orchestrator","text":"<p>Binds datasource, transforms, sinks, and optional controls into a cohesive workflow.</p> <p>Responsibilities: - Instantiate plugins from configuration - Validate security levels across components - Compute operating level (minimum of all components) - Share middleware instances across experiment runs</p> <p>Example: <pre><code>orchestrator = Orchestrator(\n    datasource=csv_datasource,\n    llm=azure_openai_client,\n    sinks=[csv_sink, excel_sink],\n    middleware=[pii_shield, audit_logger]\n)\n\noperating_level = orchestrator.compute_operating_level()\n# \u2192 SecurityLevel.OFFICIAL (minimum across all components)\n</code></pre></p>"},{"location":"architecture/overview/#workflow-runner","title":"Workflow Runner","text":"<p>Executes the workflow with concurrency, retries, and validation.</p> <p>Execution Flow: <pre><code>1. Load data from datasource\n        \u2193\n2. For each row (with concurrency):\n   a. Apply middleware.before_request()\n   b. Call LLM.transform()\n   c. Apply middleware.after_response()\n   d. Validate response\n   e. Handle retries on errors\n        \u2193\n3. Run aggregation plugins\n        \u2193\n4. Execute artifact pipeline (sinks)\n</code></pre></p> <p>Key Features: - Concurrency: Thread pool execution with backlog control - Retries: Exponential backoff with configurable max attempts - Checkpointing: Resume from last processed row - Early Stop: Conditional halt based on metrics - Cost Tracking: Token usage and API cost aggregation</p>"},{"location":"architecture/overview/#transforms","title":"Transforms","text":"<p>Process data through transforms (LLMs, ETL, analytics, rules) with middleware pipeline.</p> <p>Middleware Stack: <pre><code>Request \u2192 PII Shield \u2192 Classified Material Filter \u2192 Prompt Shield \u2192 LLM API\n                                                                      \u2193\nResponse \u2190 Audit Logger \u2190 Health Monitor \u2190 Content Safety \u2190 LLM API\n</code></pre></p> <p>Middleware Types: - Security Filters: Block PII, classified markings, banned terms - Monitoring: Audit logging, health metrics, latency tracking - Content Safety: Azure Content Safety API integration</p> <p>Example: <pre><code># Middleware runs in declaration order\nllm_with_middleware = LLMClient(\n    middleware=[\n        PIIShield(on_violation=\"abort\"),\n        ClassifiedMaterialFilter(on_violation=\"abort\"),\n        AuditLogger(include_prompts=False),\n        HealthMonitor(heartbeat_interval=60)\n    ]\n)\n</code></pre></p>"},{"location":"architecture/overview/#artifact-pipeline","title":"Artifact Pipeline","text":"<p>Executes sinks in dependency order with security enforcement.</p> <p>Dependency Resolution: <pre><code>sinks:\n  # Independent (run first, in parallel)\n  - name: csv\n    type: csv\n\n  - name: excel\n    type: excel_workbook\n\n  # Dependent (runs after csv)\n  - name: signed\n    type: signed_artifact\n    consumes: [csv]\n</code></pre></p> <p>Execution Order: <pre><code>1. csv, excel (parallel)\n       \u2193\n2. signed (waits for csv)\n</code></pre></p> <p>Security Enforcement: - Each sink validates it can handle data classification - Insufficient clearance aborts pipeline - Security level downgrade attempts are blocked</p> <p>Example: <pre><code>pipeline = ArtifactPipeline(\n    sinks=[csv_sink, excel_sink, signed_sink],\n    operating_level=SecurityLevel.OFFICIAL\n)\n\n# Each sink validates before writing\npipeline.execute(frame, metadata)\n</code></pre></p>"},{"location":"architecture/overview/#data-flow","title":"Data Flow","text":""},{"location":"architecture/overview/#end-to-end-pipeline","title":"End-to-End Pipeline","text":"<pre><code>CSV File (OFFICIAL classification)\n    \u2193\nDatasource.load_data()\n    \u2193\nClassifiedDataFrame(data, classification=OFFICIAL)\n    \u2193\nExperimentRunner (operating_level=OFFICIAL)\n    \u2193\nFor each row:\n  \u251c\u2500 Middleware.before_request() \u2192 PII check, audit log\n  \u251c\u2500 LLM.transform() \u2192 Azure OpenAI API call\n  \u251c\u2500 Middleware.after_response() \u2192 Content safety, health metrics\n  \u2514\u2500 Validation \u2192 Regex/JSON validation\n    \u2193\nAggregation plugins \u2192 Statistics, cost summary\n    \u2193\nArtifactPipeline\n  \u251c\u2500 CSVSink.write() \u2192 results.csv\n  \u251c\u2500 ExcelSink.write() \u2192 report.xlsx\n  \u2514\u2500 SignedSink.write() \u2192 signed_bundle.tar.gz + .sig\n</code></pre>"},{"location":"architecture/overview/#security-level-propagation","title":"Security Level Propagation","text":"<pre><code>1. Datasource declares: SecurityLevel.OFFICIAL\n        \u2193\n2. Data tagged: ClassifiedDataFrame(classification=OFFICIAL)\n        \u2193\n3. Operating level computed: MIN(datasource, llm, sinks) = OFFICIAL\n        \u2193\n4. Each component validates:\n   - Datasource: OFFICIAL clearance, operating at OFFICIAL \u2192 \u2705 OK\n   - LLM: SECRET clearance, operating at OFFICIAL \u2192 \u2705 OK (trusted downgrade)\n   - Sinks: OFFICIAL clearance, operating at OFFICIAL \u2192 \u2705 OK\n        \u2193\n5. Pipeline executes with OFFICIAL enforcement throughout\n</code></pre> <pre><code>graph TD\n    START[Start Pipeline]\n    DECLARE[Datasource declares&lt;br/&gt;SecurityLevel.OFFICIAL]\n    TAG[Data tagged&lt;br/&gt;ClassifiedDataFrame&lt;br/&gt;classification=OFFICIAL]\n    COMPUTE[Compute Operating Level&lt;br/&gt;min datasource, llm, sinks&lt;br/&gt;= OFFICIAL]\n    VALIDATE{Validate Each&lt;br/&gt;Component}\n\n    DS_CHECK[Datasource: OFFICIAL \u2265 OFFICIAL&lt;br/&gt;\u2705 OK]\n    LLM_CHECK[LLM: SECRET \u2265 OFFICIAL&lt;br/&gt;\u2705 OK trusted downgrade]\n    SINK_CHECK[Sinks: OFFICIAL \u2265 OFFICIAL&lt;br/&gt;\u2705 OK]\n\n    EXECUTE[\ud83c\udf89 Pipeline Executes&lt;br/&gt;with OFFICIAL enforcement]\n\n    START --&gt; DECLARE\n    DECLARE --&gt; TAG\n    TAG --&gt; COMPUTE\n    COMPUTE --&gt; VALIDATE\n    VALIDATE --&gt; DS_CHECK\n    VALIDATE --&gt; LLM_CHECK\n    VALIDATE --&gt; SINK_CHECK\n    DS_CHECK &amp; LLM_CHECK &amp; SINK_CHECK --&gt; EXECUTE\n\n    style START fill:#E6F3FF\n    style DECLARE fill:#FFE6E6\n    style TAG fill:#FFF3E6\n    style COMPUTE fill:#ADD8E6\n    style DS_CHECK fill:#90EE90\n    style LLM_CHECK fill:#90EE90\n    style SINK_CHECK fill:#90EE90\n    style EXECUTE fill:#98FB98</code></pre>"},{"location":"architecture/overview/#security-posture-highlights","title":"Security Posture Highlights","text":""},{"location":"architecture/overview/#prompt-hygiene","title":"Prompt Hygiene","text":"<p>Prompts render through StrictUndefined Jinja environment:</p> <pre><code># Missing variables raise errors (no silent failures)\ntemplate = jinja_env.from_string(\"Hello {name}\")\ntemplate.render({\"invalid_key\": \"value\"})  # \u274c Raises UndefinedError\n</code></pre>"},{"location":"architecture/overview/#output-sanitization","title":"Output Sanitization","text":"<p>Spreadsheet sinks neutralize formula injection:</p> <pre><code># Input\nvalue = \"=SUM(A1:A10)\"\n\n# Sanitized output\nsanitized = \"'=SUM(A1:A10)\"  # Prefixed to prevent execution\n</code></pre>"},{"location":"architecture/overview/#signed-artifacts","title":"Signed Artifacts","text":"<p>Bundles embed HMAC manifests for tamper evidence:</p> <pre><code>experiment_results.tar.gz          # Bundle\nexperiment_results.tar.gz.sig      # HMAC-SHA256 signature\nexperiment_results.tar.gz.manifest # Metadata (files, checksums, security level)\n</code></pre>"},{"location":"architecture/overview/#middleware-security-stack","title":"Middleware Security Stack","text":"<p>Production middleware order:</p> <pre><code>llm:\n  middleware:\n    - type: pii_shield              # 1. Block PII\n    - type: classified_material     # 2. Block classified markings\n    - type: prompt_shield           # 3. Block banned terms\n    - type: azure_content_safety    # 4. External content safety check\n    - type: audit_logger            # 5. Log sanitized prompts\n    - type: health_monitor          # 6. Track performance\n</code></pre>"},{"location":"architecture/overview/#advanced-features","title":"Advanced Features","text":""},{"location":"architecture/overview/#concurrency-control","title":"Concurrency Control","text":"<p>Thread pool execution with rate limiter awareness:</p> <pre><code>concurrency:\n  max_workers: 4\n  backlog_threshold: 100\n  enabled: true\n</code></pre> <p>Decision tree: <pre><code>Is concurrency.enabled? \u2192 No \u2192 Sequential execution\n        \u2193 Yes\nIs rate_limiter saturated? \u2192 Yes \u2192 Sequential (avoid backpressure)\n        \u2193 No\nLaunch ThreadPoolExecutor(max_workers=4) \u2192 Parallel execution\n</code></pre></p>"},{"location":"architecture/overview/#checkpoint-recovery","title":"Checkpoint Recovery","text":"<p>Resume experiments from last processed row:</p> <pre><code>checkpoint:\n  path: checkpoints/experiment.json\n  field: id  # Row identifier\n</code></pre> <p>Behavior: - Skips rows with IDs already in checkpoint file - Appends new results incrementally - Minimizes replay during resume</p>"},{"location":"architecture/overview/#baseline-comparison","title":"Baseline Comparison","text":"<p>Compare experiment results against baseline:</p> <pre><code>baseline:\n  experiment_name: previous_run\n  comparison_plugins:\n    - type: score_significance\n      criteria: [accuracy, relevance]\n      alpha: 0.05\n</code></pre> <p>Output: <pre><code>{\n  \"baseline_comparison\": {\n    \"delta\": 0.12,\n    \"p_value\": 0.03,\n    \"significant\": true,\n    \"effect_size\": \"medium\"\n  }\n}\n</code></pre></p>"},{"location":"architecture/overview/#early-stop","title":"Early Stop","text":"<p>Halt experiment when condition met:</p> <pre><code>early_stop:\n  - type: threshold\n    metric: accuracy\n    threshold: 0.95\n    comparison: greater\n    min_rows: 100\n</code></pre> <p>Behavior: - Evaluates after each row - Halts queue submission when triggered - Records trigger metadata for audit</p>"},{"location":"architecture/overview/#extension-points","title":"Extension Points","text":""},{"location":"architecture/overview/#custom-plugins","title":"Custom Plugins","text":"<p>Elspeth supports custom implementations of:</p> Plugin Type Interface Example Use Case Datasource <code>load_data()</code> PostgreSQL, REST API, Snowflake Transform <code>transform()</code> Custom LLM, Rule engine, ML model Sink <code>write()</code> S3, BigQuery, Kafka, Webhook Middleware <code>before_request()</code>, <code>after_response()</code> Custom security filter, Telemetry Validation <code>validate()</code> JSON Schema, Regex, Custom rules Aggregation <code>aggregate()</code> Custom statistics, Business metrics <p>See API Reference for plugin development guide.</p>"},{"location":"architecture/overview/#areas-to-monitor","title":"Areas to Monitor","text":""},{"location":"architecture/overview/#credential-management","title":"Credential Management","text":"<p>Secure practices: - \u2705 Use environment variables for secrets - \u2705 Inject via CI/CD secret stores - \u2705 Never commit secrets to Git - \u2705 Rotate credentials regularly - \u2705 Use managed identities (Azure, AWS IAM) when possible</p> <p>Plugins requiring credentials: - Azure Blob sinks - GitHub/Azure DevOps repository sinks - Signed artifact sinks (signing keys) - Azure Content Safety middleware - LLM clients (API keys)</p>"},{"location":"architecture/overview/#network-boundaries","title":"Network Boundaries","text":"<p>Outbound HTTP calls: - Azure Content Safety middleware - GitHub/Azure DevOps repository sinks - LLM API calls (Azure OpenAI, OpenAI HTTP)</p> <p>Security controls: - Configure outbound firewall rules - Set timeouts appropriately - Use allowlisted endpoints only - Monitor for failed connections</p>"},{"location":"architecture/overview/#dependency-management","title":"Dependency Management","text":"<p>Optional extras: - Statistical plugins \u2192 scipy, numpy - Excel sinks \u2192 openpyxl - Visual analytics \u2192 matplotlib, seaborn</p> <p>Best practices: - Pin exact versions (requirements lockfiles) - Monitor for vulnerabilities - Regular dependency audits - Use <code>--require-hashes</code> for installs</p>"},{"location":"architecture/overview/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"architecture/overview/#scalability","title":"Scalability","text":"Component Bottleneck Mitigation Datasource File I/O Stream large files, use blob storage LLM API Rate limits Adaptive rate limiter, concurrency control Sinks Disk writes Parallel execution, async writes Memory Large datasets Process in chunks, streaming DataFrames"},{"location":"architecture/overview/#optimization-tips","title":"Optimization Tips","text":"<ol> <li>Use concurrency for I/O-bound workloads (LLM calls)</li> <li>Checkpoint frequently for long-running experiments</li> <li>Use adaptive rate limiting to maximize throughput</li> <li>Filter data early (datasource-level filtering)</li> <li>Minimize middleware (only necessary security filters)</li> </ol>"},{"location":"architecture/overview/#related-documentation","title":"Related Documentation","text":"<ul> <li>ADR Catalogue - Architecture Decision Records</li> <li>Security Model - Bell-LaPadula MLS enforcement</li> <li>Configuration Guide - YAML configuration reference</li> <li>API Reference - Plugin development</li> </ul>"},{"location":"architecture/overview/#key-design-decisions","title":"Key Design Decisions","text":"Decision Rationale ADR Nominal typing (ABC not Protocol) Prevent security bypass attacks via duck typing ADR-004 Immutable classification Prevent data laundering and downgrade attacks ADR-002 Frozen dataclass Classification cannot be modified after creation ADR-002a min() for operating level \"Weakest link\" determines pipeline security ADR-002 @final security methods Prevent subclass override of security enforcement ADR-004 Constructor protection Only datasources create ClassifiedDataFrame ADR-002a Frozen plugins Dedicated infrastructure (allow_downgrade=False) ADR-005 <p>See ADR Catalogue for complete decision history.</p> <p>Architecture Summary</p> <p>Elspeth implements a security-first pipeline architecture with:</p> <ul> <li>\u2705 6 core layers (CLI \u2192 Datasource \u2192 Orchestrator \u2192 Runner \u2192 LLM \u2192 Sinks)</li> <li>\u2705 4 design principles (Defense, Configuration as Code, Traceability, Least Privilege)</li> <li>\u2705 Multiple extension points for custom plugins</li> <li>\u2705 Defense-in-depth with middleware, validation, and signing</li> <li>\u2705 Fail-fast enforcement at every security boundary</li> </ul> <p>The architecture balances flexibility (plugin system) with security (immutable classification, typed protocols, validation).</p>"},{"location":"architecture/security-policy/","title":"Elspeth Security Policy","text":"<p>Document Version: 1.0 Last Updated: 2025-10-26 Status: Active Classification: OFFICIAL</p>"},{"location":"architecture/security-policy/#executive-summary","title":"Executive Summary","text":"<p>Elspeth implements a defense-in-depth security architecture based on Bell-LaPadula Multi-Level Security (MLS) principles. This policy consolidates 7 Architecture Decision Records (ADRs 002, 002a, 002b, 003, 004, 005, 006) into a cohesive security framework for protecting classified data ranging from UNOFFICIAL to SECRET.</p>"},{"location":"architecture/security-policy/#core-security-guarantees","title":"Core Security Guarantees","text":"<ol> <li>Mandatory Access Control (MAC): Pipeline-wide security level enforcement based on minimum clearance principle</li> <li>Immutable Classification: Data classifications cannot be downgraded once assigned (high water mark)</li> <li>Plugin Validation: All plugins undergo security clearance validation before data retrieval</li> <li>Fail-Fast Enforcement: Security violations abort pipelines before data access</li> <li>Fail-Loud Invariants: Critical security violations trigger emergency logging and platform termination</li> <li>Defense in Depth: Multiple independent security layers (type system, runtime checks, policy enforcement, testing)</li> </ol>"},{"location":"architecture/security-policy/#applicable-security-levels","title":"Applicable Security Levels","text":"<p>Elspeth uses Australian Protective Security Policy Framework (PSPF) classifications:</p> Level Numeric Description Example Use Cases UNOFFICIAL 0 Publicly releasable information Public datasets, test data, marketing content OFFICIAL 1 Government/business information requiring basic protection Internal reports, business analytics, operational data OFFICIAL SENSITIVE 2 Information requiring additional safeguards Personnel records, procurement data, sensitive research PROTECTED 3 High-impact information requiring strong protection Security assessments, commercial-in-confidence, privacy data SECRET 4 Very high-impact information, substantial damage if disclosed National security, classified research, intelligence data"},{"location":"architecture/security-policy/#policy-1-multi-level-security-mls-enforcement","title":"Policy 1: Multi-Level Security (MLS) Enforcement","text":"<p>Source: ADR-002 Status: Mandatory Enforcement: Compile-time (MyPy) + Runtime (suite_runner.py) + Test (CI)</p>"},{"location":"architecture/security-policy/#11-bell-lapadula-access-control-model","title":"1.1 Bell-LaPadula Access Control Model","text":"<p>Elspeth implements the Bell-LaPadula \"no read up, no write down\" security model with the following rules:</p>"},{"location":"architecture/security-policy/#rule-1-simple-security-property-no-read-up","title":"Rule 1: Simple Security Property (\"No Read Up\")","text":"<p>A subject (plugin) at a given security level cannot READ information classified at a HIGHER level.</p> <p>Implementation: Plugins with LOWER clearance cannot operate in pipelines with HIGHER operating levels.</p> <p>Example (REJECTION): <pre><code># \u274c ABORTS: UNOFFICIAL datasource cannot operate in SECRET pipeline\ndatasource:\n  type: csv_local\n  security_level: UNOFFICIAL\n\ntransform:\n  type: azure_openai\n  security_level: SECRET\n\n# Operating level = min(UNOFFICIAL, SECRET) = UNOFFICIAL\n# Transform validation: UNOFFICIAL &lt; SECRET \u2192 REJECT (insufficient clearance)\n</code></pre></p> <p>Example (SUCCESS): <pre><code># \u2705 SUCCESS: SECRET datasource can operate in UNOFFICIAL pipeline (trusted downgrade)\ndatasource:\n  type: azure_blob\n  security_level: SECRET\n  allow_downgrade: true  # Explicit opt-in for trusted downgrade\n\ntransform:\n  type: mock_llm\n  security_level: UNOFFICIAL\n\n# Operating level = min(SECRET, UNOFFICIAL) = UNOFFICIAL\n# Datasource validation: UNOFFICIAL &lt; SECRET and allow_downgrade=true \u2192 ALLOW (filters data)\n</code></pre></p>"},{"location":"architecture/security-policy/#rule-2-star-property-no-write-down","title":"Rule 2: Star Property (\"No Write Down\")","text":"<p>A subject (plugin) at a given security level cannot WRITE information to a LOWER classification level without explicit authorization.</p> <p>Implementation: Data classifications can only INCREASE via explicit <code>with_uplifted_classification()</code> calls. Downgrade attempts raise <code>SecurityCriticalError</code>.</p> <p>Example (REJECTION): <pre><code># \u274c CRITICAL ERROR: Classification downgrade violates invariant\nsecret_frame = ClassifiedDataFrame.create_from_datasource(df, SecurityLevel.SECRET)\nresult = secret_frame.with_uplifted_classification(SecurityLevel.UNOFFICIAL)\n# \u2192 Raises SecurityCriticalError! Platform terminates immediately.\n</code></pre></p> <p>Example (SUCCESS): <pre><code># \u2705 SUCCESS: Classification uplift is allowed (high water mark)\nunofficial_frame = ClassifiedDataFrame.create_from_datasource(df, SecurityLevel.UNOFFICIAL)\nresult = unofficial_frame.with_uplifted_classification(SecurityLevel.SECRET)\n# \u2192 Returns new frame with SECRET classification\n</code></pre></p>"},{"location":"architecture/security-policy/#12-operating-level-computation","title":"1.2 Operating Level Computation","text":"<p>The operating level is the minimum security level across ALL pipeline components:</p> <pre><code>operating_level = min(\n    datasource.security_level,\n    transform.security_level,\n    sink1.security_level,\n    sink2.security_level,\n    # ... all other plugins\n)\n</code></pre> <p>Validation Sequence:</p> <ol> <li>Pre-Execution Validation (fail-fast, before data retrieval):</li> <li>Collect all plugin security levels</li> <li>Compute operating level as minimum</li> <li>Validate each plugin can operate at computed level</li> <li> <p>ABORT pipeline if any validation fails</p> </li> <li> <p>Runtime Classification Checks:</p> </li> <li>Datasource creates <code>ClassifiedDataFrame</code> with declared level</li> <li>Transforms preserve or uplift classification (never downgrade)</li> <li> <p>Sinks verify input classification matches expectations</p> </li> <li> <p>Sink Chaining Validation:</p> </li> <li>Each sink validates it can accept the output classification</li> <li>Chained sinks (artifact pipeline) validate dependency classification compatibility</li> </ol>"},{"location":"architecture/security-policy/#13-bell-lapadula-directionality-data-vs-plugin-operations","title":"1.3 Bell-LaPadula Directionality: Data vs Plugin Operations","text":"<p>CRITICAL DISTINCTION: Data classifications and plugin operations move in OPPOSITE directions:</p> Aspect Direction Enforcement Example Data Classifications Can only INCREASE <code>with_uplifted_classification()</code> UNOFFICIAL \u2192 OFFICIAL \u2192 SECRET Plugin Operations Can only DECREASE <code>allow_downgrade=True</code> SECRET \u2192 OFFICIAL \u2192 UNOFFICIAL <p>Forbidden Operations: - \u274c UNOFFICIAL plugin operating at SECRET level (insufficient clearance) - \u274c SECRET data downgrading to UNOFFICIAL (no write down) - \u274c Frozen plugin (allow_downgrade=False) operating below declared level</p> <p>Allowed Operations: - \u2705 SECRET plugin operating at UNOFFICIAL level (if allow_downgrade=True - trusted to filter) - \u2705 UNOFFICIAL data uplifted to SECRET (explicit via with_uplifted_classification()) - \u2705 Frozen plugin operating at EXACT declared level only</p>"},{"location":"architecture/security-policy/#14-compliance-requirements","title":"1.4 Compliance Requirements","text":"<p>Implementation Checklist: - [x] All plugins MUST declare <code>security_level</code> parameter - [x] All plugins MUST declare <code>allow_downgrade</code> parameter (no default) - [x] Operating level computed BEFORE data retrieval - [x] Validation failures ABORT pipeline with <code>SecurityValidationError</code> - [x] All components validated via <code>validate_can_operate_at_level()</code> - [x] Pipeline execution logs include security level in audit trail</p> <p>Audit Evidence: - Security level computation: <code>src/elspeth/core/experiments/suite_runner.py:_compute_operating_level()</code> - Validation enforcement: <code>src/elspeth/core/experiments/suite_runner.py:_validate_component_clearances()</code> - Test coverage: <code>tests/test_adr002_*.py</code> (Bell-LaPadula scenarios)</p>"},{"location":"architecture/security-policy/#policy-2-trusted-container-model","title":"Policy 2: Trusted Container Model","text":"<p>Source: ADR-002a Status: Mandatory Enforcement: Compile-time (frozen dataclass) + Runtime (factory method) + Test (CI)</p>"},{"location":"architecture/security-policy/#21-classifieddataframe-immutability","title":"2.1 ClassifiedDataFrame Immutability","text":"<p>All classified data MUST be encapsulated in <code>ClassifiedDataFrame</code> containers with the following guarantees:</p> <ol> <li>Immutable Classification: Security metadata cannot be modified after creation</li> <li>Datasource-Only Creation: Only datasources can create fresh containers (prevents laundering)</li> <li>Content Mutability: Data content CAN be modified (e.g., adding columns, filtering rows)</li> <li>Classification Propagation: Transformations create new containers preserving/uplifting classification</li> </ol>"},{"location":"architecture/security-policy/#22-container-creation-rules","title":"2.2 Container Creation Rules","text":""},{"location":"architecture/security-policy/#rule-1-datasource-only-factory-method","title":"Rule 1: Datasource-Only Factory Method","text":"<pre><code># \u2705 ALLOWED: Datasource creating ClassifiedDataFrame\n@dataclass(frozen=True)\nclass ClassifiedDataFrame:\n    data: pd.DataFrame\n    classification: SecurityLevel\n\n    @classmethod\n    def create_from_datasource(\n        cls,\n        data: pd.DataFrame,\n        classification: SecurityLevel\n    ) -&gt; \"ClassifiedDataFrame\":\n        \"\"\"Factory method - ONLY datasources may call this.\"\"\"\n        return cls(data=data, classification=classification)\n</code></pre> <p>Enforcement: Code review + convention (future: decorator validation)</p>"},{"location":"architecture/security-policy/#rule-2-transform-classification-preservation","title":"Rule 2: Transform Classification Preservation","text":"<pre><code># \u2705 ALLOWED: Transform preserving classification\ndef transform(self, frame: ClassifiedDataFrame) -&gt; ClassifiedDataFrame:\n    processed_data = self._process(frame.data)\n    # New container, SAME or HIGHER classification\n    return ClassifiedDataFrame(\n        data=processed_data,\n        classification=frame.classification  # Preserved\n    )\n</code></pre>"},{"location":"architecture/security-policy/#rule-3-explicit-uplift-only","title":"Rule 3: Explicit Uplift Only","text":"<pre><code># \u2705 ALLOWED: Explicit classification uplift\ndef transform(self, frame: ClassifiedDataFrame) -&gt; ClassifiedDataFrame:\n    processed_data = self._process(frame.data)\n    plugin_level = self.get_security_level()\n    target_level = max(frame.classification, plugin_level)\n    return frame.with_uplifted_classification(target_level)\n</code></pre>"},{"location":"architecture/security-policy/#23-attack-scenario-prevention","title":"2.3 Attack Scenario Prevention","text":"<p>Threat: Classification Laundering Attack</p> <p>A malicious plugin creates a \"fresh\" ClassifiedDataFrame with LOWER classification to bypass MLS controls:</p> <pre><code># \u274c PREVENTED: Malicious plugin laundering SECRET data as UNOFFICIAL\nclass MaliciousTransform(BasePlugin):\n    def transform(self, frame: ClassifiedDataFrame) -&gt; ClassifiedDataFrame:\n        # Received SECRET frame\n        assert frame.classification == SecurityLevel.SECRET\n\n        # ATTACK: Create \"fresh\" frame with UNOFFICIAL classification\n        # This would allow SECRET data to reach UNOFFICIAL sinks!\n        laundered = ClassifiedDataFrame.create_from_datasource(\n            frame.data,  # SECRET data\n            SecurityLevel.UNOFFICIAL  # \u274c Laundered classification\n        )\n        return laundered\n</code></pre> <p>Defense: - Factory method convention: ONLY datasources create containers - Code review: Grep for <code>create_from_datasource</code> in transform code - Future enhancement: Decorator validation or capability-based security</p>"},{"location":"architecture/security-policy/#24-compliance-requirements","title":"2.4 Compliance Requirements","text":"<p>Implementation Checklist: - [x] ClassifiedDataFrame is a frozen dataclass (immutable fields) - [x] Only datasources use <code>create_from_datasource()</code> - [x] Transforms use existing containers or <code>with_uplifted_classification()</code> - [x] No direct ClassifiedDataFrame() constructor calls outside core - [x] Audit trail logs classification at each pipeline stage</p> <p>Audit Evidence: - Container implementation: <code>src/elspeth/core/security/classified_data.py</code> - Test coverage: <code>tests/test_classified_dataframe.py</code> - Code review checklist: No <code>create_from_datasource()</code> in <code>src/elspeth/plugins/nodes/transforms/</code></p>"},{"location":"architecture/security-policy/#policy-3-immutable-security-policy-metadata","title":"Policy 3: Immutable Security Policy Metadata","text":"<p>Source: ADR-002b Status: Mandatory Enforcement: Compile-time (code review) + Registry (schema validation) + CI (lint rules)</p>"},{"location":"architecture/security-policy/#31-author-owned-security-policy","title":"3.1 Author-Owned Security Policy","text":"<p>Security policy metadata is immutable, author-owned, and signed. Operators cannot override security policy through configuration, environment variables, or runtime hooks.</p>"},{"location":"architecture/security-policy/#rule-1-immutable-policy-fields","title":"Rule 1: Immutable Policy Fields","text":"<p>The following fields are defined solely in plugin code by the author: - <code>security_level</code>: Plugin's security clearance - <code>allow_downgrade</code>: Whether plugin can operate at lower levels - Future policy fields: <code>max_operating_level</code>, compliance tags, etc.</p> <p>Forbidden: Configuration-driven policy overrides</p> <pre><code># \u274c FORBIDDEN: Cannot override security policy via config\ndatasource:\n  type: azure_blob\n  security_level: PROTECTED  # \u274c REJECTED by registry\n  allow_downgrade: false     # \u274c REJECTED by registry\n</code></pre> <p>Correct: Policy defined in code only</p> <pre><code># \u2705 CORRECT: Policy hard-coded by plugin author\nclass AzureBlobDatasource(BasePlugin):\n    def __init__(self, *, config_path: str, profile: str):\n        # Policy is hard-coded, NOT configurable\n        super().__init__(\n            security_level=SecurityLevel.PROTECTED,\n            allow_downgrade=True  # Author's decision\n        )\n        # ... rest of initialization\n</code></pre>"},{"location":"architecture/security-policy/#rule-2-registry-enforcement","title":"Rule 2: Registry Enforcement","text":"<p>Plugin registries reject registration schemas that expose policy fields as configurable parameters.</p> <p>Factory Implementation: <pre><code>def create_azure_blob_datasource(opts: dict, ctx: PluginContext) -&gt; AzureBlobDatasource:\n    \"\"\"Factory ignores security policy fields from config.\"\"\"\n    # Strip security policy fields if present (defensive)\n    safe_opts = {k: v for k, v in opts.items()\n                 if k not in (\"security_level\", \"allow_downgrade\")}\n\n    # Plugin sets policy internally\n    return AzureBlobDatasource(**safe_opts)\n</code></pre></p> <p>Registry Schema: <pre><code>AZURE_BLOB_SCHEMA = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"config_path\": {\"type\": \"string\"},\n        \"profile\": {\"type\": \"string\"},\n        # \u274c NO security_level or allow_downgrade properties\n    },\n    \"required\": [\"config_path\", \"profile\"]\n}\n</code></pre></p>"},{"location":"architecture/security-policy/#rule-3-signature-attestation","title":"Rule 3: Signature Attestation","text":"<p>Published plugins include policy metadata in the signing manifest. Security review verifies the implementation matches declared policy prior to signing.</p> <p>Signing Manifest: <pre><code>{\n  \"plugin\": \"AzureBlobDatasource\",\n  \"version\": \"1.2.0\",\n  \"security_policy\": {\n    \"security_level\": \"PROTECTED\",\n    \"allow_downgrade\": true,\n    \"policy_hash\": \"sha256:abc123...\"\n  },\n  \"signature\": \"...\"\n}\n</code></pre></p> <p>Verification Process: 1. Security team reviews plugin code 2. Extracts declared policy (<code>security_level</code>, <code>allow_downgrade</code>) 3. Verifies policy matches security requirements 4. Signs plugin with attested policy metadata 5. Runtime validates signature matches running code</p>"},{"location":"architecture/security-policy/#32-frozen-vs-trusted-downgrade-selection","title":"3.2 Frozen vs Trusted Downgrade Selection","text":"<p>Authors choose security policy explicitly at development time, not configuration time:</p> Author Choice Policy Declaration Operator Selection Trusted Downgrade <code>allow_downgrade=True</code> in code Use this plugin for mixed-classification workflows Frozen <code>allow_downgrade=False</code> in code Use this plugin ONLY for dedicated classification domains <p>Operator Workflow: <pre><code># Operator selects the RIGHT PLUGIN, not the right configuration\ndatasource:\n  type: azure_blob_trusted  # allow_downgrade=True (hard-coded)\n\n# OR\n\ndatasource:\n  type: azure_blob_frozen   # allow_downgrade=False (hard-coded)\n</code></pre></p>"},{"location":"architecture/security-policy/#33-attack-scenario-prevention","title":"3.3 Attack Scenario Prevention","text":"<p>Threat: Configuration-driven security downgrade</p> <p>An operator (malicious or misconfigured) attempts to override security policy via YAML:</p> <pre><code># ATTACK: Try to override frozen plugin to allow downgrade\ndatasource:\n  type: dedicated_secret_source  # Author set allow_downgrade=False\n  allow_downgrade: true          # \u274c ATTACKER attempts override\n</code></pre> <p>Defense: 1. Registry: Rejects schema with policy fields \u2192 Plugin won't register 2. Factory: Strips policy fields from opts \u2192 Override ignored 3. Plugin: Hard-coded policy in <code>__init__</code> \u2192 Immutable 4. CI Lint: Detects policy fields in YAML \u2192 Build fails</p> <p>Result: Attack prevented at 4 independent layers.</p>"},{"location":"architecture/security-policy/#34-compliance-requirements","title":"3.4 Compliance Requirements","text":"<p>Implementation Checklist: - [x] All plugins define policy in <code>__init__</code>, not from parameters - [x] Registry schemas exclude <code>security_level</code> and <code>allow_downgrade</code> - [x] Factory functions strip policy fields from configuration - [x] CI lint rules detect policy fields in YAML configs - [x] Signing manifests include attested policy metadata - [x] Security review process validates policy before signing</p> <p>Audit Evidence: - Plugin implementation: Hard-coded policy in <code>super().__init__()</code> - Registry schemas: No policy fields in JSON schemas - CI lint: <code>.github/workflows/config-lint.yml</code> - Signing manifest: <code>manifests/plugins/*.json</code></p>"},{"location":"architecture/security-policy/#policy-4-plugin-security-architecture","title":"Policy 4: Plugin Security Architecture","text":"<p>Source: ADR-003, ADR-004 Status: Mandatory Enforcement: Compile-time (ABC inheritance, MyPy) + Runtime (registry, isinstance()) + Test (CI)</p>"},{"location":"architecture/security-policy/#41-mandatory-baseplugin-inheritance-security-bones","title":"4.1 Mandatory BasePlugin Inheritance (\"Security Bones\")","text":"<p>All plugins MUST explicitly inherit from <code>BasePlugin</code> ABC to participate in security validation.</p>"},{"location":"architecture/security-policy/#rule-1-nominal-typing-requirement","title":"Rule 1: Nominal Typing Requirement","text":"<pre><code>from abc import ABC, abstractmethod\n\nclass BasePlugin(ABC):\n    \"\"\"Abstract base class with concrete security enforcement (\"security bones\").\"\"\"\n\n    def __init__(\n        self,\n        *,\n        security_level: SecurityLevel,\n        allow_downgrade: bool,  # MANDATORY, no default\n        **kwargs\n    ):\n        \"\"\"Initialize plugin with security parameters.\n\n        Args:\n            security_level: Plugin's security clearance (MANDATORY).\n            allow_downgrade: Whether plugin can operate at lower levels (MANDATORY).\n                - True: Trusted downgrade (can filter to lower levels)\n                - False: Frozen plugin (exact level matching only)\n\n        Raises:\n            TypeError: If allow_downgrade not provided (no default).\n            ValueError: If security_level is None.\n        \"\"\"\n        if security_level is None:\n            raise ValueError(\"security_level cannot be None (ADR-004)\")\n\n        self._security_level = security_level\n        self._allow_downgrade = allow_downgrade\n        super().__init__(**kwargs)\n\n    @property\n    def security_level(self) -&gt; SecurityLevel:\n        \"\"\"Read-only security level property.\"\"\"\n        return self._security_level\n\n    @property\n    def allow_downgrade(self) -&gt; bool:\n        \"\"\"Read-only downgrade permission property.\"\"\"\n        return self._allow_downgrade\n\n    @final  # Cannot be overridden - sealed for security\n    def validate_can_operate_at_level(self, operating_level: SecurityLevel) -&gt; None:\n        \"\"\"Validate plugin can operate at pipeline level (SEALED - security enforcement).\n\n        Raises:\n            SecurityValidationError: If plugin cannot operate at given level.\n        \"\"\"\n        # Check 1: Insufficient clearance (Bell-LaPadula \"no read up\")\n        if operating_level &gt; self._security_level:\n            raise SecurityValidationError(\n                f\"{type(self).__name__} has clearance {self._security_level.name}, \"\n                f\"but pipeline requires {operating_level.name}. Insufficient clearance.\"\n            )\n\n        # Check 2: Frozen plugin downgrade rejection\n        if operating_level &lt; self._security_level and not self._allow_downgrade:\n            raise SecurityValidationError(\n                f\"{type(self).__name__} is frozen at {self._security_level.name} \"\n                f\"(allow_downgrade=False). Cannot operate at lower level {operating_level.name}.\"\n            )\n\n        # Check 3: Valid operation (exact match or trusted downgrade)\n</code></pre> <p>Why Sealed Methods (@final)?</p> <p>Preventing override attacks: <pre><code># \u274c PREVENTED: Malicious override bypassing security\nclass MaliciousPlugin(BasePlugin):\n    # This override will FAIL due to @final + __init_subclass__ enforcement\n    def validate_can_operate_at_level(self, operating_level: SecurityLevel) -&gt; None:\n        pass  # \u274c Bypass attempt - blocked by @final decorator\n</code></pre></p>"},{"location":"architecture/security-policy/#rule-2-central-plugin-type-registry","title":"Rule 2: Central Plugin Type Registry","text":"<p>All plugin types MUST be registered in <code>PLUGIN_TYPE_REGISTRY</code> to ensure security validation coverage.</p> <pre><code># src/elspeth/core/base/plugin_types.py\nPLUGIN_TYPE_REGISTRY = {\n    \"llm_client\": {\"type\": \"singleton\", \"protocol\": \"LLMClient\"},\n    \"llm_middlewares\": {\"type\": \"list\", \"protocol\": \"LLMMiddleware\"},\n    \"row_plugins\": {\"type\": \"list\", \"protocol\": \"RowExperimentPlugin\"},\n    \"aggregator_plugins\": {\"type\": \"list\", \"protocol\": \"AggregationExperimentPlugin\"},\n    \"validation_plugins\": {\"type\": \"list\", \"protocol\": \"ValidationPlugin\"},\n    \"early_stop_plugins\": {\"type\": \"list\", \"protocol\": \"EarlyStopPlugin\"},\n}\n\ndef collect_all_plugins(runner) -&gt; list[BasePlugin]:\n    \"\"\"Collect all plugins using registry (ensures completeness).\"\"\"\n    plugins = []\n    for attr_name, config in PLUGIN_TYPE_REGISTRY.items():\n        attr = getattr(runner, attr_name, None)\n        if attr is None:\n            continue\n\n        if config[\"type\"] == \"singleton\":\n            if isinstance(attr, BasePlugin):\n                plugins.append(attr)\n        elif config[\"type\"] == \"list\":\n            plugins.extend([p for p in attr if isinstance(p, BasePlugin)])\n\n    return plugins\n</code></pre> <p>Registry Completeness Test: <pre><code>def test_plugin_registry_complete():\n    \"\"\"SECURITY: Verify all plugin types registered.\"\"\"\n    runner_attrs = [\n        a for a in dir(ExperimentRunner)\n        if (a.endswith('_plugins') or a.endswith('_middlewares') or a.endswith('_client'))\n        and not a.startswith('_')\n    ]\n\n    registered = set(PLUGIN_TYPE_REGISTRY.keys())\n    missing = set(runner_attrs) - registered\n\n    assert not missing, (\n        f\"SECURITY: {missing} exist in ExperimentRunner but NOT in registry. \"\n        f\"Will bypass ADR-002 validation!\"\n    )\n</code></pre></p>"},{"location":"architecture/security-policy/#42-defense-matrix","title":"4.2 Defense Matrix","text":"Failure Mode Layer 1 (ABC) Layer 2 (Registry) Layer 3 (Test) Outcome Accidental class with matching methods \u2705 CATCHES - - Rejected (no inheritance) New plugin type, forgot to register - \u274c Misses \u2705 CATCHES Test fails New plugin type, forgot both \u2705 CATCHES \u274c Misses \u2705 CATCHES Test fails + rejected Malicious override of sealed method \u2705 CATCHES - - <code>@final</code> blocks override Correct implementation \u2705 Passes \u2705 Passes \u2705 Passes Works \u2705 <p>Result: No single point of failure. Multiple independent defenses.</p>"},{"location":"architecture/security-policy/#43-compliance-requirements","title":"4.3 Compliance Requirements","text":"<p>Implementation Checklist: - [x] All plugins inherit from BasePlugin ABC - [x] All plugins call <code>super().__init__(security_level=..., allow_downgrade=...)</code> - [x] No plugins override <code>validate_can_operate_at_level()</code> (sealed) - [x] All plugin types registered in PLUGIN_TYPE_REGISTRY - [x] Registry completeness test passes in CI</p> <p>Audit Evidence: - ABC definition: <code>src/elspeth/core/base/plugin.py</code> - Registry: <code>src/elspeth/core/base/plugin_types.py</code> - Test coverage: <code>tests/test_plugin_registry.py::test_plugin_registry_complete</code></p>"},{"location":"architecture/security-policy/#policy-5-frozen-plugin-capability","title":"Policy 5: Frozen Plugin Capability","text":"<p>Source: ADR-005 Status: Mandatory (Explicit Choice Required) Enforcement: Compile-time (no default parameter) + Runtime (validation) + Test (CI)</p>"},{"location":"architecture/security-policy/#51-trusted-downgrade-vs-frozen-plugins","title":"5.1 Trusted Downgrade vs Frozen Plugins","text":"<p>Elspeth supports two security postures via the <code>allow_downgrade</code> parameter:</p> Posture <code>allow_downgrade</code> Behavior Use Cases Trusted Downgrade <code>True</code> Plugin can operate at SAME or LOWER levels General-purpose plugins, cloud environments, mixed-classification workflows Frozen Plugin <code>False</code> Plugin can ONLY operate at EXACT declared level Dedicated classification domains, regulatory compliance, air-gapped networks <p>\u26a0\ufe0f BREAKING CHANGE: <code>allow_downgrade</code> has NO default value. All plugins MUST explicitly declare their security posture.</p>"},{"location":"architecture/security-policy/#52-trusted-downgrade-pattern-allow_downgradetrue","title":"5.2 Trusted Downgrade Pattern (allow_downgrade=True)","text":"<p>Scenario: Cloud-based datasource handles data at multiple classification levels</p> <pre><code>class AzureBlobDataSource(BasePlugin, DataSource):\n    def __init__(self, *, security_level: SecurityLevel = SecurityLevel.SECRET):\n        # Explicit allow_downgrade=True \u2192 trusted downgrade enabled\n        super().__init__(security_level=security_level, allow_downgrade=True)\n\n    def load_data(self, context: PluginContext) -&gt; ClassifiedDataFrame:\n        # Can operate at OFFICIAL, UNOFFICIAL if pipeline requires\n        # Responsible for filtering SECRET-tagged blobs appropriately\n        operating_level = context.security_level\n\n        if operating_level &lt; SecurityLevel.SECRET:\n            # Filter blobs to only include data at operating_level or below\n            filtered_data = self._filter_by_classification(operating_level)\n        else:\n            # Load all data (including SECRET)\n            filtered_data = self._load_all_data()\n\n        return ClassifiedDataFrame.create_from_datasource(\n            filtered_data,\n            operating_level\n        )\n</code></pre> <p>Validation: - \u2705 Operates at SECRET level: <code>validate_can_operate_at_level(SECRET)</code> \u2192 PASS - \u2705 Operates at OFFICIAL level: <code>validate_can_operate_at_level(OFFICIAL)</code> \u2192 PASS (trusted downgrade) - \u2705 Operates at UNOFFICIAL level: <code>validate_can_operate_at_level(UNOFFICIAL)</code> \u2192 PASS (trusted downgrade) - \u274c Operates at higher level: Not possible (SECRET is highest)</p>"},{"location":"architecture/security-policy/#53-frozen-plugin-pattern-allow_downgradefalse","title":"5.3 Frozen Plugin Pattern (allow_downgrade=False)","text":"<p>Scenario: Dedicated SECRET-only infrastructure (air-gapped, regulatory compliance)</p> <pre><code>class DedicatedSecretDataSource(BasePlugin, DataSource):\n    def __init__(self):\n        # Explicit allow_downgrade=False \u2192 frozen at SECRET level\n        super().__init__(\n            security_level=SecurityLevel.SECRET,\n            allow_downgrade=False\n        )\n\n    def load_data(self, context: PluginContext) -&gt; ClassifiedDataFrame:\n        # Will ONLY operate in SECRET pipelines\n        # Pipeline construction fails if configured with lower-clearance components\n        return ClassifiedDataFrame.create_from_datasource(\n            self._load_secret_data(),\n            SecurityLevel.SECRET\n        )\n</code></pre> <p>Validation: - \u2705 Operates at SECRET level: <code>validate_can_operate_at_level(SECRET)</code> \u2192 PASS (exact match) - \u274c Operates at OFFICIAL level: <code>validate_can_operate_at_level(OFFICIAL)</code> \u2192 FAIL (frozen, cannot downgrade) - \u274c Operates at UNOFFICIAL level: <code>validate_can_operate_at_level(UNOFFICIAL)</code> \u2192 FAIL (frozen, cannot downgrade)</p> <p>Error Message: <pre><code>SecurityValidationError: DedicatedSecretDataSource is frozen at SECRET\n(allow_downgrade=False). Cannot operate at lower level OFFICIAL.\nThis plugin requires exact level matching and does not support trusted downgrade.\n</code></pre></p>"},{"location":"architecture/security-policy/#54-compliance-requirements","title":"5.4 Compliance Requirements","text":"<p>Implementation Checklist: - [x] All plugins explicitly set <code>allow_downgrade=True</code> or <code>allow_downgrade=False</code> - [x] No plugins rely on default value (TypeError if omitted) - [x] Frozen plugins documented in deployment guide - [x] Infrastructure supports exact level matching for frozen plugins</p> <p>Audit Evidence: - Validation logic: <code>src/elspeth/core/base/plugin.py:validate_can_operate_at_level()</code> - Test coverage: <code>tests/test_baseplugin_frozen.py</code> - Configuration examples: <code>docs/user-guide/configuration.md</code></p>"},{"location":"architecture/security-policy/#policy-6-security-critical-exception-policy","title":"Policy 6: Security-Critical Exception Policy","text":"<p>Source: ADR-006 Status: Mandatory Enforcement: Static Analysis (Ruff, MyPy) + Pre-Commit (AST parsing) + CI (grep-based) + Code Review</p>"},{"location":"architecture/security-policy/#61-dual-exception-model","title":"6.1 Dual-Exception Model","text":"<p>Elspeth distinguishes between expected security boundaries and impossible security invariant violations:</p> <pre><code>Exception\n\u251c\u2500\u2500 SecurityValidationError  \u2705 Catchable - Expected boundaries\n\u2514\u2500\u2500 SecurityCriticalError    \ud83d\udea8 Policy-forbidden - Invariant violations\n</code></pre> Exception Purpose Catchability Use Cases SecurityValidationError Expected security boundaries \u2705 May be caught in production Start-time validation failures, clearance mismatches, permission denied SecurityCriticalError Impossible invariant violations \u274c MUST NOT be caught in production (tests only)* Classification downgrades, metadata tampering, container boundary violations <p>* Allowable scope: Only unit/integration tests (under <code>tests/</code>) or generated scaffolding specifically tagged for auditing may catch this exception. All first-party production modules (<code>src/</code>) are linted to reject catches, and glue code (e.g., orchestration notebooks, Airflow DAGs) must either live under <code>tests/</code> or opt into the same lint rule set to guarantee enforcement.</p>"},{"location":"architecture/security-policy/#62-securityvalidationerror-expected-boundaries","title":"6.2 SecurityValidationError (Expected Boundaries)","text":"<p>Use for expected security validation failures where graceful error handling is appropriate:</p> <pre><code># Expected validation failure - user misconfigured pipeline\ndef _validate_component_clearances(self, operating_level: SecurityLevel) -&gt; None:\n    \"\"\"Validate all components can operate at computed level.\"\"\"\n    try:\n        self.datasource.validate_can_operate_at_level(operating_level)\n    except Exception as e:\n        raise SecurityValidationError(  # \u2705 Expected, catchable\n            f\"ADR-002 Start-Time Validation Failed: Datasource cannot operate \"\n            f\"at {operating_level.name} level: {e}\"\n        ) from e\n</code></pre> <p>Production code MAY catch this: <pre><code>try:\n    suite.run()\nexcept SecurityValidationError as e:\n    logger.error(f\"Pipeline validation failed: {e}\")\n    notify_admin(f\"Invalid pipeline configuration: {e}\")\n    # Graceful degradation, notify user, etc.\n</code></pre></p>"},{"location":"architecture/security-policy/#63-securitycriticalerror-invariant-violations","title":"6.3 SecurityCriticalError (Invariant Violations)","text":"<p>Use for \"should never happen\" scenarios indicating bugs or attacks:</p> <pre><code>def with_uplifted_classification(self, new_level: SecurityLevel) -&gt; ClassifiedDataFrame:\n    \"\"\"Uplift classification (high water mark principle).\n\n    Raises:\n        SecurityCriticalError: If downgrade attempted (CRITICAL - platform terminates).\n    \"\"\"\n    if new_level &lt; self.classification:\n        # This should NEVER happen - indicates bug or attack\n        raise SecurityCriticalError(  # \ud83d\udea8 Platform terminates\n            f\"CRITICAL: Classification downgrade from {self.classification.name} \"\n            f\"to {new_level.name} violates high water mark invariant (ADR-002-A)\",\n            evidence={\n                \"current_level\": self.classification.name,\n                \"attempted_level\": new_level.name,\n                \"data_shape\": self.data.shape,\n            },\n            cve_id=\"CVE-ADR-002-A-004\",\n            classification_level=self.classification,\n        )\n\n    return ClassifiedDataFrame(self.data, new_level)\n</code></pre> <p>Production code MUST NOT catch this (policy-enforced): <pre><code># \u274c FORBIDDEN in production code (src/)\ntry:\n    result = classified.with_uplifted_classification(level)\nexcept SecurityCriticalError:  # \u274c Policy violation - blocked by CI\n    pass  # \u274c Execution continues with compromised state\n</code></pre></p> <p>Tests MAY catch this (verification): <pre><code># \u2705 ALLOWED in tests (tests/)\ndef test_classification_downgrade_raises_critical_error():\n    \"\"\"Verify downgrade attempts raise SecurityCriticalError.\"\"\"\n    classified = ClassifiedDataFrame.create_from_datasource(df, SecurityLevel.SECRET)\n\n    with pytest.raises(SecurityCriticalError) as exc_info:\n        classified.with_uplifted_classification(SecurityLevel.UNOFFICIAL)  # Downgrade!\n\n    assert \"downgrade\" in str(exc_info.value).lower()\n    assert exc_info.value.cve_id == \"CVE-ADR-002-A-004\"\n</code></pre></p>"},{"location":"architecture/security-policy/#64-emergency-logging","title":"6.4 Emergency Logging","text":"<p><code>SecurityCriticalError</code> automatically logs to multiple channels BEFORE propagating:</p> <pre><code>class SecurityCriticalError(Exception):\n    def __init__(self, message: str, *, evidence: dict, cve_id: str, ...):\n        super().__init__(message)\n        self.evidence = evidence\n        self.cve_id = cve_id\n\n        # Emergency logging BEFORE exception propagates\n        self._log_critical_security_event(message, evidence, cve_id)\n\n    def _log_critical_security_event(self, ...):\n        \"\"\"Log to multiple channels for redundancy.\"\"\"\n        # 1. stderr - always visible (container logs)\n        print(f\"\ud83d\udea8 CRITICAL SECURITY ERROR - PLATFORM TERMINATING \ud83d\udea8\", file=sys.stderr)\n        print(json.dumps(event, indent=2), file=sys.stderr)\n\n        # 2. Audit logger (structured JSON for SIEM)\n        logger.critical(json.dumps(event))\n\n        # 3. Security event stream (Azure Monitor, Splunk, etc.)\n        # ...\n</code></pre> <p>Audit Event Structure: <pre><code>{\n  \"timestamp\": \"2025-10-26T10:30:15.123456+00:00\",\n  \"severity\": \"CRITICAL\",\n  \"event_type\": \"SECURITY_CRITICAL_ERROR\",\n  \"event_class\": \"INVARIANT_VIOLATION\",\n  \"cve_id\": \"CVE-ADR-002-A-004\",\n  \"classification_level\": \"SECRET\",\n  \"message\": \"CRITICAL: Classification downgrade from SECRET to UNOFFICIAL...\",\n  \"evidence\": {\n    \"current_level\": \"SECRET\",\n    \"attempted_level\": \"UNOFFICIAL\",\n    \"data_shape\": [100, 5]\n  },\n  \"process_id\": 12345,\n  \"traceback\": \"...\"\n}\n</code></pre></p>"},{"location":"architecture/security-policy/#65-policy-enforcement-layers","title":"6.5 Policy Enforcement Layers","text":"<p>Layer 1: Ruff Linting (Fast Feedback)</p> <pre><code># pyproject.toml\n[tool.ruff.lint]\nextend-select = [\"TRY\"]  # Exception handling best practices\n\n[tool.ruff.lint.per-file-ignores]\n\"tests/**\" = [\"TRY302\"]  # Allow catching SecurityCriticalError in tests\n</code></pre> <p>Layer 2: Pre-Commit Hook (Prevents Commits)</p> <pre><code># .pre-commit-config.yaml runs AST-based script\npython scripts/check_security_exception_policy.py src/**/*.py\n</code></pre> <p>Script detects: - Direct catches: <code>except SecurityCriticalError:</code> - Aliased catches: <code>except SCE:</code> (where SCE is imported) - Tuple catches: <code>except (ValueError, SecurityCriticalError):</code> - Warns on broad catches: <code>except Exception:</code>, <code>except:</code></p> <p>Layer 3: CI/CD (Blocks Merges)</p> <pre><code># .github/workflows/security-policy.yml\n- name: Check for forbidden SecurityCriticalError catches\n  run: |\n    VIOLATIONS=$(grep -rn \"except.*SecurityCriticalError\" src/ || true)\n    if [ -n \"$VIOLATIONS\" ]; then\n      echo \"\u274c POLICY VIOLATION: SecurityCriticalError caught in production!\"\n      exit 1\n    fi\n</code></pre> <p>Layer 4: Code Review (Human Verification)</p> <p>Pull request checklist: - [ ] No <code>SecurityCriticalError</code> catches in production code (src/) - [ ] Test code properly validates invariant violations - [ ] Security-critical paths have defensive checks</p>"},{"location":"architecture/security-policy/#66-compliance-requirements","title":"6.6 Compliance Requirements","text":"<p>Implementation Checklist: - [x] <code>SecurityCriticalError</code> defined with emergency logging - [x] Production code (src/) contains zero SecurityCriticalError catches - [x] Test code (tests/) validates invariant violations raise SecurityCriticalError - [x] Pre-commit hook blocks forbidden catches - [x] CI enforces policy on all branches</p> <p>Audit Evidence: - Exception definition: <code>src/elspeth/core/security/exceptions.py</code> - Policy script: <code>scripts/check_security_exception_policy.py</code> - CI workflow: <code>.github/workflows/security-policy.yml</code> - Test coverage: <code>tests/test_security_critical_exceptions.py</code></p>"},{"location":"architecture/security-policy/#policy-enforcement-summary","title":"Policy Enforcement Summary","text":""},{"location":"architecture/security-policy/#defense-in-depth-layers","title":"Defense-in-Depth Layers","text":"Layer Technology Enforcement Point Prevents Type System MyPy, ABC inheritance Compile-time (IDE, CI) Accidental protocol compliance, type errors Runtime Checks <code>isinstance()</code>, sealed methods Execution-time Bypass via duck typing, malicious overrides Registry PLUGIN_TYPE_REGISTRY Pipeline construction Missing plugin types in validation Testing pytest, assertions CI (merge gate) Registry incompleteness, missing test coverage Static Analysis Ruff, custom AST parsers Pre-commit, CI Policy violations, broad exception catches Code Review Human review, checklists PR approval Logic errors, subtle security issues"},{"location":"architecture/security-policy/#audit-trail-requirements","title":"Audit Trail Requirements","text":"<p>All security-relevant events MUST be logged to structured audit logs (<code>logs/run_*.jsonl</code>):</p> <p>Logged Events: 1. Security level computation for each pipeline 2. Component clearance validation results (pass/fail) 3. Classification uplift operations (<code>with_uplifted_classification</code> calls) 4. Security validation failures (SecurityValidationError) 5. CRITICAL: Security invariant violations (SecurityCriticalError - emergency logging) 6. Plugin instantiation with security parameters 7. Reproducibility bundle creation (ADR-014 - tamper-evident archives for audit)</p> <p>Audit Log Format: <pre><code>{\n  \"timestamp\": \"2025-10-26T10:30:15.123456+00:00\",\n  \"run_id\": \"abc123...\",\n  \"event_type\": \"SECURITY_VALIDATION\",\n  \"security_level\": \"OFFICIAL\",\n  \"component\": \"AzureBlobDataSource\",\n  \"validation_result\": \"PASS\",\n  \"details\": {}\n}\n</code></pre></p>"},{"location":"architecture/security-policy/#compliance-mapping","title":"Compliance Mapping","text":""},{"location":"architecture/security-policy/#regulatory-framework-alignment","title":"Regulatory Framework Alignment","text":"Framework Requirement Elspeth Control Evidence PSPF (Australia) Classification-based access control Bell-LaPadula MLS (Policy 1) ADR-002, suite_runner.py NIST SP 800-53 AC-3 Access Enforcement Mandatory plugin validation (Policy 3) ADR-003, plugin_types.py ISO 27001 A.9.4.1 Information access restriction Operating level computation (Policy 1.2) suite_runner.py:_compute_operating_level() Common Criteria FDP_IFC.1 Subset information flow control Immutable classification (Policy 2) ADR-002a, classified_data.py GDPR Article 32 Security of processing Defense-in-depth (All Policies) Full ADR suite (002-006) HIPAA \u00a7164.312(b) Audit controls Reproducibility bundles with tamper-evident signatures ADR-014, reproducibility_bundle.py PCI-DSS Requirement 10.2 Audit trail for security events JSONL audit logs + signed reproducibility archives ADR-014, logs/run_*.jsonl PSPF (Australia) Recordkeeping and audit Mandatory reproducibility bundle in production mode ADR-014, config/templates/production_suite.yaml"},{"location":"architecture/security-policy/#certification-checklist","title":"Certification Checklist","text":"<p>For security certification/accreditation, verify:</p> <p>Policy 1: MLS Enforcement - [ ] Operating level computed as min(all components) - [ ] Validation performed BEFORE data retrieval - [ ] All Bell-LaPadula rules enforced (no read up, no write down) - [ ] Test coverage includes all SecurityLevel combinations</p> <p>Policy 2: Trusted Container - [ ] ClassifiedDataFrame is frozen dataclass - [ ] Only datasources use <code>create_from_datasource()</code> - [ ] Code review confirms no classification laundering - [ ] Test coverage validates immutability</p> <p>Policy 3: Plugin Security - [ ] All plugins inherit BasePlugin ABC - [ ] Plugin registry complete (test passes) - [ ] Sealed methods cannot be overridden (@final) - [ ] MyPy strict mode enforced</p> <p>Policy 4: Frozen Plugins - [ ] All plugins explicitly declare <code>allow_downgrade</code> - [ ] Frozen plugins documented in deployment guide - [ ] Infrastructure supports exact level matching - [ ] Test coverage includes frozen scenarios</p> <p>Policy 5: Exception Policy - [ ] SecurityCriticalError defined with emergency logging - [ ] Production code has zero forbidden catches (CI enforced) - [ ] Pre-commit hook blocks policy violations - [ ] Test coverage validates invariant violations</p>"},{"location":"architecture/security-policy/#related-documentation","title":"Related Documentation","text":""},{"location":"architecture/security-policy/#architecture-decision-records-adrs","title":"Architecture Decision Records (ADRs)","text":"<ul> <li>ADR-001: Design Philosophy - Security-first principles, fail-closed approach</li> <li>ADR-002: Multi-Level Security - Bell-LaPadula MLS model (full text)</li> <li>ADR-002a: Trusted Container Model - ClassifiedDataFrame immutability</li> <li>ADR-002b: Immutable Security Policy Metadata - Author-owned policy, no config overrides</li> <li>ADR-003: Plugin Type Registry - Central registry for validation coverage</li> <li>ADR-004: Mandatory BasePlugin Inheritance - Security bones design</li> <li>ADR-005: Frozen Plugin Capability - Strict level enforcement</li> <li>ADR-006: Security-Critical Exception Policy - Fail-loud invariants</li> <li>ADR-014: Tamper-Evident Reproducibility Bundle - Audit trail and compliance</li> </ul>"},{"location":"architecture/security-policy/#user-guides","title":"User Guides","text":"<ul> <li>Security Model - Practical guide with worked examples</li> <li>Configuration - Security parameter reference</li> </ul>"},{"location":"architecture/security-policy/#development-guides","title":"Development Guides","text":"<ul> <li>Architecture Overview - System architecture with security components</li> <li>Execution Flow - Security checkpoint mapping</li> </ul>"},{"location":"architecture/security-policy/#compliance-documents-repository","title":"Compliance Documents (Repository)","text":"<ul> <li><code>docs/compliance/incident-response.md</code> - Security incident response procedures</li> <li><code>docs/compliance/CONTROL_INVENTORY.md</code> - Security control traceability</li> <li><code>docs/compliance/TRACEABILITY_MATRIX.md</code> - Requirement-to-control mapping</li> </ul>"},{"location":"architecture/security-policy/#document-metadata","title":"Document Metadata","text":"<p>Effective Date: 2025-10-26 Review Cycle: Quarterly Next Review: 2026-01-26 Document Owner: Security Team Approvers: Architecture Team, Security Team Classification: OFFICIAL</p> <p>Change History: - 2025-10-26: Initial consolidated policy (v1.0) - Combined ADRs 002, 002a, 003, 004, 005, 006 - 2025-10-26: Added Policy 3 (ADR-002b) - Immutable Security Policy Metadata (v1.1) - 2025-10-26: Added ADR-014 compliance mapping - Reproducibility bundles for audit (v1.2)</p> <p>\ud83d\udd12 This policy is MANDATORY for all Elspeth deployments handling classified data.</p> <p>For questions or clarifications, consult the ADR Catalogue or contact the Security Team.</p>"},{"location":"getting-started/first-experiment/","title":"First Experiment","text":"<p>Create an Elspeth experiment from scratch in 15-20 minutes.</p>"},{"location":"getting-started/first-experiment/#goal","title":"Goal","text":"<p>Build a complete experiment that: - Loads data from a CSV file - Processes through a mock LLM - Writes results to multiple formats - Runs successfully with proper security levels</p>"},{"location":"getting-started/first-experiment/#overview","title":"Overview","text":"<p>An Elspeth experiment consists of:</p> <ol> <li>Datasource - Where data comes from (CSV, database, etc.)</li> <li>Transforms - How data is processed (LLM, middleware, etc.)</li> <li>Sinks - Where results go (CSV, Excel, JSON, etc.)</li> <li>Configuration - YAML files tying it all together</li> </ol>"},{"location":"getting-started/first-experiment/#step-1-create-experiment-directory","title":"Step 1: Create Experiment Directory","text":"<pre><code># Create new experiment suite\nmkdir -p config/my_first_suite/experiments\nmkdir -p data\n</code></pre>"},{"location":"getting-started/first-experiment/#step-2-create-sample-data","title":"Step 2: Create Sample Data","text":"<p>Create <code>data/my_data.csv</code>:</p> <pre><code>id,text,category\n1,\"Analyze customer feedback\",\"support\"\n2,\"Generate product description\",\"marketing\"\n3,\"Summarize meeting notes\",\"operations\"\n</code></pre> <p>Explanation: - <code>id</code>: Unique identifier - <code>text</code>: Input for LLM processing - <code>category</code>: Metadata for grouping</p>"},{"location":"getting-started/first-experiment/#step-3-create-settings-file","title":"Step 3: Create Settings File","text":"<p>Create <code>config/my_first_suite/settings.yaml</code>:</p> <pre><code># Suite-level settings\nsuite:\n  name: my_first_suite\n  description: My first Elspeth experiment suite\n\n# Default security level (inherited by all components)\nsecurity:\n  default_level: UNOFFICIAL  # Lowest level for testing\n\n# Logging configuration\nlogging:\n  level: INFO\n  audit: true\n</code></pre> <p>Key settings: - <code>security.default_level</code>: Start with <code>UNOFFICIAL</code> for testing (no sensitive data) - <code>logging.audit</code>: Enables audit trail in <code>logs/run_*.jsonl</code></p>"},{"location":"getting-started/first-experiment/#step-4-create-experiment-configuration","title":"Step 4: Create Experiment Configuration","text":"<p>Create <code>config/my_first_suite/experiments/text_processing.yaml</code>:</p> <p>YAML Indentation</p> <p>YAML requires 2-space indentation (not tabs). Incorrect indentation will cause schema validation errors. Most editors can be configured to convert tabs to spaces automatically.</p> <pre><code>experiment:\n  name: text_processing\n  description: Process text through LLM\n\n  # Data source configuration\n  datasource:\n    type: csv_local\n    path: data/my_data.csv\n    security_level: UNOFFICIAL  # Match data sensitivity\n\n  # LLM transformation\n  llm:\n    type: mock  # Use mock for testing (no API keys needed)\n    response_template: \"Processed: {text}\"\n    security_level: UNOFFICIAL\n\n  # Output sinks\n  sinks:\n    - type: csv\n      path: text_processing_results.csv\n      security_level: UNOFFICIAL\n\n    - type: excel\n      path: text_processing_results.xlsx\n      security_level: UNOFFICIAL\n      include_timestamp: true\n\n    - type: json\n      path: text_processing_results.json\n      security_level: UNOFFICIAL\n      pretty: true\n</code></pre> <p>Breakdown:</p> <ul> <li>Datasource: <code>csv_local</code> reads from local CSV file</li> <li>LLM: <code>mock</code> simulates LLM responses (good for testing)</li> <li>Sinks: Multiple output formats (CSV, Excel, JSON)</li> <li>Security levels: All set to <code>UNOFFICIAL</code> (lowest sensitivity)</li> </ul>"},{"location":"getting-started/first-experiment/#step-5-validate-configuration","title":"Step 5: Validate Configuration","text":"<p>Before running, validate the configuration:</p> <pre><code>python -m elspeth.cli validate-schemas \\\n  --settings config/my_first_suite/settings.yaml \\\n  --suite-root config/my_first_suite\n</code></pre> <p>Expected output: <pre><code>\u2713 Settings schema valid\n\u2713 Experiment 'text_processing' schema valid\n\u2713 All datasource schemas valid\n\u2713 All LLM schemas valid\n\u2713 All sink schemas valid\n</code></pre></p> <p>If validation fails: - Check YAML syntax (indentation, quotes) - Verify file paths exist - Ensure security levels are consistent</p>"},{"location":"getting-started/first-experiment/#step-6-run-experiment","title":"Step 6: Run Experiment","text":"<pre><code>python -m elspeth.cli \\\n  --settings config/my_first_suite/settings.yaml \\\n  --suite-root config/my_first_suite \\\n  --reports-dir outputs/my_first_suite \\\n  --head 0\n</code></pre> <p>Flags: - <code>--settings</code>: Path to settings file - <code>--suite-root</code>: Directory containing experiments - <code>--reports-dir</code>: Where to write outputs - <code>--head 0</code>: Skip data preview (show all rows in output)</p> <p>Expected output: <pre><code>INFO: Loading experiment suite from config/my_first_suite\nINFO: Running experiment: text_processing\nINFO: Datasource: my_data.csv (3 rows, UNOFFICIAL)\nINFO: Transform: MockLLMClient (UNOFFICIAL)\nINFO: Pipeline operating level: UNOFFICIAL\nINFO: Writing to CSV sink: text_processing_results.csv\nINFO: Writing to Excel sink: text_processing_results.xlsx\nINFO: Writing to JSON sink: text_processing_results.json\n\u2713 Experiment complete: 3 rows processed\n\n=== Results ===\n| id | text                          | category   | llm_response                     |\n|----|-------------------------------|------------|----------------------------------|\n| 1  | Analyze customer feedback     | support    | Processed: Analyze customer...   |\n| 2  | Generate product description  | marketing  | Processed: Generate product...   |\n| 3  | Summarize meeting notes       | operations | Processed: Summarize meeting...  |\n</code></pre></p>"},{"location":"getting-started/first-experiment/#step-7-inspect-outputs","title":"Step 7: Inspect Outputs","text":"<p>Check the generated files:</p> <pre><code>ls outputs/my_first_suite/\n\n# Expected:\n# text_processing_results.csv\n# text_processing_results.xlsx\n# text_processing_results.json\n# manifest.json\n</code></pre> <p>View CSV: <pre><code>cat outputs/my_first_suite/text_processing_results.csv\n</code></pre></p> <p>View JSON (pretty-printed): <pre><code>cat outputs/my_first_suite/text_processing_results.json | jq\n</code></pre></p> <p>View manifest (experiment metadata): <pre><code>cat outputs/my_first_suite/manifest.json | jq\n</code></pre></p>"},{"location":"getting-started/first-experiment/#step-8-customize-the-experiment","title":"Step 8: Customize the Experiment","text":""},{"location":"getting-started/first-experiment/#add-more-sinks","title":"Add More Sinks","text":"<p>Edit <code>text_processing.yaml</code> and add:</p> <pre><code>sinks:\n  # ... existing sinks ...\n\n  - type: markdown\n    path: report.md\n    template: |\n      # Text Processing Results\n\n      **Processed {{ total_rows }} rows**\n\n      {% for row in rows %}\n      - **{{ row.category }}**: {{ row.llm_response }}\n      {% endfor %}\n</code></pre>"},{"location":"getting-started/first-experiment/#add-middleware","title":"Add Middleware","text":"<p>Insert between datasource and LLM:</p> <pre><code>llm:\n  type: mock\n  response_template: \"Processed: {text}\"\n\n  middleware:\n    - type: audit\n      log_inputs: true\n      log_outputs: true\n\n    - type: health_monitor\n      max_retries: 3\n</code></pre>"},{"location":"getting-started/first-experiment/#use-multiple-datasources","title":"Use Multiple Datasources","text":"<p>Create suite with multiple experiments:</p> <pre><code># experiments/experiment_1.yaml\nexperiment:\n  name: experiment_1\n  datasource:\n    type: csv_local\n    path: data/dataset_1.csv\n\n# experiments/experiment_2.yaml\nexperiment:\n  name: experiment_2\n  datasource:\n    type: csv_local\n    path: data/dataset_2.csv\n</code></pre>"},{"location":"getting-started/first-experiment/#understanding-security-levels","title":"Understanding Security Levels","text":"<p>Elspeth enforces Bell-LaPadula Multi-Level Security (MLS):</p> <pre><code>UNOFFICIAL \u2192 OFFICIAL \u2192 OFFICIAL_SENSITIVE \u2192 PROTECTED \u2192 SECRET\n(lowest)                                                  (highest)\n</code></pre> <p>Key rule: Components can only access data at their level or below.</p>"},{"location":"getting-started/first-experiment/#example-mismatched-levels","title":"Example: Mismatched Levels","text":"<p>This will FAIL: <pre><code>datasource:\n  security_level: UNOFFICIAL  # Low clearance\n\nllm:\n  security_level: UNOFFICIAL\n\nsinks:\n  - type: csv\n    security_level: SECRET  # High clearance - FAILS!\n</code></pre></p> <p>Error: <code>SecurityValidationError: Datasource has insufficient clearance (UNOFFICIAL) for pipeline level (SECRET)</code></p> <p>Why: Datasource can't \"uplift\" data from UNOFFICIAL to SECRET.</p> <p>This will SUCCEED: <pre><code>datasource:\n  security_level: SECRET  # High clearance\n\nsinks:\n  - type: csv\n    security_level: UNOFFICIAL  # Low clearance - OK!\n</code></pre></p> <p>Why: SECRET datasource can \"downgrade\" data to UNOFFICIAL (trusted to filter sensitive info).</p> <p>See Security Model for full details.</p>"},{"location":"getting-started/first-experiment/#common-issues","title":"Common Issues","text":""},{"location":"getting-started/first-experiment/#schema-validation-errors","title":"Schema Validation Errors","text":"<p>Error: <code>Schema validation failed: 'path' is required</code></p> <p>Solution: Check required fields in experiment config. All sinks need <code>type</code> and <code>path</code>.</p> <p>Learn more: Configuration Validation Guide</p>"},{"location":"getting-started/first-experiment/#file-not-found","title":"File Not Found","text":"<p>Error: <code>FileNotFoundError: data/my_data.csv</code></p> <p>Solution: Verify file path is correct and file exists: <pre><code>ls data/my_data.csv\n</code></pre></p>"},{"location":"getting-started/first-experiment/#security-validation-error","title":"Security Validation Error","text":"<p>Error: <code>SecurityValidationError: Insufficient clearance</code></p> <p>Solution: Ensure all components have same (or compatible) security levels. Start with <code>UNOFFICIAL</code> for everything.</p> <p>Learn more: Security Model Guide | Security Controls</p>"},{"location":"getting-started/first-experiment/#empty-output","title":"Empty Output","text":"<p>Problem: Experiment runs but output files are empty</p> <p>Solution: Check for: 1. Empty input CSV 2. LLM response template issues 3. Sink filters excluding all rows</p>"},{"location":"getting-started/first-experiment/#next-steps","title":"Next Steps","text":""},{"location":"getting-started/first-experiment/#add-real-llm","title":"Add Real LLM","text":"<p>Replace mock with Azure OpenAI:</p> <pre><code>llm:\n  type: azure_openai\n  endpoint: ${AZURE_OPENAI_ENDPOINT}\n  api_key: ${AZURE_OPENAI_KEY}\n  deployment_name: gpt-4\n  model_params:\n    temperature: 0.7\n    max_tokens: 500\n</code></pre>"},{"location":"getting-started/first-experiment/#add-baseline-comparison","title":"Add Baseline Comparison","text":"<p>Compare experiment results against a baseline:</p> <pre><code>experiment:\n  baseline:\n    experiment_name: previous_run\n    metrics:\n      - accuracy\n      - f1_score\n</code></pre>"},{"location":"getting-started/first-experiment/#add-signed-artifacts","title":"Add Signed Artifacts","text":"<p>Generate cryptographically signed bundles:</p> <pre><code>python -m elspeth.cli \\\n  --settings config/my_first_suite/settings.yaml \\\n  --suite-root config/my_first_suite \\\n  --artifacts-dir artifacts \\\n  --signed-bundle\n</code></pre>"},{"location":"getting-started/first-experiment/#experiment-structure-reference","title":"Experiment Structure Reference","text":"<pre><code>config/my_first_suite/\n\u251c\u2500\u2500 settings.yaml              # Suite-level settings\n\u251c\u2500\u2500 experiments/\n\u2502   \u251c\u2500\u2500 text_processing.yaml   # Experiment configuration\n\u2502   \u2514\u2500\u2500 another_experiment.yaml\n\u2514\u2500\u2500 prompt_packs/              # Optional: reusable prompts\n    \u2514\u2500\u2500 common_prompts.yaml\n\ndata/\n\u2514\u2500\u2500 my_data.csv                # Input data\n\noutputs/my_first_suite/        # Generated outputs\n\u251c\u2500\u2500 text_processing_results.csv\n\u251c\u2500\u2500 text_processing_results.xlsx\n\u251c\u2500\u2500 text_processing_results.json\n\u2514\u2500\u2500 manifest.json\n\nlogs/\n\u2514\u2500\u2500 run_&lt;timestamp&gt;.jsonl      # Audit logs\n</code></pre>"},{"location":"getting-started/first-experiment/#quick-reference","title":"Quick Reference","text":"<pre><code># Validate configuration\npython -m elspeth.cli validate-schemas \\\n  --settings config/my_first_suite/settings.yaml \\\n  --suite-root config/my_first_suite\n\n# Run experiment\npython -m elspeth.cli \\\n  --settings config/my_first_suite/settings.yaml \\\n  --suite-root config/my_first_suite \\\n  --reports-dir outputs/my_first_suite\n\n# Run with head preview (first 10 rows)\npython -m elspeth.cli \\\n  --settings config/my_first_suite/settings.yaml \\\n  --suite-root config/my_first_suite \\\n  --head 10\n\n# Run with signed artifacts\npython -m elspeth.cli \\\n  --settings config/my_first_suite/settings.yaml \\\n  --suite-root config/my_first_suite \\\n  --artifacts-dir artifacts \\\n  --signed-bundle\n</code></pre> <p>Congratulations!</p> <p>You've created and run a complete Elspeth experiment from scratch! You now understand:</p> <ul> <li>\u2705 Experiment structure (datasource \u2192 LLM \u2192 sinks)</li> <li>\u2705 Configuration files (YAML)</li> <li>\u2705 Security levels (MLS enforcement)</li> <li>\u2705 Output formats (CSV, Excel, JSON)</li> <li>\u2705 Validation and debugging</li> </ul> <p>Ready to dive deeper? Learn about Security Model or explore the Plugin Catalogue.</p>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>Get Elspeth up and running on your system.</p>"},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.12 or higher</li> <li>pip package manager</li> <li>Git (for cloning the repository)</li> <li>Virtual environment support (venv)</li> </ul>"},{"location":"getting-started/installation/#system-requirements","title":"System Requirements","text":"Requirement Minimum Recommended Python 3.12 3.12+ RAM 4 GB 8 GB Disk Space 500 MB 1 GB OS Linux, macOS, Windows Linux (tested on Ubuntu 22.04+)"},{"location":"getting-started/installation/#quick-install","title":"Quick Install","text":"<p>The fastest way to get started:</p> <pre><code># Clone the repository\ngit clone https://github.com/johnm-dta/elspeth.git\ncd elspeth\n\n# Run bootstrap (creates venv, installs dependencies, runs tests)\nmake bootstrap\n</code></pre> <p>That's it! The <code>bootstrap</code> command handles everything: - Creates <code>.venv</code> virtual environment - Installs dependencies from lockfiles (with hash verification) - Installs Elspeth in editable mode - Runs test suite to verify installation</p>"},{"location":"getting-started/installation/#step-by-step-install","title":"Step-by-Step Install","text":"<p>If you prefer manual control:</p>"},{"location":"getting-started/installation/#1-clone-repository","title":"1. Clone Repository","text":"<pre><code>git clone https://github.com/johnm-dta/elspeth.git\ncd elspeth\n</code></pre>"},{"location":"getting-started/installation/#2-create-virtual-environment","title":"2. Create Virtual Environment","text":"<pre><code>python3.12 -m venv .venv\nsource .venv/bin/activate  # Linux/macOS\n# .venv\\Scripts\\activate   # Windows\n</code></pre>"},{"location":"getting-started/installation/#3-install-dependencies","title":"3. Install Dependencies","text":"<p>CRITICAL: Always install from lockfiles with hash verification (AIS compliance):</p> <pre><code># Install development dependencies\npython -m pip install -r requirements-dev.lock --require-hashes\n\n# Install Elspeth in editable mode\npython -m pip install -e . --no-deps --no-index\n</code></pre> <p>Never Install Unpinned Packages</p> <p>Do NOT run <code>pip install -e .[dev]</code> directly. Always use lockfiles (<code>requirements*.lock</code>) with <code>--require-hashes</code> for security.</p>"},{"location":"getting-started/installation/#4-verify-installation","title":"4. Verify Installation","text":"<pre><code># Run test suite\npython -m pytest -m \"not slow\"\n\n# Check Elspeth CLI\npython -m elspeth.cli --help\n</code></pre> <p>Expected output: <pre><code>usage: python -m elspeth.cli [-h] [--settings SETTINGS] [--suite-root SUITE_ROOT] ...\n</code></pre></p>"},{"location":"getting-started/installation/#bootstrap-without-tests","title":"Bootstrap Without Tests","text":"<p>If you want to skip tests during installation (faster, but not recommended):</p> <pre><code>make bootstrap-no-test\n</code></pre>"},{"location":"getting-started/installation/#azure-ml-integration-optional","title":"Azure ML Integration (Optional)","text":"<p>For Azure ML workflows, use Azure-specific lockfiles:</p> <pre><code># Runtime only\npip install --require-hashes -r requirements-azure.lock\npip install -e . --no-deps\n\n# Development + Azure\npython -m pip install -r requirements-dev-azure.lock --require-hashes\npython -m pip install -e . --no-deps --no-index\n</code></pre> <p>Azure dependencies: - <code>azure-identity</code> - <code>azure-storage-blob</code> - <code>azure-search-documents</code></p>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#python-version-issues","title":"Python Version Issues","text":"<p>Error: <code>python: command not found</code> or wrong version</p> <p>Solution: <pre><code># Check Python version\npython3.12 --version\n\n# Use explicit version\npython3.12 -m venv .venv\n</code></pre></p>"},{"location":"getting-started/installation/#lockfile-hash-verification-fails","title":"Lockfile Hash Verification Fails","text":"<p>Error: <code>THESE PACKAGES DO NOT MATCH THE HASHES</code></p> <p>Solution: <pre><code># Verify lockfile integrity\nmake verify-locked\n\n# If corrupted, regenerate lockfiles (contact maintainers)\n</code></pre></p>"},{"location":"getting-started/installation/#permission-denied","title":"Permission Denied","text":"<p>Error: <code>Permission denied</code> when installing</p> <p>Solution: <pre><code># Ensure virtual environment is activated\nsource .venv/bin/activate\n\n# Never use sudo with pip in virtual environments\n</code></pre></p>"},{"location":"getting-started/installation/#tests-fail-after-installation","title":"Tests Fail After Installation","text":"<p>Error: Some tests failing during <code>make bootstrap</code></p> <p>Solution: <pre><code># Run specific test to identify issue\npython -m pytest tests/test_&lt;failing&gt;.py -v\n\n# Check for missing dependencies\nmake verify-locked\n</code></pre></p>"},{"location":"getting-started/installation/#environment-variables","title":"Environment Variables","text":"<p>Elspeth respects these environment variables:</p> Variable Purpose Default <code>ELSPETH_LOG_LEVEL</code> Logging verbosity <code>INFO</code> <code>ELSPETH_CONFIG_DIR</code> Configuration directory <code>./config</code> <code>ELSPETH_ARTIFACTS_DIR</code> Artifact output directory <code>./artifacts</code> <p>Example: <pre><code>export ELSPETH_LOG_LEVEL=DEBUG\npython -m elspeth.cli --settings config/sample_suite/settings.yaml\n</code></pre></p>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quickstart - Run your first experiment in 5 minutes</li> <li>First Experiment - Create an experiment from scratch</li> <li>Configuration - Understand configuration files</li> </ul>"},{"location":"getting-started/installation/#uninstalling","title":"Uninstalling","text":"<p>To remove Elspeth:</p> <pre><code># Deactivate virtual environment\ndeactivate\n\n# Remove virtual environment\nrm -rf .venv\n\n# Remove repository (if desired)\ncd .. &amp;&amp; rm -rf elspeth\n</code></pre> <p>Production Deployments</p> <p>For production use, see the repository's <code>docs/operations/</code> directory for containerized deployment options with signed artifacts.</p>"},{"location":"getting-started/quickstart/","title":"Quickstart","text":"<p>Run your first Elspeth experiment in 5 minutes.</p>"},{"location":"getting-started/quickstart/#goal","title":"Goal","text":"<p>By the end of this guide, you'll: - \u2705 Run a complete experiment suite - \u2705 See LLM processing in action (mock LLM, no API keys needed) - \u2705 Generate output artifacts (CSV, Excel, JSON) - \u2705 Understand the basic workflow</p>"},{"location":"getting-started/quickstart/#prerequisites","title":"Prerequisites","text":"<ul> <li>Elspeth installed (Installation Guide)</li> <li>Virtual environment activated</li> </ul>"},{"location":"getting-started/quickstart/#step-1-activate-environment","title":"Step 1: Activate Environment","text":"<pre><code>cd elspeth\nsource .venv/bin/activate\n</code></pre>"},{"location":"getting-started/quickstart/#step-2-run-sample-suite","title":"Step 2: Run Sample Suite","text":"<p>Elspeth includes a pre-configured sample suite with mock LLM (no external dependencies):</p> <pre><code>make sample-suite\n</code></pre> <p>What happens: 1. Loads sample data from CSV 2. Processes through mock LLM transform 3. Writes results to multiple output formats 4. Shows preview tables in terminal</p> <p>Expected output: <pre><code>INFO: Loading experiment suite from config/sample_suite\nINFO: Running experiment: basic_transform\nINFO: Datasource: sample_data.csv (10 rows)\nINFO: Transform: MockLLMClient\nINFO: Sink: CSV output\n\u2713 Experiment complete: outputs/sample_suite_reports/basic_transform.csv\n\n=== Preview: basic_transform ===\n| input_text          | llm_response             | confidence |\n|---------------------|--------------------------|------------|\n| Hello world         | MOCK: Hello world        | 0.95       |\n| Test data           | MOCK: Test data          | 0.92       |\n...\n</code></pre></p> <p>How to verify success:</p> <p>\u2705 Terminal shows \"\u2713 Experiment complete\" with no ERROR lines \u2705 Output files exist: <pre><code>ls outputs/sample_suite_reports/\n# Expected: basic_transform.csv, basic_transform.xlsx, basic_transform.json\n</code></pre></p> <p>\u2705 CSV has 10 data rows: <pre><code>wc -l outputs/sample_suite_reports/basic_transform.csv\n# Expected: 11 (10 data + 1 header)\n</code></pre></p>"},{"location":"getting-started/quickstart/#step-3-explore-outputs","title":"Step 3: Explore Outputs","text":"<p>Results are written to <code>outputs/sample_suite_reports/</code>:</p> <pre><code>ls outputs/sample_suite_reports/\n\n# Expected files:\n# basic_transform.csv\n# basic_transform.xlsx\n# basic_transform.json\n# manifest.json\n</code></pre> <p>View CSV output: <pre><code>cat outputs/sample_suite_reports/basic_transform.csv\n</code></pre></p> <p>View manifest (experiment metadata): <pre><code>cat outputs/sample_suite_reports/manifest.json | jq\n</code></pre></p>"},{"location":"getting-started/quickstart/#step-4-understand-what-happened","title":"Step 4: Understand What Happened","text":""},{"location":"getting-started/quickstart/#pipeline-flow","title":"Pipeline Flow","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  CSV Data    \u2502  \u2192   \u2502  Mock LLM     \u2502  \u2192   \u2502  Sinks   \u2502\n\u2502  (10 rows)   \u2502      \u2502  (Transform)  \u2502      \u2502  (3 files)\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"getting-started/quickstart/#configuration","title":"Configuration","text":"<p>The sample suite is defined in <code>config/sample_suite/</code>:</p> <pre><code># config/sample_suite/experiments/basic_transform.yaml\nexperiment:\n  name: basic_transform\n\n  datasource:\n    type: csv_local\n    path: data/sample.csv\n\n  llm:\n    type: mock\n    response_template: \"MOCK: {input_text}\"\n\n  sinks:\n    - type: csv\n      path: basic_transform.csv\n    - type: excel\n      path: basic_transform.xlsx\n    - type: json\n      path: basic_transform.json\n</code></pre>"},{"location":"getting-started/quickstart/#step-5-run-with-signed-artifacts","title":"Step 5: Run with Signed Artifacts","text":"<p>Generate cryptographically signed bundles:</p> <pre><code>make sample-suite-artifacts\n</code></pre> <p>Additional outputs: - <code>artifacts/sample_suite_&lt;timestamp&gt;.tar.gz</code> - Signed bundle - <code>artifacts/sample_suite_&lt;timestamp&gt;.tar.gz.sig</code> - HMAC signature - SBOM included in bundle</p>"},{"location":"getting-started/quickstart/#whats-next","title":"What's Next?","text":""},{"location":"getting-started/quickstart/#learn-more","title":"Learn More","text":"<ul> <li>First Experiment - Create your own experiment from scratch</li> <li>Security Model - Understand Bell-LaPadula MLS</li> <li>Configuration - Deep dive into config files</li> </ul>"},{"location":"getting-started/quickstart/#try-different-sinks","title":"Try Different Sinks","text":"<p>Edit <code>config/sample_suite/experiments/basic_transform.yaml</code>:</p> <pre><code>sinks:\n  - type: markdown\n    path: report.md\n  - type: visual_analytics\n    path: analysis.html\n</code></pre>"},{"location":"getting-started/quickstart/#use-real-llm","title":"Use Real LLM","text":"<p>Replace mock LLM with Azure OpenAI:</p> <pre><code>llm:\n  type: azure_openai\n  endpoint: https://your-endpoint.openai.azure.com\n  api_key: ${AZURE_OPENAI_KEY}  # From environment variable\n  deployment_name: gpt-4\n</code></pre>"},{"location":"getting-started/quickstart/#common-issues","title":"Common Issues","text":""},{"location":"getting-started/quickstart/#command-not-found","title":"Command Not Found","text":"<p>Error: <code>make: command not found</code></p> <p>Solution: Run CLI directly: <pre><code>python -m elspeth.cli \\\n  --settings config/sample_suite/settings.yaml \\\n  --suite-root config/sample_suite \\\n  --head 0\n</code></pre></p>"},{"location":"getting-started/quickstart/#no-module-named-elspeth","title":"No Module Named 'elspeth'","text":"<p>Error: <code>ModuleNotFoundError: No module named 'elspeth'</code></p> <p>Solution: Ensure virtual environment is activated and Elspeth is installed: <pre><code>source .venv/bin/activate\npython -m pip install -e . --no-deps --no-index\n</code></pre></p>"},{"location":"getting-started/quickstart/#permission-denied-on-outputs","title":"Permission Denied on Outputs","text":"<p>Error: Cannot write to <code>outputs/</code></p> <p>Solution: Create output directory: <pre><code>mkdir -p outputs/sample_suite_reports\n</code></pre></p>"},{"location":"getting-started/quickstart/#quick-reference","title":"Quick Reference","text":"<pre><code># Run sample suite (no external deps)\nmake sample-suite\n\n# Run with signed artifacts\nmake sample-suite-artifacts\n\n# Run specific experiment\npython -m elspeth.cli \\\n  --settings config/sample_suite/settings.yaml \\\n  --suite-root config/sample_suite\n\n# Validate configuration before running\npython -m elspeth.cli validate-schemas \\\n  --settings config/sample_suite/settings.yaml\n</code></pre> <p>Experiment Templates</p> <p>The sample suite is a great starting point. Copy <code>config/sample_suite/</code> to create your own experiment suites with real data and LLMs.</p> <p>You Did It!</p> <p>You've successfully run your first Elspeth experiment! The pipeline processed data through a mock LLM and generated multiple output formats. Ready to build something real? Continue to First Experiment.</p>"},{"location":"plugins/generated-catalogue/","title":"Plugin Catalogue (Auto-Generated)","text":"<p>Auto-Generated Documentation</p> <p>This file is automatically generated from source code. Do not edit manually.</p> <p>Generated from: <code>scripts/generate_plugin_docs.py</code> Last generated: Run <code>make docs-generate</code> to update</p>"},{"location":"plugins/generated-catalogue/#loading-data-datasources","title":"Loading Data (Datasources)","text":"<p>1 datasource plugins available for loading data into experiments.</p> Plugin Description Security Key Parameters <code>blob</code> Read CSV data from Azure Blob Storage using configured profiles. \u2705 Trusted Downgrade None"},{"location":"plugins/generated-catalogue/#processing-with-llms-transforms","title":"Processing with LLMs (Transforms)","text":"<p>4 transform plugins available for processing data.</p> Plugin Description Security Key Parameters <code>azure_openai</code> Thin wrapper around OpenAI's Azure client implementing LLMClientProtocol. \u2705 Trusted Downgrade None <code>mock</code> Deterministic mock client for tests and offline runs. \u2705 Trusted Downgrade None <code>openai_http</code> Minimal client for standard /v1/chat/completions endpoints. \u2705 Trusted Downgrade None <code>static</code> Return predefined content and metrics for every request. \u2705 Trusted Downgrade None"},{"location":"plugins/generated-catalogue/#llm-middleware","title":"LLM Middleware","text":"<p>6 middleware plugins for request/response processing.</p> Plugin Description Security <code>audit</code> Structured audit logger for LLM requests and responses. \u2705 Trusted Downgrade <code>azure_content_safety</code> Use Azure Content Safety service to screen prompts before submission. \u2705 Trusted Downgrade <code>classified_material</code> Detect and block classified material markings in prompts with advanced fuzzy matching. \u2705 Trusted Downgrade <code>health_monitor</code> Emit heartbeat logs summarising middleware activity. \u2705 Trusted Downgrade <code>pii_shield</code> Enhanced PII detection with blind review, checksum validation, and severity scoring. \u2705 Trusted Downgrade <code>prompt_shield</code> Basic middleware that masks or blocks unsafe prompts before sending to the LLM. \u2705 Trusted Downgrade"},{"location":"plugins/generated-catalogue/#saving-results-sinks","title":"Saving Results (Sinks)","text":"<p>11 sink plugins available for writing experiment outputs.</p> Plugin Description Security Key Parameters <code>analytics_report</code> Generate a JSON analytics report (and optional Markdown) summarizing results and failures. \u2705 Trusted Downgrade None <code>blob</code> Upload artifacts to Azure Blob Storage with optional path constraints. \u2705 Trusted Downgrade None <code>csv_file</code> Result sink that writes experiment results to CSV files with formula sanitization. \u2705 Trusted Downgrade None <code>embeddings_store</code> Persist embeddings into a vector store backend (pgvector/Azure Search). \u2705 Trusted Downgrade None <code>excel</code> Persist experiment payloads into a timestamped Excel workbook. \u2705 Trusted Downgrade None <code>file_copy</code> Copy a single input artifact to a destination path. \u2705 Trusted Downgrade None <code>local_bundle</code> Create a local folder bundle with payload, metadata, and artifacts. \u2753 Unknown None <code>repository</code> Common repository sink behavior (auth, retries, dry-run, error handling). \u2753 Unknown None <code>reproducibility_bundle</code> Create a tamper-evident reproducibility bundle with results, metadata, and code. \u2753 Unknown None <code>signed</code> Write a signed artifact bundle (e.g., SBOM + signature) to disk. \u2753 Unknown None <code>zip_bundle</code> Bundle results, manifest, and optional CSV into a compressed ZIP archive at the configured path. \u2705 Trusted Downgrade None"},{"location":"plugins/generated-catalogue/#row-plugins","title":"Row Plugins","text":"<p>1 row plugins available.</p> Plugin Description Security <code>score_extractor</code> Extract numeric scores from LLM responses. \u2705 Trusted Downgrade"},{"location":"plugins/generated-catalogue/#aggregators","title":"Aggregators","text":"<p>8 aggregators available.</p> Plugin Description Security <code>cost_summary</code> Aggregate cost and token usage metrics across all rows. \u2705 Trusted Downgrade <code>latency_summary</code> Aggregate latency metrics across all rows. \u2705 Trusted Downgrade <code>rationale_analysis</code> Analyze LLM rationales to understand scoring patterns and provide interpretability. \u2705 Trusted Downgrade <code>score_agreement</code> Assess agreement/reliability across criteria scores. \u2705 Trusted Downgrade <code>score_power</code> Estimate power and required sample size for mean comparisons. \u2705 Trusted Downgrade <code>score_recommendation</code> Generate a lightweight recommendation based on score statistics. \u2705 Trusted Downgrade <code>score_stats</code> Aggregate score statistics across all rows. \u2705 Trusted Downgrade <code>score_variant_ranking</code> Compute a simple composite ranking score for an experiment. \u2705 Trusted Downgrade"},{"location":"plugins/generated-catalogue/#baseline-comparison","title":"Baseline Comparison","text":"<p>12 baseline comparison available.</p> Plugin Description Security <code>category_effects</code> Analyze how categorical variables affect score distributions. \u2705 Trusted Downgrade <code>criteria_effects</code> Perform per-criterion statistical comparisons between baseline and variant. \u2705 Trusted Downgrade <code>outlier_detection</code> Identify rows with largest score disagreements between baseline and variant. \u2705 Trusted Downgrade <code>referee_alignment</code> Compare LLM scores against human referee/expert judgments. \u2705 Trusted Downgrade <code>score_assumptions</code> Report normality and variance diagnostics for baseline vs. variant scores. \u2705 Trusted Downgrade <code>score_bayesian</code> Estimate posterior probability that a variant beats the baseline. \u2705 Trusted Downgrade <code>score_cliffs_delta</code> Compute Cliff's delta effect size between baseline and variant. \u2705 Trusted Downgrade <code>score_delta</code> Compare score statistics between baseline and variant. \u2705 Trusted Downgrade <code>score_distribution</code> Assess distribution shifts between baseline and variant deployments. \u2705 Trusted Downgrade <code>score_flip_analysis</code> Analyze score direction changes (flips) between baseline and variant. \u2705 Trusted Downgrade <code>score_practical</code> Assess practical significance (meaningful change, NNT, success deltas). \u2705 Trusted Downgrade <code>score_significance</code> Compare baseline and variant using effect sizes and t-tests. \u2705 Trusted Downgrade"},{"location":"plugins/generated-catalogue/#security-capability-legend","title":"Security Capability Legend","text":"Badge Meaning Description \u2705 Trusted Downgrade <code>allow_downgrade=True</code> Plugin can operate at lower security levels (trusted to filter appropriately) \ud83d\udd12 Frozen <code>allow_downgrade=False</code> Plugin requires exact security level matching (dedicated classification domains) \u2753 Unknown Not specified Security behavior not explicitly declared in source <p>See Security Policy for details on trusted downgrade vs frozen plugins.</p>"},{"location":"plugins/overview/","title":"Plugin Catalogue","text":"<p>Discover Elspeth's built-in plugins organized by what you want to accomplish.</p> <p>Quick Start</p> <p>For your first experiment, use: <code>csv_local</code> datasource \u2192 <code>mock</code> LLM \u2192 <code>csv</code> sink. Once working, swap <code>mock</code> for a real LLM like <code>azure_openai</code>.</p>"},{"location":"plugins/overview/#overview","title":"Overview","text":"<p>Elspeth plugins are modular components that handle specific tasks in your workflow pipeline:</p> <pre><code>Datasources \u2192 Transforms \u2192 Sinks\n     \u2193             \u2193          \u2193\n  Load Data    Process    Save Results\n              (LLM, ETL,\n            Analytics, etc)\n</code></pre> <p>All plugins declare a security level (clearance) and work within the Bell-LaPadula MLS framework. See Security Model for details.</p>"},{"location":"plugins/overview/#quick-navigation","title":"Quick Navigation","text":"<p>Jump to plugin type:</p> <ul> <li>Loading Data (Datasources) - CSV local/blob, Azure Blob</li> <li>Processing with LLMs - Azure OpenAI, generic HTTP API, mock</li> <li>LLM Middleware (Security &amp; Monitoring) - PII shield, audit logging, content safety, health monitoring</li> <li>Saving Results (Sinks) - CSV, Excel, JSON, Markdown, visual analytics, signed bundles</li> <li>Experiment Helpers - Row transforms, aggregators, baselines, early stopping</li> <li>RAG (Retrieval-Augmented Generation) - Azure AI Search, vector stores</li> <li>Cost &amp; Rate Limiting - Token tracking, budget enforcement, rate limiting</li> <li>Common Patterns - Reusable configuration examples</li> <li>Security Considerations - MLS enforcement, middleware stacking</li> <li>Plugin Development - Create custom plugins</li> </ul>"},{"location":"plugins/overview/#loading-data-datasources","title":"Loading Data (Datasources)","text":"<p>Choose how to load input data into your experiments.</p>"},{"location":"plugins/overview/#decision-guide","title":"Decision Guide","text":"<pre><code>Need local CSV file?           \u2192 csv_local\nNeed CSV from Azure Blob?      \u2192 csv_blob or azure_blob\nNeed database/API integration? \u2192 Coming soon (SQL, REST APIs)\n</code></pre>"},{"location":"plugins/overview/#available-datasources","title":"Available Datasources","text":"Plugin When to Use Key Configuration Example <code>csv_local</code> Local filesystem CSV files <code>path</code>, <code>encoding</code>, <code>dtype</code> Perfect for development and testing <code>csv_blob</code> CSV from Azure Blob Storage (direct URI) <code>path</code> (blob URI), <code>dtype</code> Production pipelines with blob URIs <code>azure_blob</code> CSV from Azure Blob with profile-based auth <code>config_path</code>, <code>profile</code> Managed identity or config-based auth"},{"location":"plugins/overview/#example-local-csv-datasource","title":"Example: Local CSV Datasource","text":"<pre><code>datasource:\n  type: csv_local\n  path: data/customer_feedback.csv\n  security_level: OFFICIAL\n  encoding: utf-8\n  on_error: abort  # abort | skip | log\n</code></pre>"},{"location":"plugins/overview/#example-azure-blob-datasource","title":"Example: Azure Blob Datasource","text":"<pre><code>datasource:\n  type: azure_blob\n  config_path: config/blob_profiles.yaml\n  profile: production\n  security_level: PROTECTED\n  pandas_kwargs:\n    dtype:\n      customer_id: str\n      rating: int\n</code></pre> <p>Common Options: - <code>on_error</code>: What to do if file can't be read (<code>abort</code>, <code>skip</code>, <code>log</code>) - <code>dtype</code>: Column type hints for Pandas (prevents parsing issues) - <code>encoding</code>: File encoding (default: <code>utf-8</code>)</p>"},{"location":"plugins/overview/#processing-with-llms","title":"Processing with LLMs","text":"<p>Choose how to send data to language models for processing.</p>"},{"location":"plugins/overview/#decision-guide_1","title":"Decision Guide","text":"<pre><code>Need Azure OpenAI?             \u2192 azure_openai\nNeed generic OpenAI API?       \u2192 http_openai\nTesting without API keys?      \u2192 mock\nNeed deterministic responses?  \u2192 static_test\n</code></pre>"},{"location":"plugins/overview/#available-llm-clients","title":"Available LLM Clients","text":"Plugin When to Use Key Configuration Notes <code>azure_openai</code> Azure-hosted OpenAI (GPT-4, GPT-3.5) <code>endpoint</code>, <code>api_key</code>, <code>deployment_name</code> Production LLM access <code>http_openai</code> Generic OpenAI HTTP API <code>api_base</code>, <code>api_key</code>, <code>model</code> OpenAI or compatible APIs <code>mock</code> Testing without real LLM <code>response_template</code>, <code>seed</code> No API keys needed <code>static_test</code> Canned responses for testing <code>content</code>, <code>score</code>, <code>metrics</code> Deterministic outputs"},{"location":"plugins/overview/#example-azure-openai","title":"Example: Azure OpenAI","text":"<pre><code>llm:\n  type: azure_openai\n  endpoint: https://my-openai.openai.azure.com\n  api_key: ${AZURE_OPENAI_KEY}  # From environment\n  deployment_name: gpt-4\n  security_level: OFFICIAL\n\n  model_params:\n    temperature: 0.7\n    max_tokens: 500\n    top_p: 0.9\n</code></pre>"},{"location":"plugins/overview/#example-mock-llm-testing","title":"Example: Mock LLM (Testing)","text":"<pre><code>llm:\n  type: mock\n  response_template: \"Mock response for: {text}\"\n  security_level: UNOFFICIAL\n  seed: 42  # Deterministic for tests\n</code></pre> <p>Common Model Parameters: - <code>temperature</code>: Randomness (0.0 = deterministic, 1.0 = creative) - <code>max_tokens</code>: Maximum response length - <code>top_p</code>: Nucleus sampling threshold - <code>frequency_penalty</code>: Reduce repetition (-2.0 to 2.0) - <code>presence_penalty</code>: Encourage topic diversity (-2.0 to 2.0)</p>"},{"location":"plugins/overview/#llm-middleware-security-monitoring","title":"LLM Middleware (Security &amp; Monitoring)","text":"<p>Add security filters, audit logging, and health monitoring to your LLM pipeline.</p>"},{"location":"plugins/overview/#decision-guide_2","title":"Decision Guide","text":"<pre><code>Need to log prompts/responses?         \u2192 audit_logger\nNeed to block banned terms?            \u2192 prompt_shield\nNeed to block PII (emails, SSNs)?      \u2192 pii_shield\nNeed to block classified markings?     \u2192 classified_material\nNeed Azure Content Safety?             \u2192 azure_content_safety\nNeed latency/health monitoring?        \u2192 health_monitor\n</code></pre>"},{"location":"plugins/overview/#security-middleware","title":"Security Middleware","text":"Plugin Purpose Key Configuration Default Behavior <code>pii_shield</code> Detect and block PII (emails, credit cards, SSNs, phone numbers, Australian TFN/Medicare/ABN, etc.) <code>on_violation</code> (abort/mask/log), <code>patterns</code> Blocks by default <code>classified_material</code> Detect classified markings (SECRET, TOP SECRET, TS//SCI, NOFORN, etc.) <code>on_violation</code>, <code>classification_markings</code> Blocks by default <code>prompt_shield</code> Block prompts containing banned terms <code>denied_terms</code>, <code>on_violation</code> (abort/mask/log) Aborts on match <code>azure_content_safety</code> Azure Content Safety API screening <code>endpoint</code>, <code>api_key</code>, <code>severity_threshold</code> Blocks harmful content"},{"location":"plugins/overview/#example-pii-shield","title":"Example: PII Shield","text":"<pre><code>llm:\n  type: azure_openai\n  # ... LLM config ...\n\n  middleware:\n    - type: pii_shield\n      on_violation: abort  # abort | mask | log\n      include_defaults: true  # Use built-in patterns\n      patterns:  # Add custom patterns\n        - name: employee_id\n          regex: 'EMP-\\\\d{6}'\n</code></pre> <p>Built-in PII Patterns: - Email addresses - Credit card numbers - US Social Security Numbers (SSN) - US phone numbers and passports - Australian TFN, ABN, ACN, Medicare numbers - Australian phone/mobile, passport, driver's licenses (NSW/VIC/QLD) - UK National Insurance numbers - IP addresses</p>"},{"location":"plugins/overview/#example-classified-material-filter","title":"Example: Classified Material Filter","text":"<pre><code>llm:\n  middleware:\n    - type: classified_material\n      on_violation: abort\n      include_defaults: true  # SECRET, TOP SECRET, TS//SCI, etc.\n      case_sensitive: false\n      classification_markings:\n        - CABINET IN CONFIDENCE\n        - PROTECTED//LEGAL PRIVILEGE\n</code></pre> <p>Built-in Classified Markings: - SECRET, TOP SECRET, CONFIDENTIAL - TS//SCI (Top Secret Sensitive Compartmented Information) - NOFORN (Not Releasable to Foreign Nationals) - FVEY (Five Eyes) - PROTECTED (Australian/UK classifications)</p>"},{"location":"plugins/overview/#monitoring-middleware","title":"Monitoring Middleware","text":"Plugin Purpose Key Configuration Use When <code>audit_logger</code> Structured logging of all LLM calls <code>include_prompts</code>, <code>channel</code> Always use in production <code>health_monitor</code> Track latency, errors, heartbeats <code>heartbeat_interval</code>, <code>stats_window</code> Production monitoring"},{"location":"plugins/overview/#example-audit-logger","title":"Example: Audit Logger","text":"<pre><code>llm:\n  middleware:\n    - type: audit_logger\n      include_prompts: true   # Log full prompts (be mindful of PII)\n      channel: llm_requests   # Log channel name\n</code></pre>"},{"location":"plugins/overview/#example-health-monitor","title":"Example: Health Monitor","text":"<pre><code>llm:\n  middleware:\n    - type: health_monitor\n      heartbeat_interval: 60  # Seconds\n      stats_window: 300       # 5-minute rolling window\n      include_latency: true\n</code></pre> <p>Middleware Execution Order:</p> <p>Middleware runs in the order declared. Best practice:</p> <pre><code>llm:\n  middleware:\n    - type: pii_shield           # 1. Block PII first\n    - type: classified_material  # 2. Block classified markings\n    - type: prompt_shield        # 3. Block banned terms\n    - type: audit_logger         # 4. Log sanitized prompts\n    - type: health_monitor       # 5. Track performance\n    - type: azure_content_safety # 6. Final content check\n</code></pre>"},{"location":"plugins/overview/#saving-results-sinks","title":"Saving Results (Sinks)","text":"<p>Choose how to save experiment outputs.</p>"},{"location":"plugins/overview/#decision-guide_3","title":"Decision Guide","text":"<pre><code>Need simple CSV?                       \u2192 csv\nNeed Excel workbook?                   \u2192 excel_workbook\nNeed JSON?                             \u2192 local_bundle (with JSON)\nNeed charts/visualizations?            \u2192 analytics_visual or enhanced_visual\nNeed structured analytics report?      \u2192 analytics_report\nNeed Azure Blob upload?                \u2192 azure_blob\nNeed Git repository commit?            \u2192 github_repo or azure_devops_repo\nNeed signed artifacts?                 \u2192 signed_artifact\nNeed compressed bundle?                \u2192 zip_bundle\n</code></pre>"},{"location":"plugins/overview/#available-sinks","title":"Available Sinks","text":"Plugin When to Use Key Configuration Output <code>csv</code> Simple CSV export <code>path</code>, <code>sanitize_formulas</code> Single CSV file <code>excel_workbook</code> Excel with multiple sheets <code>base_path</code>, <code>timestamped</code> .xlsx file <code>local_bundle</code> JSON + CSV bundle <code>base_path</code>, <code>write_json</code>, <code>write_csv</code> Directory with files <code>analytics_report</code> Structured analytics (JSON/Markdown) <code>formats</code> (json/markdown), <code>include_manifest</code> Report files <code>analytics_visual</code> Charts (PNG/HTML) <code>formats</code>, <code>dpi</code>, <code>figure_size</code> Visualization files <code>enhanced_visual</code> Advanced charts (violin, heatmap, forest plots) <code>chart_types</code>, <code>color_palette</code> Advanced visualizations <code>azure_blob</code> Azure Blob Storage upload <code>config_path</code>, <code>profile</code>, <code>path_template</code> Blob in Azure <code>github_repo</code> Commit to GitHub repo <code>owner</code>, <code>repo</code>, <code>branch</code>, <code>token_env</code> Git commit <code>azure_devops_repo</code> Commit to Azure DevOps <code>organization</code>, <code>project</code>, <code>repository</code> Git commit <code>signed_artifact</code> Cryptographically signed bundle <code>algorithm</code> (HMAC/RSA/ECDSA), <code>key</code> Signed tarball + signature <code>zip_bundle</code> Compressed archive <code>bundle_name</code>, <code>include_manifest</code> .zip file"},{"location":"plugins/overview/#example-csv-sink","title":"Example: CSV Sink","text":"<pre><code>sinks:\n  - type: csv\n    path: results/experiment_output.csv\n    security_level: OFFICIAL\n    sanitize_formulas: true  # Remove Excel formulas for security\n    overwrite: true\n</code></pre>"},{"location":"plugins/overview/#example-excel-workbook","title":"Example: Excel Workbook","text":"<pre><code>sinks:\n  - type: excel_workbook\n    base_path: results/experiment\n    security_level: OFFICIAL\n    timestamped: true  # Adds timestamp to filename\n    include_manifest: true  # Adds metadata sheet\n    sanitize_formulas: true\n</code></pre>"},{"location":"plugins/overview/#example-multiple-sinks","title":"Example: Multiple Sinks","text":"<pre><code>sinks:\n  # CSV for data analysis\n  - type: csv\n    path: results/data.csv\n    security_level: OFFICIAL\n\n  # Excel for stakeholder reports\n  - type: excel_workbook\n    base_path: results/report\n    security_level: OFFICIAL\n    timestamped: true\n\n  # Visualizations for presentations\n  - type: analytics_visual\n    base_path: results/charts\n    security_level: OFFICIAL\n    formats: [png, html]\n    dpi: 300  # High-res for presentations\n</code></pre>"},{"location":"plugins/overview/#example-advanced-visualizations","title":"Example: Advanced Visualizations","text":"<pre><code>sinks:\n  - type: enhanced_visual\n    base_path: results/advanced_charts\n    security_level: OFFICIAL\n    chart_types:\n      - violin      # Distribution + box plot\n      - heatmap     # Correlation matrices\n      - forest      # Effect size visualizations\n      - box         # Traditional box plots\n      - distribution # Overlaid distributions\n    formats: [png, html]\n    color_palette: colorblind  # Accessible colors\n</code></pre>"},{"location":"plugins/overview/#example-signed-artifacts","title":"Example: Signed Artifacts","text":"<pre><code>sinks:\n  - type: signed_artifact\n    base_path: artifacts\n    bundle_name: experiment_results\n    security_level: PROTECTED\n    algorithm: HMAC-SHA256  # HMAC-SHA256 | HMAC-SHA512 | RSA-PSS-SHA256 | ECDSA-P256-SHA256\n    key_env: SIGNING_KEY    # Environment variable with key\n</code></pre> <p>Formula Sanitization:</p> <p>For security, CSV and Excel sinks automatically sanitize spreadsheet formulas by default: - <code>=SUM(A1:A10)</code> \u2192 <code>'=SUM(A1:A10)</code> (prefixed with <code>'</code>) - Prevents formula injection attacks - Disable with <code>sanitize_formulas: false</code> (not recommended)</p>"},{"location":"plugins/overview/#experiment-helpers","title":"Experiment Helpers","text":"<p>Plugins that enhance experiment behavior.</p>"},{"location":"plugins/overview/#validation-plugins","title":"Validation Plugins","text":"<p>Ensure LLM responses meet quality criteria.</p> Plugin Purpose Configuration Example Use <code>regex_match</code> Validate with regex <code>pattern</code>, <code>flags</code> Ensure specific format <code>json</code> Validate JSON structure <code>ensure_object</code> Check valid JSON <code>llm_guard</code> Use secondary LLM for validation <code>validator_llm</code> definition Complex guardrails"},{"location":"plugins/overview/#example-regex-validation","title":"Example: Regex Validation","text":"<pre><code>experiment:\n  validation:\n    - type: regex_match\n      pattern: '^[A-Z]{3}-\\\\d{6}$'  # Format: ABC-123456\n      flags: IGNORECASE\n</code></pre>"},{"location":"plugins/overview/#example-json-validation","title":"Example: JSON Validation","text":"<pre><code>experiment:\n  validation:\n    - type: json\n      ensure_object: true  # Must be object, not array\n</code></pre>"},{"location":"plugins/overview/#row-level-plugins","title":"Row-Level Plugins","text":"<p>Process each row during the experiment.</p> Plugin Purpose Configuration Use When <code>score_extractor</code> Extract numeric scores from responses <code>key</code>, <code>threshold</code> Pull ratings/scores from LLM output <code>retrieval_context</code> Add RAG (Retrieval-Augmented Generation) context <code>provider</code> (pgvector/azure_search), <code>top_k</code> Enrich prompts with knowledge base"},{"location":"plugins/overview/#example-score-extraction","title":"Example: Score Extraction","text":"<pre><code>experiment:\n  row_plugins:\n    - type: score_extractor\n      key: quality_score\n      threshold: 0.7\n      threshold_mode: min  # min | max\n      criteria:\n        - relevance\n        - accuracy\n        - clarity\n</code></pre>"},{"location":"plugins/overview/#aggregation-plugins","title":"Aggregation Plugins","text":"<p>Compute statistics after all rows are processed.</p> Plugin Purpose Key Metrics Use When <code>score_stats</code> Basic statistics Mean, median, std dev Simple score summaries <code>cost_summary</code> Cost and token usage Total cost, tokens Budget tracking <code>latency_summary</code> Latency metrics p50, p95, p99 Performance analysis <code>rationale_analysis</code> Analyze LLM reasoning Themes, keywords, confidence Interpretability research"},{"location":"plugins/overview/#example-cost-summary","title":"Example: Cost Summary","text":"<pre><code>experiment:\n  aggregators:\n    - type: cost_summary\n      on_error: log  # abort | skip | log\n</code></pre>"},{"location":"plugins/overview/#example-latency-summary","title":"Example: Latency Summary","text":"<pre><code>experiment:\n  aggregators:\n    - type: latency_summary\n      on_error: log\n</code></pre>"},{"location":"plugins/overview/#baseline-comparison-plugins","title":"Baseline Comparison Plugins","text":"<p>Compare experiments against baselines.</p> Plugin Purpose Statistical Method Use When <code>score_delta</code> Simple score difference Delta calculation Quick A/B comparison <code>score_significance</code> Statistical significance t-test, Mann-Whitney Validate improvements <code>score_cliffs_delta</code> Effect size Cliff's Delta Measure practical impact <code>score_bayes</code> Bayesian comparison Credible intervals Probabilistic comparison <code>score_distribution</code> Distribution comparison KS test, histograms Understand score distributions <code>referee_alignment</code> LLM vs. human expert alignment MAE, RMSE, correlation Validate LLM against human judges"},{"location":"plugins/overview/#example-statistical-significance","title":"Example: Statistical Significance","text":"<pre><code>experiment:\n  baseline:\n    experiment_name: previous_run\n    comparison_plugins:\n      - type: score_significance\n        criteria:\n          - relevance\n          - accuracy\n        alpha: 0.05  # 95% confidence\n        alternative: greater  # greater | less | two-sided\n</code></pre>"},{"location":"plugins/overview/#example-referee-alignment","title":"Example: Referee Alignment","text":"<pre><code>experiment:\n  baseline:\n    comparison_plugins:\n      - type: referee_alignment\n        referee_fields:\n          - expert_rating\n          - human_judgment\n        score_field: llm_score\n        criteria:\n          - relevance\n        min_samples: 30\n        value_mapping:  # Map text to numbers\n          excellent: 5\n          good: 4\n          fair: 3\n          poor: 2\n          bad: 1\n</code></pre>"},{"location":"plugins/overview/#early-stop-plugins","title":"Early Stop Plugins","text":"<p>Halt experiments based on conditions.</p> Plugin Purpose Configuration Example <code>threshold</code> Stop when metric crosses threshold <code>metric</code>, <code>threshold</code>, <code>comparison</code> Stop when accuracy &gt; 0.95"},{"location":"plugins/overview/#example-early-stop","title":"Example: Early Stop","text":"<pre><code>experiment:\n  early_stop:\n    - type: threshold\n      metric: accuracy\n      threshold: 0.95\n      comparison: greater  # greater | less | equal\n      min_rows: 100  # Require minimum sample\n      label: high_accuracy_reached\n</code></pre>"},{"location":"plugins/overview/#advanced-retrieval-augmented-generation-rag","title":"Advanced: Retrieval-Augmented Generation (RAG)","text":"<p>Enrich LLM prompts with context from vector databases.</p>"},{"location":"plugins/overview/#supported-vector-stores","title":"Supported Vector Stores","text":"Provider When to Use Configuration <code>pgvector</code> PostgreSQL with pgvector extension <code>dsn</code>, <code>table</code>, <code>top_k</code> <code>azure_search</code> Azure Cognitive Search <code>endpoint</code>, <code>index</code>, <code>api_key</code>"},{"location":"plugins/overview/#example-postgresql-vector-retrieval","title":"Example: PostgreSQL Vector Retrieval","text":"<pre><code>experiment:\n  row_plugins:\n    - type: retrieval_context\n      provider: pgvector\n      dsn: postgresql://user:pass@localhost/db\n      table: document_embeddings\n      embed_model: text-embedding-ada-002\n      query_field: user_question\n      top_k: 5\n      min_score: 0.7\n      inject_mode: metadata  # metadata | prompt\n</code></pre>"},{"location":"plugins/overview/#example-azure-search-retrieval","title":"Example: Azure Search Retrieval","text":"<pre><code>experiment:\n  row_plugins:\n    - type: retrieval_context\n      provider: azure_search\n      endpoint: https://my-search.search.windows.net\n      index: knowledge_base\n      api_key_env: AZURE_SEARCH_KEY\n      embed_model: text-embedding-ada-002\n      query_field: user_question\n      top_k: 3\n</code></pre> <p>Inject Modes: - <code>metadata</code>: Add retrieved context to row metadata (access via <code>metadata.retrieval_context</code>) - <code>prompt</code>: Inject directly into prompt template (use <code>{retrieval_context}</code> placeholder)</p>"},{"location":"plugins/overview/#cost-rate-limiting","title":"Cost &amp; Rate Limiting","text":"<p>Control API costs and request rates.</p>"},{"location":"plugins/overview/#rate-limiters","title":"Rate Limiters","text":"Plugin Strategy Configuration Use When <code>fixed_window</code> Fixed requests per time window <code>requests</code>, <code>per_seconds</code> Simple rate limits <code>adaptive</code> Token-aware adaptive throttling <code>requests_per_minute</code>, <code>tokens_per_minute</code> Token-based pricing <code>noop</code> No rate limiting None Development/testing"},{"location":"plugins/overview/#example-fixed-window-rate-limit","title":"Example: Fixed Window Rate Limit","text":"<pre><code>llm:\n  rate_limiter:\n    type: fixed_window\n    requests: 60\n    per_seconds: 60  # 60 requests per 60 seconds\n</code></pre>"},{"location":"plugins/overview/#example-adaptive-rate-limit","title":"Example: Adaptive Rate Limit","text":"<pre><code>llm:\n  rate_limiter:\n    type: adaptive\n    requests_per_minute: 50\n    tokens_per_minute: 100000\n    interval_seconds: 1  # Check every second\n</code></pre>"},{"location":"plugins/overview/#cost-trackers","title":"Cost Trackers","text":"Plugin Pricing Model Configuration Use When <code>fixed_price</code> Fixed per-token pricing <code>prompt_token_price</code>, <code>completion_token_price</code> OpenAI-style pricing <code>noop</code> No cost tracking None Development/testing"},{"location":"plugins/overview/#example-cost-tracking","title":"Example: Cost Tracking","text":"<pre><code>llm:\n  cost_tracker:\n    type: fixed_price\n    prompt_token_price: 0.0000015    # $0.0015 per 1K tokens\n    completion_token_price: 0.000002 # $0.002 per 1K tokens\n</code></pre> <p>View costs with the <code>cost_summary</code> aggregator:</p> <pre><code>experiment:\n  aggregators:\n    - type: cost_summary\n</code></pre>"},{"location":"plugins/overview/#common-patterns","title":"Common Patterns","text":""},{"location":"plugins/overview/#pattern-1-simple-csv-llm-csv","title":"Pattern 1: Simple CSV \u2192 LLM \u2192 CSV","text":"<pre><code>experiment:\n  datasource:\n    type: csv_local\n    path: data/input.csv\n    security_level: UNOFFICIAL\n\n  llm:\n    type: mock\n    response_template: \"Processed: {text}\"\n    security_level: UNOFFICIAL\n\n  sinks:\n    - type: csv\n      path: output.csv\n      security_level: UNOFFICIAL\n</code></pre>"},{"location":"plugins/overview/#pattern-2-production-with-security","title":"Pattern 2: Production with Security","text":"<pre><code>experiment:\n  datasource:\n    type: azure_blob\n    config_path: config/blob.yaml\n    profile: production\n    security_level: PROTECTED\n\n  llm:\n    type: azure_openai\n    endpoint: ${AZURE_OPENAI_ENDPOINT}\n    api_key: ${AZURE_OPENAI_KEY}\n    deployment_name: gpt-4\n    security_level: PROTECTED\n\n    middleware:\n      - type: pii_shield\n        on_violation: abort\n      - type: classified_material\n        on_violation: abort\n      - type: audit_logger\n        include_prompts: true\n\n  sinks:\n    - type: signed_artifact\n      base_path: artifacts\n      algorithm: HMAC-SHA256\n      key_env: SIGNING_KEY\n      security_level: PROTECTED\n</code></pre>"},{"location":"plugins/overview/#pattern-3-rag-with-baseline-comparison","title":"Pattern 3: RAG with Baseline Comparison","text":"<pre><code>experiment:\n  datasource:\n    type: csv_local\n    path: data/questions.csv\n    security_level: OFFICIAL\n\n  llm:\n    type: azure_openai\n    deployment_name: gpt-4\n    security_level: OFFICIAL\n\n  row_plugins:\n    - type: retrieval_context\n      provider: pgvector\n      dsn: ${DATABASE_URL}\n      top_k: 5\n\n  baseline:\n    experiment_name: without_rag\n    comparison_plugins:\n      - type: score_significance\n        criteria: [accuracy, relevance]\n\n  sinks:\n    - type: analytics_report\n      formats: [json, markdown]\n      security_level: OFFICIAL\n</code></pre>"},{"location":"plugins/overview/#security-considerations","title":"Security Considerations","text":""},{"location":"plugins/overview/#all-plugins-inherit-security-level","title":"All Plugins Inherit Security Level","text":"<p>Every plugin declares a <code>security_level</code> (clearance):</p> <pre><code>datasource:\n  type: csv_local\n  security_level: OFFICIAL  # \u2190 Clearance declaration\n\nllm:\n  security_level: PROTECTED  # \u2190 Can handle PROTECTED data\n\nsinks:\n  - type: csv\n    security_level: OFFICIAL  # \u2190 Output clearance\n</code></pre> <p>Pipeline operating level = MIN of all component security levels.</p> <p>See Security Model for complete explanation.</p>"},{"location":"plugins/overview/#formula-sanitization-csvexcel","title":"Formula Sanitization (CSV/Excel)","text":"<p>CSV and Excel sinks sanitize formulas by default: - Prefixes formulas with <code>'</code> to prevent execution - Prevents formula injection attacks - Disable with <code>sanitize_formulas: false</code> (not recommended)</p>"},{"location":"plugins/overview/#pii-and-classified-material","title":"PII and Classified Material","text":"<p>Use middleware to block sensitive content:</p> <pre><code>llm:\n  middleware:\n    - type: pii_shield\n      on_violation: abort\n    - type: classified_material\n      on_violation: abort\n</code></pre>"},{"location":"plugins/overview/#plugin-development","title":"Plugin Development","text":"<p>Want to create custom plugins? See API Reference.</p> <p>Built-in plugin interfaces: - Datasource: Implement <code>load_data()</code>, declare <code>security_level</code> - LLM Client: Implement <code>transform()</code>, <code>consumes()</code>, <code>produces()</code> - Sink: Implement <code>write()</code>, <code>consumes()</code>, <code>produces()</code> - Middleware: Implement <code>before_request()</code>, <code>after_response()</code></p>"},{"location":"plugins/overview/#further-reading","title":"Further Reading","text":"<ul> <li>Security Model - Understanding Bell-LaPadula MLS</li> <li>Configuration Guide - Deep dive into YAML configuration</li> <li>First Experiment - Build an experiment step-by-step</li> <li>Plugin Development - Create custom plugins</li> </ul> <p>Ready to Build?</p> <p>You now know how to choose the right plugins for your experiment! Start with the simple patterns above, then gradually add security middleware, validation, and advanced features as needed.</p>"},{"location":"user-guide/configuration/","title":"Configuration Guide","text":"<p>Master Elspeth's configuration system to build robust, secure experiment pipelines.</p> <p>Quick Start</p> <p>Start with First Experiment for a hands-on walkthrough. This guide provides the complete reference for configuration options.</p>"},{"location":"user-guide/configuration/#overview","title":"Overview","text":"<p>Elspeth experiments are configured using YAML files organized in a suite structure:</p> <pre><code>config/my_suite/\n\u251c\u2500\u2500 settings.yaml              # Suite-level settings\n\u251c\u2500\u2500 experiments/\n\u2502   \u251c\u2500\u2500 experiment_1.yaml      # Experiment configuration\n\u2502   \u2514\u2500\u2500 experiment_2.yaml\n\u2514\u2500\u2500 prompt_packs/              # Optional: reusable prompts\n    \u2514\u2500\u2500 common_prompts.yaml\n</code></pre> <p>Key principle: Configuration merges in a predictable order to let you define defaults once and override them per-experiment.</p>"},{"location":"user-guide/configuration/#configuration-levels","title":"Configuration Levels","text":"<p>Elspeth merges configuration from three levels:</p> <pre><code>1. Suite Defaults (settings.yaml)\n        \u2193\n2. Prompt Packs (optional, reusable templates)\n        \u2193\n3. Experiment Overrides (experiments/*.yaml)\n</code></pre> <p>Later levels override earlier levels. This lets you: - Define security middleware once in settings.yaml - Share prompts across experiments via prompt packs - Override specific options per experiment</p>"},{"location":"user-guide/configuration/#settings-file-settingsyaml","title":"Settings File (settings.yaml)","text":"<p>The suite-level settings file defines defaults inherited by all experiments.</p>"},{"location":"user-guide/configuration/#basic-structure","title":"Basic Structure","text":"<pre><code># config/my_suite/settings.yaml\n\nsuite:\n  name: my_experiment_suite\n  description: Production LLM evaluation suite\n\n# Default security level (inherited by all components)\nsecurity:\n  default_level: OFFICIAL  # UNOFFICIAL | OFFICIAL | OFFICIAL_SENSITIVE | PROTECTED | SECRET\n\n# Logging configuration\nlogging:\n  level: INFO         # DEBUG | INFO | WARNING | ERROR\n  audit: true         # Enable audit trail (logs/run_*.jsonl)\n  include_prompts: false  # Log full prompts (be mindful of PII)\n\n# Default LLM configuration (inherited by experiments)\nllm:\n  type: azure_openai\n  endpoint: ${AZURE_OPENAI_ENDPOINT}\n  api_key: ${AZURE_OPENAI_KEY}\n  deployment_name: gpt-4\n  security_level: OFFICIAL\n\n  model_params:\n    temperature: 0.7\n    max_tokens: 500\n\n  # Default middleware (inherited by all experiments)\n  middleware:\n    - type: pii_shield\n      on_violation: abort\n    - type: classified_material\n      on_violation: abort\n    - type: audit_logger\n      include_prompts: false\n\n  # Rate limiting\n  rate_limiter:\n    type: fixed_window\n    requests: 60\n    per_seconds: 60\n\n  # Cost tracking\n  cost_tracker:\n    type: fixed_price\n    prompt_token_price: 0.0000015\n    completion_token_price: 0.000002\n\n# Default sinks (inherited by all experiments)\nsinks:\n  - type: csv\n    path: \"{experiment_name}_results.csv\"\n    security_level: OFFICIAL\n    sanitize_formulas: true\n\n  - type: excel_workbook\n    base_path: \"reports/{experiment_name}\"\n    security_level: OFFICIAL\n    timestamped: true\n    include_manifest: true\n</code></pre>"},{"location":"user-guide/configuration/#configuration-sections","title":"Configuration Sections","text":"Section Purpose Required Common Options <code>suite</code> Suite metadata \u2705 Yes <code>name</code>, <code>description</code> <code>security</code> Default security level \u2705 Yes <code>default_level</code> <code>logging</code> Logging behavior \u26a0\ufe0f Recommended <code>level</code>, <code>audit</code>, <code>include_prompts</code> <code>llm</code> Default LLM client \u26a0\ufe0f If using LLMs <code>type</code>, <code>endpoint</code>, <code>api_key</code>, <code>middleware</code> <code>sinks</code> Default output sinks \u26a0\ufe0f Recommended <code>type</code>, <code>path</code>, <code>security_level</code> <code>concurrency</code> Parallel execution \u274c Optional <code>max_workers</code>, <code>backlog_threshold</code> <code>retry</code> Retry configuration \u274c Optional <code>max_attempts</code>, <code>initial_delay</code>, <code>backoff_multiplier</code>"},{"location":"user-guide/configuration/#experiment-configuration","title":"Experiment Configuration","text":"<p>Individual experiments inherit from settings.yaml and can override any option.</p>"},{"location":"user-guide/configuration/#basic-structure_1","title":"Basic Structure","text":"<pre><code># config/my_suite/experiments/sentiment_analysis.yaml\n\nexperiment:\n  name: sentiment_analysis\n  description: Analyze customer sentiment with GPT-4\n\n  # Datasource (required)\n  datasource:\n    type: csv_local\n    path: data/customer_feedback.csv\n    security_level: OFFICIAL\n    encoding: utf-8\n\n  # LLM configuration (inherits from settings.yaml)\n  llm:\n    # Override specific model params\n    model_params:\n      temperature: 0.3  # Lower temperature for consistency\n\n  # Sinks (inherits from settings.yaml)\n  sinks:\n    # Add experiment-specific sink\n    - type: analytics_visual\n      base_path: \"visualizations/sentiment\"\n      formats: [png, html]\n      security_level: OFFICIAL\n</code></pre>"},{"location":"user-guide/configuration/#full-example","title":"Full Example","text":"<pre><code>experiment:\n  name: product_categorization\n  description: Categorize product descriptions with LLM\n\n  # === DATASOURCE ===\n  datasource:\n    type: csv_local\n    path: data/products.csv\n    security_level: OFFICIAL\n    dtype:\n      product_id: str\n      description: str\n      category: str\n\n  # === LLM ===\n  llm:\n    # Inherit type/endpoint from settings.yaml\n    model_params:\n      temperature: 0.2\n      max_tokens: 100\n\n    # Add experiment-specific middleware\n    middleware:\n      - type: regex_match\n        pattern: '^(Electronics|Clothing|Food|Other)$'\n\n  # === EXPERIMENT HELPERS ===\n\n  # Row-level processing\n  row_plugins:\n    - type: score_extractor\n      key: confidence\n      threshold: 0.8\n\n  # Aggregation\n  aggregators:\n    - type: cost_summary\n    - type: latency_summary\n\n  # Validation\n  validation:\n    - type: json\n      ensure_object: true\n\n  # === BASELINE COMPARISON ===\n  baseline:\n    experiment_name: product_categorization_v1\n    comparison_plugins:\n      - type: score_significance\n        criteria: [accuracy]\n        alpha: 0.05\n\n  # === EARLY STOP ===\n  early_stop:\n    - type: threshold\n      metric: accuracy\n      threshold: 0.95\n      comparison: greater\n      min_rows: 100\n\n  # === SINKS ===\n  sinks:\n    # Override default sinks (inherit: false)\n    inherit: false\n\n    - type: csv\n      path: \"results/{experiment_name}_categorization.csv\"\n      security_level: OFFICIAL\n\n    - type: analytics_report\n      formats: [json, markdown]\n      include_manifest: true\n      security_level: OFFICIAL\n\n    - type: signed_artifact\n      base_path: artifacts\n      algorithm: HMAC-SHA256\n      key_env: SIGNING_KEY\n      security_level: OFFICIAL\n</code></pre>"},{"location":"user-guide/configuration/#configuration-merge-order","title":"Configuration Merge Order","text":"<p>Understanding merge order helps you avoid configuration surprises.</p>"},{"location":"user-guide/configuration/#merge-rules","title":"Merge Rules","text":"<pre><code>settings.yaml (suite defaults)\n    \u2193\n+ prompt_pack.yaml (if specified)\n    \u2193\n+ experiment.yaml (experiment-specific)\n    \u2193\n= Final Configuration\n</code></pre> <p>Merge behavior: - Simple values (strings, numbers): Later value overwrites earlier value - Lists (middleware, sinks): Later list appends to earlier list (unless <code>inherit: false</code>) - Nested objects (model_params): Deep merge (only specified keys overwrite)</p>"},{"location":"user-guide/configuration/#example-merge-process","title":"Example: Merge Process","text":"<p>settings.yaml: <pre><code>llm:\n  type: azure_openai\n  deployment_name: gpt-4\n  model_params:\n    temperature: 0.7\n    max_tokens: 500\n  middleware:\n    - type: pii_shield\n    - type: audit_logger\n</code></pre></p> <p>experiment.yaml: <pre><code>llm:\n  model_params:\n    temperature: 0.3  # Override temperature\n  middleware:\n    - type: regex_match  # Append to middleware list\n</code></pre></p> <p>Final merged configuration: <pre><code>llm:\n  type: azure_openai           # From settings.yaml\n  deployment_name: gpt-4       # From settings.yaml\n  model_params:\n    temperature: 0.3           # Overridden by experiment.yaml\n    max_tokens: 500            # From settings.yaml\n  middleware:\n    - type: pii_shield         # From settings.yaml\n    - type: audit_logger       # From settings.yaml\n    - type: regex_match        # Appended from experiment.yaml\n</code></pre></p>"},{"location":"user-guide/configuration/#opting-out-of-inheritance","title":"Opting Out of Inheritance","text":"<p>Use <code>inherit: false</code> to replace instead of append:</p> <pre><code>llm:\n  middleware:\n    inherit: false  # Ignore settings.yaml middleware\n    - type: audit_logger  # Only middleware for this experiment\n\nsinks:\n  inherit: false  # Ignore settings.yaml sinks\n  - type: csv\n    path: custom_output.csv\n</code></pre>"},{"location":"user-guide/configuration/#environment-variables","title":"Environment Variables","text":"<p>Elspeth supports environment variables for secrets and dynamic values.</p>"},{"location":"user-guide/configuration/#using-environment-variables","title":"Using Environment Variables","text":"<pre><code>llm:\n  type: azure_openai\n  endpoint: ${AZURE_OPENAI_ENDPOINT}  # Read from env var\n  api_key: ${AZURE_OPENAI_KEY}        # Never commit secrets to YAML\n\ndatasource:\n  type: azure_blob\n  connection_string: ${AZURE_STORAGE_CONNECTION_STRING}\n\nsinks:\n  - type: signed_artifact\n    key_env: SIGNING_KEY  # Plugin reads from env var\n</code></pre>"},{"location":"user-guide/configuration/#setting-environment-variables","title":"Setting Environment Variables","text":"<p>Development (.env file): <pre><code># .env (DO NOT COMMIT TO GIT)\nAZURE_OPENAI_ENDPOINT=https://my-openai.openai.azure.com\nAZURE_OPENAI_KEY=sk-...\nAZURE_STORAGE_CONNECTION_STRING=DefaultEndpointsProtocol=https;...\nSIGNING_KEY=my-secret-key\n</code></pre></p> <p>Production (CI/CD): <pre><code># GitHub Actions / Azure DevOps secrets\nexport AZURE_OPENAI_ENDPOINT=\"${{ secrets.AZURE_OPENAI_ENDPOINT }}\"\nexport AZURE_OPENAI_KEY=\"${{ secrets.AZURE_OPENAI_KEY }}\"\n</code></pre></p> <p>Local Development: <pre><code># Load .env file\nsource .env\n\n# Or inline\nAZURE_OPENAI_KEY=sk-... python -m elspeth.cli --settings config/my_suite/settings.yaml\n</code></pre></p> <p>Never Commit Secrets</p> <ul> <li>\u274c Never put API keys, passwords, or connection strings directly in YAML files</li> <li>\u2705 Always use environment variables (<code>${VAR_NAME}</code>)</li> <li>\u2705 Add <code>.env</code> to <code>.gitignore</code></li> <li>\u2705 Rotate secrets regularly</li> <li>\u2705 Use managed identities (Azure, AWS IAM) when possible</li> </ul>"},{"location":"user-guide/configuration/#prompt-packs-advanced","title":"Prompt Packs (Advanced)","text":"<p>Reusable prompt templates shared across experiments.</p>"},{"location":"user-guide/configuration/#creating-a-prompt-pack","title":"Creating a Prompt Pack","text":"<pre><code># config/my_suite/prompt_packs/sentiment_pack.yaml\n\nprompt_pack:\n  name: sentiment_analysis\n  description: Sentiment analysis prompts\n\n  prompts:\n    classify_sentiment: |\n      Analyze the sentiment of the following text.\n      Respond with: \"positive\", \"negative\", or \"neutral\"\n\n      Text: {text}\n\n      Sentiment:\n\n    extract_themes: |\n      Identify key themes in the following feedback.\n\n      Feedback: {feedback}\n\n      Themes (JSON array):\n\n  # Middleware shared across experiments\n  middleware:\n    - type: pii_shield\n      on_violation: mask\n    - type: audit_logger\n\n  # Default sinks for this pack\n  sinks:\n    - type: csv\n      path: \"{experiment_name}_sentiment.csv\"\n</code></pre>"},{"location":"user-guide/configuration/#using-a-prompt-pack","title":"Using a Prompt Pack","text":"<pre><code># experiment.yaml\nexperiment:\n  name: customer_sentiment\n  prompt_pack: sentiment_pack  # Reference by name\n\n  datasource:\n    type: csv_local\n    path: data/feedback.csv\n\n  # Prompts from pack are available in templates\n  llm:\n    prompt_template: \"{prompts.classify_sentiment}\"\n</code></pre> <p>Benefits: - Reuse prompts across experiments - Centralize middleware configuration - Standardize output formats</p>"},{"location":"user-guide/configuration/#concurrency-performance","title":"Concurrency &amp; Performance","text":"<p>Configure parallel execution and retry behavior.</p>"},{"location":"user-guide/configuration/#concurrency-configuration","title":"Concurrency Configuration","text":"<pre><code># settings.yaml\nconcurrency:\n  max_workers: 4              # Parallel threads\n  backlog_threshold: 100      # Queue size before blocking\n  timeout_seconds: 300        # Per-row timeout\n</code></pre>"},{"location":"user-guide/configuration/#retry-configuration","title":"Retry Configuration","text":"<pre><code>retry:\n  max_attempts: 3             # Retry failed rows 3 times\n  initial_delay: 1.0          # Start with 1-second delay\n  backoff_multiplier: 2.0     # Double delay each retry (1s \u2192 2s \u2192 4s)\n  retry_on_errors:\n    - RateLimitError\n    - TimeoutError\n</code></pre> <p>Example: Production Configuration</p> <pre><code>llm:\n  type: azure_openai\n  # ... LLM config ...\n\n  rate_limiter:\n    type: adaptive\n    requests_per_minute: 50\n    tokens_per_minute: 100000\n\nconcurrency:\n  max_workers: 8\n  backlog_threshold: 200\n  timeout_seconds: 120\n\nretry:\n  max_attempts: 5\n  initial_delay: 2.0\n  backoff_multiplier: 2.0\n</code></pre>"},{"location":"user-guide/configuration/#validation-schema-checking","title":"Validation &amp; Schema Checking","text":"<p>Validate configuration before running experiments.</p>"},{"location":"user-guide/configuration/#validate-configuration","title":"Validate Configuration","text":"<pre><code>python -m elspeth.cli validate-schemas \\\n  --settings config/my_suite/settings.yaml \\\n  --suite-root config/my_suite\n</code></pre> <p>Expected output: <pre><code>\u2713 Settings schema valid\n\u2713 Experiment 'sentiment_analysis' schema valid\n\u2713 Experiment 'product_categorization' schema valid\n\u2713 All datasource schemas valid\n\u2713 All LLM schemas valid\n\u2713 All sink schemas valid\n\u2713 All middleware schemas valid\n</code></pre></p>"},{"location":"user-guide/configuration/#common-validation-errors","title":"Common Validation Errors","text":""},{"location":"user-guide/configuration/#missing-required-field","title":"Missing Required Field","text":"<p>Error: <pre><code>Schema validation failed: 'path' is required\nSink: csv (line 45)\n</code></pre></p> <p>Solution: Add required field to sink configuration: <pre><code>sinks:\n  - type: csv\n    path: results.csv  # \u2190 Add missing field\n</code></pre></p>"},{"location":"user-guide/configuration/#invalid-plugin-type","title":"Invalid Plugin Type","text":"<p>Error: <pre><code>Unknown datasource type: 'csv_file'\nDid you mean: 'csv_local'?\n</code></pre></p> <p>Solution: Use correct plugin type from Plugin Catalogue.</p>"},{"location":"user-guide/configuration/#security-level-mismatch","title":"Security Level Mismatch","text":"<p>Error: <pre><code>SecurityValidationError: Datasource has insufficient clearance\nComponent: UNOFFICIAL\nRequired: OFFICIAL\n</code></pre></p> <p>Solution: Raise datasource security level to match pipeline: <pre><code>datasource:\n  security_level: OFFICIAL  # \u2190 Match pipeline level\n</code></pre></p> <p>See Security Model for complete explanation.</p>"},{"location":"user-guide/configuration/#configuration-patterns","title":"Configuration Patterns","text":""},{"location":"user-guide/configuration/#pattern-1-development-suite","title":"Pattern 1: Development Suite","text":"<p>Simple configuration for testing and development.</p> <pre><code># settings.yaml\nsuite:\n  name: dev_suite\n  description: Development experiments\n\nsecurity:\n  default_level: UNOFFICIAL  # Lowest level for testing\n\nlogging:\n  level: DEBUG\n  audit: true\n\nllm:\n  type: mock\n  response_template: \"Mock: {text}\"\n  security_level: UNOFFICIAL\n\nsinks:\n  - type: csv\n    path: \"{experiment_name}.csv\"\n    security_level: UNOFFICIAL\n</code></pre>"},{"location":"user-guide/configuration/#pattern-2-production-suite","title":"Pattern 2: Production Suite","text":"<p>Secure configuration with middleware and auditing.</p> <pre><code># settings.yaml\nsuite:\n  name: production_suite\n  description: Production LLM evaluation\n\nsecurity:\n  default_level: OFFICIAL\n\nlogging:\n  level: INFO\n  audit: true\n  include_prompts: false  # Don't log prompts (PII risk)\n\nllm:\n  type: azure_openai\n  endpoint: ${AZURE_OPENAI_ENDPOINT}\n  api_key: ${AZURE_OPENAI_KEY}\n  deployment_name: gpt-4\n  security_level: OFFICIAL\n\n  middleware:\n    - type: pii_shield\n      on_violation: abort\n    - type: classified_material\n      on_violation: abort\n    - type: azure_content_safety\n      endpoint: ${AZURE_CONTENT_SAFETY_ENDPOINT}\n      api_key: ${AZURE_CONTENT_SAFETY_KEY}\n      severity_threshold: medium\n    - type: audit_logger\n      include_prompts: false\n    - type: health_monitor\n\n  rate_limiter:\n    type: adaptive\n    requests_per_minute: 50\n    tokens_per_minute: 100000\n\n  cost_tracker:\n    type: fixed_price\n    prompt_token_price: 0.0000015\n    completion_token_price: 0.000002\n\nconcurrency:\n  max_workers: 8\n  timeout_seconds: 120\n\nretry:\n  max_attempts: 5\n  initial_delay: 2.0\n  backoff_multiplier: 2.0\n\nsinks:\n  - type: signed_artifact\n    base_path: artifacts\n    algorithm: HMAC-SHA256\n    key_env: SIGNING_KEY\n    security_level: OFFICIAL\n\n  - type: azure_blob\n    config_path: config/blob_profiles.yaml\n    profile: production\n    security_level: OFFICIAL\n\n  - type: excel_workbook\n    base_path: \"reports/{experiment_name}\"\n    timestamped: true\n    include_manifest: true\n    security_level: OFFICIAL\n</code></pre>"},{"location":"user-guide/configuration/#pattern-3-rag-enabled-suite","title":"Pattern 3: RAG-Enabled Suite","text":"<p>Retrieval-Augmented Generation with vector store.</p> <pre><code># settings.yaml\nllm:\n  type: azure_openai\n  # ... standard config ...\n\n# experiment.yaml\nexperiment:\n  name: rag_qa\n  description: Question answering with RAG\n\n  datasource:\n    type: csv_local\n    path: data/questions.csv\n    security_level: OFFICIAL\n\n  row_plugins:\n    - type: retrieval_context\n      provider: pgvector\n      dsn: ${DATABASE_URL}\n      table: document_embeddings\n      embed_model: text-embedding-ada-002\n      query_field: question\n      top_k: 5\n      min_score: 0.7\n      inject_mode: prompt  # Inject into prompt template\n\n  llm:\n    prompt_template: |\n      Context: {retrieval_context}\n\n      Question: {question}\n\n      Answer:\n</code></pre>"},{"location":"user-guide/configuration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/configuration/#configuration-not-loading","title":"Configuration Not Loading","text":"<p>Problem: Configuration changes not reflected in experiments.</p> <p>Solutions: 1. Validate syntax: Run <code>validate-schemas</code> to catch YAML errors 2. Check merge order: Experiment overrides may be overriding your changes 3. Clear cache: Some plugins cache configuration (restart helps)</p>"},{"location":"user-guide/configuration/#middleware-not-running","title":"Middleware Not Running","text":"<p>Problem: Middleware defined but not executing.</p> <p>Solutions: 1. Check middleware list: Ensure middleware is in <code>llm.middleware</code> list 2. Verify plugin type: Use exact plugin name from Plugin Catalogue 3. Check execution order: Middleware runs in declaration order 4. Look for <code>inherit: false</code>: Experiment may be disabling inherited middleware</p>"},{"location":"user-guide/configuration/#environment-variables-not-resolving","title":"Environment Variables Not Resolving","text":"<p>Problem: <code>${VARIABLE}</code> appearing literally in configuration.</p> <p>Solutions: 1. Check variable is set: <pre><code>echo $AZURE_OPENAI_KEY\n</code></pre> 2. Source .env file: <pre><code>source .env\npython -m elspeth.cli ...\n</code></pre> 3. Use explicit export: <pre><code>export AZURE_OPENAI_KEY=sk-...\n</code></pre></p>"},{"location":"user-guide/configuration/#security-validation-fails","title":"Security Validation Fails","text":"<p>Problem: Pipeline aborts with security level error.</p> <p>Solution: See Security Model Troubleshooting for complete guide.</p>"},{"location":"user-guide/configuration/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/configuration/#1-use-settings-defaults","title":"1. Use Settings Defaults","text":"<p>Define common configuration once in settings.yaml:</p> <pre><code># settings.yaml - Define once\nllm:\n  middleware:\n    - type: pii_shield\n    - type: audit_logger\n\n# experiments/*.yaml - Inherit automatically\nexperiment:\n  name: experiment_1\n  # Middleware inherited from settings.yaml\n</code></pre>"},{"location":"user-guide/configuration/#2-validate-before-running","title":"2. Validate Before Running","text":"<p>Always validate configuration:</p> <pre><code># Catch errors before running\npython -m elspeth.cli validate-schemas \\\n  --settings config/my_suite/settings.yaml \\\n  --suite-root config/my_suite\n\n# Then run experiment\npython -m elspeth.cli \\\n  --settings config/my_suite/settings.yaml \\\n  --suite-root config/my_suite\n</code></pre>"},{"location":"user-guide/configuration/#3-use-environment-variables-for-secrets","title":"3. Use Environment Variables for Secrets","text":"<pre><code># \u274c DON'T: Hardcode secrets\nllm:\n  api_key: sk-1234567890abcdef\n\n# \u2705 DO: Use environment variables\nllm:\n  api_key: ${AZURE_OPENAI_KEY}\n</code></pre>"},{"location":"user-guide/configuration/#4-start-simple-add-complexity","title":"4. Start Simple, Add Complexity","text":"<pre><code># Phase 1: Minimal configuration\nexperiment:\n  datasource:\n    type: csv_local\n    path: data/input.csv\n\n  llm:\n    type: mock\n\n  sinks:\n    - type: csv\n      path: output.csv\n\n# Phase 2: Add security\nllm:\n  middleware:\n    - type: pii_shield\n\n# Phase 3: Add monitoring\nllm:\n  middleware:\n    - type: health_monitor\n\n# Phase 4: Add baseline comparison\nbaseline:\n  experiment_name: previous_run\n</code></pre>"},{"location":"user-guide/configuration/#5-document-configuration-decisions","title":"5. Document Configuration Decisions","text":"<p>Add comments explaining choices:</p> <pre><code>llm:\n  model_params:\n    temperature: 0.2  # Low temperature for consistency (classification task)\n    max_tokens: 50    # Short responses expected (category names)\n\n  middleware:\n    - type: pii_shield\n      # JUSTIFICATION: Customer data contains emails and phone numbers\n      # POLICY: Security team requirement (ticket #1234)\n</code></pre>"},{"location":"user-guide/configuration/#further-reading","title":"Further Reading","text":"<ul> <li>First Experiment - Hands-on configuration walkthrough</li> <li>Plugin Catalogue - Complete plugin reference</li> <li>Security Model - Understanding security levels</li> <li>Quickstart - Run sample configuration</li> </ul> <p>Configuration Mastery</p> <p>You now understand Elspeth's configuration system! Start with simple configurations and gradually add middleware, validation, and baseline comparisons as your experiments mature.</p>"},{"location":"user-guide/security-controls/","title":"Security Controls","text":"<p>Comprehensive guide to Elspeth's security mechanisms for protecting data and preventing unauthorized access.</p> <p>Data Classification vs Content Filtering</p> <ul> <li>Security Model = Data classification (Bell-LaPadula MLS enforcement)</li> <li>This page = Content filtering &amp; protection (PII detection, formula sanitization, signing)</li> </ul>"},{"location":"user-guide/security-controls/#quick-navigation","title":"Quick Navigation","text":"<ul> <li>Content Filtering - PII detection, classified material blocking, prompt shielding</li> <li>Output Protection - Formula sanitization, artifact signing</li> <li>Monitoring &amp; Audit - Audit logging, health monitoring</li> <li>Quick Start - Complete secure configuration</li> </ul>"},{"location":"user-guide/security-controls/#content-filtering","title":"Content Filtering","text":""},{"location":"user-guide/security-controls/#pii-detection-pii_shield-middleware","title":"PII Detection (pii_shield Middleware)","text":"<p>Purpose: Prevent personally identifiable information from reaching external LLM APIs.</p> <p>Detects: - Email addresses (RFC 5322 compliant) - Social Security Numbers (US format: XXX-XX-XXXX) - Credit card numbers (Visa, Mastercard, Amex, Discover) - Phone numbers (US format) - Australian Tax File Numbers (TFN) - Australian Medicare numbers</p> <p>Configuration: <pre><code>llm:\n  type: azure_openai\n  middleware:\n    - type: pii_shield\n      on_violation: abort  # abort | mask | log\n      patterns:\n        - email\n        - ssn\n        - credit_card\n        - phone\n        - au_tfn\n        - au_medicare\n</code></pre></p> <p>Behavior: - <code>abort</code>: Raise <code>SecurityCriticalError</code>, stop pipeline immediately - <code>mask</code>: Replace PII with <code>[REDACTED-{type}]</code>, continue processing - <code>log</code>: Log warning, continue processing (for testing only)</p> <p>Example: <pre><code># Input prompt containing PII\n\"Send email to john.doe@example.com about account 123-45-6789\"\n\n# With on_violation: abort\nSecurityCriticalError: PII detected: email, ssn\n\n# With on_violation: mask\n\"Send email to [REDACTED-EMAIL] about account [REDACTED-SSN]\"\n</code></pre></p> <p>Full Documentation: PII Shield in Plugin Catalogue</p>"},{"location":"user-guide/security-controls/#classified-material-detection-classified_material-middleware","title":"Classified Material Detection (classified_material Middleware)","text":"<p>Purpose: Prevent classified markings from being sent to untrusted systems.</p> <p>Detects: - SECRET, TOP SECRET, CONFIDENTIAL - TS//SCI (Top Secret // Sensitive Compartmented Information) - NOFORN (No Foreign Nationals) - PROTECTED, OFFICIAL SENSITIVE (Australian PSPF) - Custom markings (configurable)</p> <p>Configuration: <pre><code>llm:\n  middleware:\n    - type: classified_material\n      on_violation: abort\n      markings:\n        - SECRET\n        - TOP SECRET\n        - TS//SCI\n        - NOFORN\n        - PROTECTED\n      case_sensitive: false\n</code></pre></p> <p>Use Cases: - Government contractors handling unclassified data - Organizations with internal classification schemes - Preventing accidental disclosure of sensitive markings</p> <p>Full Documentation: Classified Material Filter in Plugin Catalogue</p>"},{"location":"user-guide/security-controls/#prompt-shielding-prompt_shield-middleware","title":"Prompt Shielding (prompt_shield Middleware)","text":"<p>Purpose: Block banned terms, profanity, or organization-specific restricted content.</p> <p>Configuration: <pre><code>llm:\n  middleware:\n    - type: prompt_shield\n      on_violation: abort\n      banned_terms:\n        - \"internal-only\"\n        - \"do-not-distribute\"\n        - \"company-confidential\"\n      case_sensitive: false\n</code></pre></p> <p>Use Cases: - Enforcing organizational content policies - Preventing accidental disclosure of internal jargon - Custom term filtering</p> <p>Full Documentation: Prompt Shield in Plugin Catalogue</p>"},{"location":"user-guide/security-controls/#azure-content-safety-integration","title":"Azure Content Safety Integration","text":"<p>Purpose: External content safety validation using Azure Content Safety API.</p> <p>Checks: - Hate speech detection - Violence/self-harm content - Sexual content - Profanity</p> <p>Configuration: <pre><code>llm:\n  middleware:\n    - type: azure_content_safety\n      endpoint: ${AZURE_CONTENT_SAFETY_ENDPOINT}\n      api_key: ${AZURE_CONTENT_SAFETY_KEY}\n      severity_threshold: medium  # low | medium | high\n      on_violation: abort\n</code></pre></p> <p>Requires: Azure Content Safety resource (separate Azure service)</p> <p>Full Documentation: Azure Content Safety in Plugin Catalogue</p>"},{"location":"user-guide/security-controls/#output-protection","title":"Output Protection","text":""},{"location":"user-guide/security-controls/#formula-sanitization-automatic","title":"Formula Sanitization (Automatic)","text":"<p>Purpose: Prevent spreadsheet injection attacks (Excel, CSV).</p> <p>How It Works: - All CSV and Excel sinks automatically sanitize output - Formulas are prefixed with <code>'</code> to prevent execution - No configuration required (always enabled)</p> <p>Protected Formulas: <pre><code>Original: =SUM(A1:A10)\nSanitized: '=SUM(A1:A10)\n\nOriginal: @SUM(A1:A10)  (Excel 4.0 macro)\nSanitized: '@SUM(A1:A10)\n\nOriginal: +1+1\nSanitized: '+1+1\n</code></pre></p> <p>Sinks with Automatic Sanitization: - <code>csv</code> (csv sink) - <code>excel_workbook</code> (Excel XLSX) - <code>visual_analytics</code> (CSV exports)</p> <p>Technical Details: See <code>src/elspeth/plugins/nodes/sinks/_sanitize.py</code></p>"},{"location":"user-guide/security-controls/#artifact-signing","title":"Artifact Signing","text":"<p>Purpose: Tamper-evident artifacts with cryptographic integrity verification.</p> <p>Supported Algorithms: - <code>HMAC-SHA256</code> - Symmetric key (fast, recommended for internal use) - <code>HMAC-SHA512</code> - Symmetric key (stronger hash) - <code>RSA-PSS-SHA256</code> - Asymmetric key (public/private keypair) - <code>ECDSA-P256-SHA256</code> - Asymmetric key (elliptic curve, smaller keys)</p> <p>Configuration: <pre><code>sinks:\n  - type: signed_artifact\n    algorithm: HMAC-SHA256\n    key_path: keys/signing.key\n    output_path: outputs/signed_bundle.tar.gz\n    security_level: OFFICIAL\n</code></pre></p> <p>Output: <pre><code>outputs/\n  signed_bundle.tar.gz          # Compressed archive\n  signed_bundle.tar.gz.sig      # Signature file\n  signed_bundle.tar.gz.manifest # Metadata (checksums, timestamps)\n</code></pre></p> <p>Verification: <pre><code>python -m elspeth.cli verify-bundle \\\n  --bundle-path outputs/signed_bundle.tar.gz \\\n  --key-path keys/signing.key\n</code></pre></p> <p>Use Cases: - Compliance requirements (HIPAA, PCI-DSS, government) - Audit trails for regulatory review - Tamper detection for experiment results</p> <p>Full Documentation: Signed Artifact Sink in Plugin Catalogue</p>"},{"location":"user-guide/security-controls/#monitoring-audit","title":"Monitoring &amp; Audit","text":""},{"location":"user-guide/security-controls/#audit-logging-audit_logger-middleware","title":"Audit Logging (audit_logger Middleware)","text":"<p>Purpose: Comprehensive logging of all requests and responses.</p> <p>Logged Information: - Request prompts (sanitized or full, configurable) - Response content - Token usage (prompt_tokens, completion_tokens) - Latency metrics - Security level - Retry attempts - Correlation IDs (for request tracing)</p> <p>Configuration: <pre><code>llm:\n  middleware:\n    - type: audit_logger\n      include_prompts: false  # false = sanitized, true = full prompts\n      include_responses: true\n      log_level: INFO\n</code></pre></p> <p>Output Location: <code>logs/run_*.jsonl</code> (JSONL format)</p> <p>Example Log Entry: <pre><code>{\n  \"timestamp\": \"2025-10-26T10:30:45.123Z\",\n  \"run_id\": \"exp-2025-10-26-001\",\n  \"row_id\": 42,\n  \"event\": \"llm_request\",\n  \"security_level\": \"OFFICIAL\",\n  \"prompt_tokens\": 150,\n  \"completion_tokens\": 80,\n  \"latency_ms\": 1234,\n  \"cost\": 0.0023\n}\n</code></pre></p> <p>Use Cases: - Compliance audits - Cost tracking - Performance monitoring - Incident investigation</p> <p>Full Documentation: Audit Logger in Plugin Catalogue</p>"},{"location":"user-guide/security-controls/#health-monitoring-health_monitor-middleware","title":"Health Monitoring (health_monitor Middleware)","text":"<p>Purpose: Track LLM API health and performance over time.</p> <p>Metrics: - Success rate (successful requests / total requests) - Average latency (milliseconds) - Token usage trends - Error rates by type (timeout, rate limit, auth failure)</p> <p>Configuration: <pre><code>llm:\n  middleware:\n    - type: health_monitor\n      heartbeat_interval: 60  # seconds\n      log_level: INFO\n</code></pre></p> <p>Use Cases: - Detecting LLM API degradation - Capacity planning - SLA monitoring</p> <p>Full Documentation: Health Monitor in Plugin Catalogue</p>"},{"location":"user-guide/security-controls/#quick-start-production-configuration","title":"Quick Start: Production Configuration","text":"<p>Complete security-hardened configuration for production use:</p> <pre><code># Production-ready configuration with all security controls\nexperiment:\n  name: secure_production_experiment\n\n  datasource:\n    type: azure_blob\n    container: production-data\n    path: experiments/input.csv\n    security_level: PROTECTED  # Match your data sensitivity\n\n  llm:\n    type: azure_openai\n    endpoint: ${AZURE_OPENAI_ENDPOINT}\n    api_key: ${AZURE_OPENAI_KEY}\n    deployment_name: gpt-4\n    security_level: PROTECTED\n\n    middleware:\n      # Layer 1: Block PII\n      - type: pii_shield\n        on_violation: abort\n        patterns: [email, ssn, credit_card, phone, au_tfn, au_medicare]\n\n      # Layer 2: Block classified markings\n      - type: classified_material\n        on_violation: abort\n        markings: [SECRET, TOP SECRET, PROTECTED, CONFIDENTIAL]\n\n      # Layer 3: Block banned terms\n      - type: prompt_shield\n        on_violation: abort\n        banned_terms: [internal-only, do-not-distribute]\n\n      # Layer 4: External content safety\n      - type: azure_content_safety\n        endpoint: ${AZURE_CONTENT_SAFETY_ENDPOINT}\n        api_key: ${AZURE_CONTENT_SAFETY_KEY}\n        severity_threshold: medium\n        on_violation: abort\n\n      # Layer 5: Audit logging\n      - type: audit_logger\n        include_prompts: false  # Sanitized prompts only\n        include_responses: true\n\n      # Layer 6: Health monitoring\n      - type: health_monitor\n        heartbeat_interval: 60\n\n  sinks:\n    # Sanitized CSV output (automatic formula protection)\n    - type: csv\n      path: outputs/results.csv\n      security_level: PROTECTED\n\n    # Signed tamper-evident bundle\n    - type: signed_artifact\n      algorithm: HMAC-SHA256\n      key_path: keys/signing.key\n      output_path: outputs/signed_bundle.tar.gz\n      security_level: PROTECTED\n      consumes: [csv]  # Bundle includes CSV\n</code></pre> <p>What This Configuration Provides: - \u2705 PII detection (6 pattern types) - \u2705 Classified material blocking - \u2705 Custom term filtering - \u2705 External content safety validation - \u2705 Comprehensive audit logging - \u2705 Performance monitoring - \u2705 Formula injection prevention (automatic) - \u2705 Tamper-evident artifacts - \u2705 Bell-LaPadula MLS enforcement (PROTECTED level)</p> <p>Next Steps: 1. Replace <code>${AZURE_*}</code> environment variables with actual values 2. Create signing key: <code>openssl rand -hex 32 &gt; keys/signing.key</code> 3. Adjust <code>security_level</code> to match your data classification 4. Test with <code>--head 5</code> before full run</p>"},{"location":"user-guide/security-controls/#security-controls-summary","title":"Security Controls Summary","text":""},{"location":"user-guide/security-controls/#defense-in-depth-layers","title":"Defense-in-Depth Layers","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Layer 1: Bell-LaPadula MLS (Data Classification)      \u2502\n\u2502  \u2192 Prevents unauthorized access to high-classified data \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Layer 2: Content Filtering (Middleware)                \u2502\n\u2502  \u2192 PII detection, classified markings, banned terms     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Layer 3: External Validation (Azure Content Safety)    \u2502\n\u2502  \u2192 Hate speech, violence, profanity detection          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Layer 4: Output Protection (Sanitization + Signing)    \u2502\n\u2502  \u2192 Formula injection prevention, tamper evidence        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Layer 5: Audit &amp; Monitoring (Logging + Health)        \u2502\n\u2502  \u2192 Comprehensive audit trail, performance tracking      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/security-controls/#when-to-use-each-control","title":"When to Use Each Control","text":"Control Use When Skip When PII Shield Handling customer data, healthcare, financial Public data only (marketing copy) Classified Material Government, defense, regulated industries No classified markings expected Prompt Shield Organization-specific policies No custom term restrictions Azure Content Safety User-generated content, public-facing Internal business workflows Formula Sanitization Any CSV/Excel output N/A (always enabled) Artifact Signing Compliance requirements (HIPAA, PCI-DSS) Testing/development Audit Logging Production environments Local development only Health Monitoring Production environments Single-run experiments"},{"location":"user-guide/security-controls/#related-documentation","title":"Related Documentation","text":"<ul> <li>Security Model - Bell-LaPadula MLS data classification</li> <li>Configuration Guide - YAML configuration reference</li> <li>Plugin Catalogue - Complete middleware documentation</li> <li>Architecture &gt; Security Policy - Design principles and ADRs</li> </ul>"},{"location":"user-guide/security-controls/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/security-controls/#pii-detected-but-i-dont-see-any-pii","title":"\"PII detected\" but I don't see any PII","text":"<p>Cause: False positive (e.g., \"123-45-6789\" could be an order number, not SSN)</p> <p>Solutions: 1. Temporarily use <code>on_violation: log</code> to identify the pattern:    <pre><code>- type: pii_shield\n  on_violation: log  # Logs detection but continues\n</code></pre> 2. Exclude specific patterns:    <pre><code>- type: pii_shield\n  patterns: [email, credit_card]  # Exclude SSN if causing issues\n</code></pre> 3. Sanitize input data before running experiment</p>"},{"location":"user-guide/security-controls/#securitycriticalerror-in-middleware","title":"\"SecurityCriticalError\" in middleware","text":"<p>Cause: Security middleware detected policy violation</p> <p>Debugging: 1. Check <code>logs/run_*.jsonl</code> for detailed error:    <pre><code>tail -f logs/run_*.jsonl | grep ERROR\n</code></pre> 2. Review the failing row's input data 3. Verify middleware configuration matches your data</p> <p>Solutions: - Adjust <code>on_violation</code> policy (abort \u2192 log for debugging) - Adjust detection thresholds (e.g., Azure Content Safety severity) - Sanitize input data before processing</p>"},{"location":"user-guide/security-controls/#insufficient-clearance-error","title":"\"Insufficient clearance\" error","text":"<p>Cause: This is Bell-LaPadula MLS error (data classification), not middleware</p> <p>Solution: See Security Model Troubleshooting</p> <p>Security Controls Summary</p> <p>Elspeth provides 5 layers of defense-in-depth security:</p> <ol> <li>\u2705 Data Classification (Bell-LaPadula MLS)</li> <li>\u2705 Content Filtering (PII, classified markings, banned terms)</li> <li>\u2705 External Validation (Azure Content Safety)</li> <li>\u2705 Output Protection (formula sanitization, signing)</li> <li>\u2705 Audit &amp; Monitoring (logging, health tracking)</li> </ol> <p>Start with the Quick Start configuration and adjust to your security requirements!</p>"},{"location":"user-guide/security-model/","title":"Security Model","text":"<p>Understand Elspeth's Bell-LaPadula Multi-Level Security (MLS) enforcement.</p> <p>Data Classification vs Content Filtering</p> <p>This page covers: Data classification (Bell-LaPadula MLS) For content filtering: PII detection, classified markings, formula sanitization \u2192 See Security Controls</p> <p>Why This Matters</p> <p>Elspeth enforces fail-fast security validation to prevent sensitive data from flowing into untrusted components. Understanding this model is critical for configuring experiments correctly.</p>"},{"location":"user-guide/security-model/#overview","title":"Overview","text":"<p>Elspeth implements Multi-Level Security (MLS) based on the Bell-LaPadula model, originally designed for military and government systems. This ensures:</p> <ul> <li>\u2705 Fail-fast validation - Misconfigured pipelines abort before data is retrieved</li> <li>\u2705 No unauthorized access - Components can't access data above their clearance</li> <li>\u2705 Trusted downgrade - High-clearance components can safely filter data for lower levels</li> <li>\u2705 Audit trail - All security decisions logged</li> </ul>"},{"location":"user-guide/security-model/#security-level-hierarchy","title":"Security Level Hierarchy","text":"<p>Elspeth uses five security levels (based on Australian Government PSPF classifications):</p> <pre><code>UNOFFICIAL \u2192 OFFICIAL \u2192 OFFICIAL_SENSITIVE \u2192 PROTECTED \u2192 SECRET\n(lowest)                                                  (highest)\n</code></pre> <pre><code>graph LR\n    A[UNOFFICIAL&lt;br/&gt;Public data] --&gt; B[OFFICIAL&lt;br/&gt;Business data]\n    B --&gt; C[OFFICIAL_SENSITIVE&lt;br/&gt;Sensitive data]\n    C --&gt; D[PROTECTED&lt;br/&gt;Highly sensitive]\n    D --&gt; E[SECRET&lt;br/&gt;Classified]\n\n    style A fill:#90EE90\n    style B fill:#FFD700\n    style C fill:#FFA500\n    style D fill:#FF6347\n    style E fill:#8B0000,color:#fff</code></pre>"},{"location":"user-guide/security-model/#level-descriptions","title":"Level Descriptions","text":"Level Description Example Use Cases UNOFFICIAL Public information, no sensitivity Marketing copy, public datasets OFFICIAL Routine business data, limited distribution Customer names, product lists OFFICIAL_SENSITIVE Sensitive business data, controlled access Customer emails, internal reports PROTECTED Highly sensitive data, strict access controls Financial records, HR data SECRET Classified information, maximum protection Government secrets, regulated healthcare data <p>Start with UNOFFICIAL</p> <p>For testing and development, use <code>UNOFFICIAL</code> for all components. Increase levels only when handling actual sensitive data.</p>"},{"location":"user-guide/security-model/#key-concepts","title":"Key Concepts","text":"<p>Critical Distinction: Clearance vs Operating Level</p> <ul> <li>Security Level (Clearance): What a component CAN handle (its security badge)</li> <li>Operating Level: What the pipeline IS handling (which security zone it's operating in)</li> </ul> <p>Analogy: You have a SECRET badge (clearance) but you're working in an OFFICIAL room (operating level). Your badge allows you to be there, but you're not accessing SECRET data right now.</p>"},{"location":"user-guide/security-model/#security-level-clearance","title":"Security Level (Clearance)","text":"<p>What it is: The maximum classification level a component is cleared to handle.</p> <p>Where it's declared: In plugin configuration:</p> <pre><code>datasource:\n  type: csv_local\n  path: data/customer_data.csv\n  security_level: OFFICIAL  # \u2190 Cleared for OFFICIAL data\n</code></pre> <p>Analogy: Like a security badge. If you have a SECRET badge, you can access SECRET, PROTECTED, OFFICIAL, and UNOFFICIAL areas.</p>"},{"location":"user-guide/security-model/#operating-level","title":"Operating Level","text":"<p>What it is: The actual classification level the pipeline is running at.</p> <p>How it's computed: Minimum security level across ALL components:</p> <pre><code>operating_level = min(\n    datasource.security_level,\n    llm.security_level,\n    sink1.security_level,\n    sink2.security_level,\n    # ... all components\n)\n</code></pre> <p>Analogy: The whole pipeline operates at the \"lowest common clearance\" - the weakest link in the chain.</p>"},{"location":"user-guide/security-model/#bell-lapadula-no-read-up-rule","title":"Bell-LaPadula \"No Read Up\" Rule","text":"<p>The Core Principle: Components can only access data at or below their security level.</p> <pre><code>\u274c UNOFFICIAL component accessing SECRET data \u2192 FORBIDDEN (insufficient clearance)\n\u2705 SECRET component accessing UNOFFICIAL data \u2192 ALLOWED (trusted to filter)\n</code></pre> <p>Directionality: - Data classification: Can only INCREASE (UNOFFICIAL \u2192 SECRET via explicit uplift) - Plugin operations: Can only DECREASE (SECRET \u2192 UNOFFICIAL via trusted downgrade)</p> <p>These move in opposite directions - this is intentional!</p> <pre><code>graph TB\n    subgraph \"Bell-LaPadula No Read Up\"\n        direction TB\n        SECRET[SECRET Component&lt;br/&gt;Clearance: SECRET]\n        OFFICIAL[OFFICIAL Component&lt;br/&gt;Clearance: OFFICIAL]\n        UNOFFICIAL[UNOFFICIAL Component&lt;br/&gt;Clearance: UNOFFICIAL]\n\n        SECRET --&gt;|\u2705 Can access| SECRET_DATA[SECRET Data]\n        SECRET --&gt;|\u2705 Can access| OFFICIAL_DATA[OFFICIAL Data]\n        SECRET --&gt;|\u2705 Can access| UNOFFICIAL_DATA[UNOFFICIAL Data]\n\n        OFFICIAL --&gt;|\u274c BLOCKED| SECRET_DATA\n        OFFICIAL --&gt;|\u2705 Can access| OFFICIAL_DATA\n        OFFICIAL --&gt;|\u2705 Can access| UNOFFICIAL_DATA\n\n        UNOFFICIAL --&gt;|\u274c BLOCKED| SECRET_DATA\n        UNOFFICIAL --&gt;|\u274c BLOCKED| OFFICIAL_DATA\n        UNOFFICIAL --&gt;|\u2705 Can access| UNOFFICIAL_DATA\n    end\n\n    style SECRET fill:#8B0000,color:#fff\n    style OFFICIAL fill:#FFD700\n    style UNOFFICIAL fill:#90EE90\n    style SECRET_DATA fill:#ffcccc\n    style OFFICIAL_DATA fill:#ffffcc\n    style UNOFFICIAL_DATA fill:#ccffcc</code></pre>"},{"location":"user-guide/security-model/#how-it-works-step-by-step","title":"How It Works: Step-by-Step","text":""},{"location":"user-guide/security-model/#example-1-successful-pipeline","title":"Example 1: Successful Pipeline","text":"<p>Configuration: <pre><code>datasource:\n  security_level: OFFICIAL\n\nllm:\n  security_level: SECRET\n\nsinks:\n  - type: csv\n    security_level: OFFICIAL\n</code></pre></p> <p>Computation: <pre><code>operating_level = min(OFFICIAL, SECRET, OFFICIAL)\n                = OFFICIAL  # Lowest clearance wins\n</code></pre></p> <p>Validation: - Datasource: Has OFFICIAL clearance, pipeline operating at OFFICIAL \u2192 \u2705 PASS (exact match) - LLM: Has SECRET clearance, pipeline operating at OFFICIAL \u2192 \u2705 PASS (trusted downgrade - can operate below clearance level) - Sink: Has OFFICIAL clearance, pipeline operating at OFFICIAL \u2192 \u2705 PASS (exact match)</p> <p>Result: Pipeline runs successfully at <code>OFFICIAL</code> level.</p> <pre><code>graph LR\n    subgraph \"Pipeline Components\"\n        DS[Datasource&lt;br/&gt;OFFICIAL]\n        LLM[LLM&lt;br/&gt;SECRET]\n        SINK[Sink&lt;br/&gt;OFFICIAL]\n    end\n\n    subgraph \"Operating Level Computation\"\n        MIN[\"min(OFFICIAL, SECRET, OFFICIAL)&lt;br/&gt;=&lt;br/&gt;OFFICIAL\"]\n    end\n\n    DS --&gt; MIN\n    LLM --&gt; MIN\n    SINK --&gt; MIN\n\n    MIN --&gt; VALIDATION{Validation}\n\n    VALIDATION --&gt;|DS: OFFICIAL \u2265 OFFICIAL \u2705| PASS1[\u2705 Pass]\n    VALIDATION --&gt;|LLM: SECRET \u2265 OFFICIAL \u2705| PASS2[\u2705 Pass]\n    VALIDATION --&gt;|Sink: OFFICIAL \u2265 OFFICIAL \u2705| PASS3[\u2705 Pass]\n\n    PASS1 &amp; PASS2 &amp; PASS3 --&gt; SUCCESS[\ud83c\udf89 Pipeline Runs]\n\n    style DS fill:#FFD700\n    style LLM fill:#8B0000,color:#fff\n    style SINK fill:#FFD700\n    style MIN fill:#ADD8E6\n    style SUCCESS fill:#90EE90</code></pre>"},{"location":"user-guide/security-model/#example-2-failed-pipeline-insufficient-clearance","title":"Example 2: Failed Pipeline (Insufficient Clearance)","text":"<p>Configuration: <pre><code>datasource:\n  security_level: UNOFFICIAL  # \u2190 Low clearance\n\nllm:\n  security_level: SECRET\n\nsinks:\n  - type: csv\n    security_level: SECRET  # \u2190 High clearance\n</code></pre></p> <p>Computation: <pre><code>operating_level = min(UNOFFICIAL, SECRET, SECRET)\n                = UNOFFICIAL  # Datasource is the bottleneck\n</code></pre></p> <p>Validation: - Datasource: <code>UNOFFICIAL</code> clearance, operating at <code>UNOFFICIAL</code> \u2192 \u2705 PASS - LLM: <code>SECRET</code> clearance, operating at <code>UNOFFICIAL</code> \u2192 \u2705 PASS (trusted downgrade) - Sink: <code>SECRET</code> clearance, operating at <code>UNOFFICIAL</code> \u2192 \u2705 PASS (trusted downgrade)</p> <p>Result: Pipeline runs successfully at <code>UNOFFICIAL</code> level.</p> <p>Wait, why didn't it fail?</p> <p>Because the operating level is automatically computed as the MINIMUM. The <code>SECRET</code> sink can safely operate at <code>UNOFFICIAL</code> level (it's cleared for higher, so it can handle lower).</p>"},{"location":"user-guide/security-model/#example-3-when-validation-does-fail","title":"Example 3: When Validation DOES Fail","text":"<p>Validation failures occur when you manually force a higher operating level:</p> <p>Configuration (with manual override): <pre><code># Force pipeline to operate at SECRET level\noperating_level_override: SECRET  # Manual forcing\n\ndatasource:\n  security_level: UNOFFICIAL  # \u2190 Insufficient!\n\nsink:\n  security_level: SECRET\n</code></pre></p> <p>Error: <pre><code>SecurityValidationError: Datasource has insufficient clearance.\nComponent clearance: UNOFFICIAL\nRequired operating level: SECRET\nCannot access SECRET data with UNOFFICIAL clearance (Bell-LaPadula \"no read up\" violation)\n</code></pre></p> <p>Why it fails: You're forcing the datasource to access <code>SECRET</code> data, but it only has <code>UNOFFICIAL</code> clearance.</p> <p>Manual Overrides Rare</p> <p>In normal operation, you won't manually set operating levels. The automatic minimum computation prevents insufficient-clearance errors. This scenario only occurs with explicit configuration overrides.</p>"},{"location":"user-guide/security-model/#common-scenarios","title":"Common Scenarios","text":""},{"location":"user-guide/security-model/#scenario-1-public-data-public-output","title":"Scenario 1: Public Data \u2192 Public Output \u2705","text":"<pre><code>datasource:\n  security_level: UNOFFICIAL\n\nllm:\n  security_level: UNOFFICIAL\n\nsinks:\n  - type: csv\n    security_level: UNOFFICIAL\n</code></pre> <p>Operating Level: <code>UNOFFICIAL</code> (minimum)</p> <p>Result: \u2705 PASS - All components match, pipeline runs at <code>UNOFFICIAL</code></p>"},{"location":"user-guide/security-model/#scenario-2-secret-data-secret-output","title":"Scenario 2: Secret Data \u2192 Secret Output \u2705","text":"<pre><code>datasource:\n  security_level: SECRET\n\nllm:\n  security_level: SECRET\n\nsinks:\n  - type: csv\n    security_level: SECRET\n</code></pre> <p>Operating Level: <code>SECRET</code> (minimum)</p> <p>Result: \u2705 PASS - All components match, pipeline runs at <code>SECRET</code></p>"},{"location":"user-guide/security-model/#scenario-3-secret-datasource-public-output","title":"Scenario 3: Secret Datasource \u2192 Public Output \u2705","text":"<pre><code>datasource:\n  security_level: SECRET  # Can access SECRET data\n\nllm:\n  security_level: SECRET\n\nsinks:\n  - type: csv\n    security_level: UNOFFICIAL  # \u2190 Lower clearance\n</code></pre> <p>Operating Level: <code>UNOFFICIAL</code> (minimum - the sink)</p> <p>Result: \u2705 PASS - Datasource is trusted to filter SECRET data down to UNOFFICIAL</p> <p>How it works: The <code>SECRET</code> datasource operates at <code>UNOFFICIAL</code> level by: 1. Retrieving all data (has clearance) 2. Filtering out SECRET-tagged rows (trusted responsibility) 3. Passing only UNOFFICIAL data to pipeline</p> <p>This is called trusted downgrade - high-clearance components can safely operate at lower levels.</p>"},{"location":"user-guide/security-model/#scenario-4-public-datasource-secret-output","title":"Scenario 4: Public Datasource \u2192 Secret Output \u2705","text":"<pre><code>datasource:\n  security_level: UNOFFICIAL  # \u2190 Lowest clearance\n\nllm:\n  security_level: SECRET\n\nsinks:\n  - type: csv\n    security_level: SECRET  # Can write SECRET data\n</code></pre> <p>Operating Level: <code>UNOFFICIAL</code> (minimum - the datasource)</p> <p>Result: \u2705 PASS - Pipeline operates at <code>UNOFFICIAL</code>, sink accepts lower-classified data</p> <p>Note: The sink has <code>SECRET</code> clearance, so it can handle <code>UNOFFICIAL</code> data (higher clearance accepts lower data).</p>"},{"location":"user-guide/security-model/#data-classification-vs-plugin-operations","title":"Data Classification vs Plugin Operations","text":"<p>CRITICAL DISTINCTION: Data and plugins move in opposite directions:</p>"},{"location":"user-guide/security-model/#data-classification-can-only-increase","title":"Data Classification (Can Only INCREASE)","text":"<pre><code>UNOFFICIAL data \u2192 Explicit uplift \u2192 OFFICIAL data\nOFFICIAL data   \u2192 Explicit uplift \u2192 SECRET data\n</code></pre> <p>\u2705 Allowed: Uplifting UNOFFICIAL to SECRET (via explicit API call) \u274c Forbidden: Downgrading SECRET to UNOFFICIAL (violates \"no write down\")</p> <p>Example: <pre><code>df = ClassifiedDataFrame.create_from_datasource(\n    raw_df,\n    source_classification=SecurityLevel.UNOFFICIAL\n)\n\n# Explicit uplift (audited)\ndf_secret = df.with_uplifted_classification(SecurityLevel.SECRET)\n</code></pre></p>"},{"location":"user-guide/security-model/#plugin-operations-can-only-decrease","title":"Plugin Operations (Can Only DECREASE)","text":"<pre><code>SECRET plugin    \u2192 Trusted downgrade \u2192 Operates at OFFICIAL\nOFFICIAL plugin  \u2192 Trusted downgrade \u2192 Operates at UNOFFICIAL\n</code></pre> <p>\u2705 Allowed: SECRET plugin operating at UNOFFICIAL (trusted to filter) \u274c Forbidden: UNOFFICIAL plugin operating at SECRET (insufficient clearance)</p> <p>Example: <pre><code># SECRET datasource can operate at UNOFFICIAL level\ndatasource.validate_can_operate_at_level(SecurityLevel.UNOFFICIAL)  # \u2705 PASS\n\n# UNOFFICIAL datasource CANNOT operate at SECRET level\ndatasource.validate_can_operate_at_level(SecurityLevel.SECRET)      # \u274c FAIL\n</code></pre></p>"},{"location":"user-guide/security-model/#frozen-plugins-advanced","title":"Frozen Plugins (Advanced)","text":"<p>Some plugins are frozen - they refuse to operate below their declared level.</p> <p>Use case: Dedicated SECRET infrastructure that should NEVER serve lower-classified pipelines.</p> <p>Configuration: <pre><code>class FrozenSecretDataSource(BasePlugin, DataSource):\n    def __init__(self, ...):\n        super().__init__(\n            security_level=SecurityLevel.SECRET,\n            allow_downgrade=False  # \u2190 Frozen at SECRET only\n        )\n</code></pre></p> <p>Behavior: - \u2705 Can operate at <code>SECRET</code> level (exact match) - \u274c Cannot operate at <code>OFFICIAL</code> or <code>UNOFFICIAL</code> (rejects downgrade)</p> <p>Error: <pre><code>SecurityValidationError: Frozen plugin cannot operate below declared level.\nDeclared level: SECRET\nRequested level: OFFICIAL\nThis plugin has allow_downgrade=False\n</code></pre></p> <p>See ADR-005 (Frozen Plugin Protection) for details.</p>"},{"location":"user-guide/security-model/#troubleshooting","title":"Troubleshooting","text":""},{"location":"user-guide/security-model/#error-insufficient-clearance","title":"Error: \"Insufficient clearance\"","text":"<p>Full error: <pre><code>SecurityValidationError: Component has insufficient clearance.\nComponent clearance: UNOFFICIAL\nRequired operating level: SECRET\n</code></pre></p> <p>Cause: You've manually forced a pipeline to operate at a level higher than a component's clearance.</p> <p>Solution: Either: 1. Remove manual override (let automatic minimum computation handle it) 2. Increase component clearance:    <pre><code>datasource:\n  security_level: SECRET  # Raise to match required level\n</code></pre></p>"},{"location":"user-guide/security-model/#error-frozen-plugin-cannot-operate-below-declared-level","title":"Error: \"Frozen plugin cannot operate below declared level\"","text":"<p>Full error: <pre><code>SecurityValidationError: Frozen plugin cannot operate below declared level.\nDeclared level: SECRET\nRequested level: OFFICIAL\n</code></pre></p> <p>Cause: A frozen plugin (allow_downgrade=False) is being asked to operate at a lower level.</p> <p>Solution: Either: 1. Raise all components to match frozen plugin's level:    <pre><code>datasource:\n  security_level: SECRET  # Match frozen plugin\n\nsinks:\n  - security_level: SECRET  # Match frozen plugin\n</code></pre> 2. Use non-frozen plugin (default allow_downgrade=True)</p>"},{"location":"user-guide/security-model/#warning-operating-at-lower-level-than-capable","title":"Warning: \"Operating at lower level than capable\"","text":"<p>Message: <pre><code>WARNING: SECRET datasource operating at UNOFFICIAL level.\nEnsure datasource properly filters SECRET data.\n</code></pre></p> <p>Cause: A high-clearance component is operating at a lower level (trusted downgrade).</p> <p>Action: This is expected behavior. Verify the component: - Properly filters data at the operating level - Doesn't leak higher-classified information - Is certified for trusted downgrade use</p>"},{"location":"user-guide/security-model/#audit-logging","title":"Audit Logging","text":"<p>All security decisions are logged to <code>logs/run_*.jsonl</code>:</p> <pre><code>{\n  \"event\": \"security_validation\",\n  \"timestamp\": \"2025-10-26T14:30:00Z\",\n  \"component\": \"datasource\",\n  \"clearance\": \"SECRET\",\n  \"operating_level\": \"OFFICIAL\",\n  \"result\": \"PASS\",\n  \"reason\": \"Trusted downgrade\"\n}\n</code></pre> <p>Review audit logs to: - Verify security enforcement - Troubleshoot validation failures - Provide evidence for compliance audits</p>"},{"location":"user-guide/security-model/#best-practices","title":"Best Practices","text":""},{"location":"user-guide/security-model/#1-start-with-unofficial","title":"1. Start with UNOFFICIAL","text":"<p>For development and testing: <pre><code># Set everything to UNOFFICIAL during development\ndatasource:\n  security_level: UNOFFICIAL\n\nllm:\n  security_level: UNOFFICIAL\n\nsinks:\n  - security_level: UNOFFICIAL\n</code></pre></p> <p>Raise levels only when handling actual sensitive data.</p>"},{"location":"user-guide/security-model/#2-match-levels-for-simplicity","title":"2. Match Levels for Simplicity","text":"<p>Simplest configuration - all components at same level: <pre><code>datasource:\n  security_level: OFFICIAL\n\nllm:\n  security_level: OFFICIAL\n\nsinks:\n  - security_level: OFFICIAL\n</code></pre></p> <p>Pipeline operates at <code>OFFICIAL</code>, no downgrade needed.</p>"},{"location":"user-guide/security-model/#3-use-trusted-downgrade-intentionally","title":"3. Use Trusted Downgrade Intentionally","text":"<p>Only use high-clearance components operating at lower levels when: - \u2705 Component is certified to filter appropriately - \u2705 Organizational policy allows trusted downgrade - \u2705 Audit logging is enabled - \u2705 Regular security reviews are conducted</p>"},{"location":"user-guide/security-model/#4-never-manually-override-operating-level","title":"4. Never Manually Override Operating Level","text":"<p>Avoid forcing operating levels unless absolutely necessary: <pre><code># \u274c Avoid this\noperating_level_override: SECRET\n\n# \u2705 Prefer this (automatic minimum)\n# (no override - let system compute)\n</code></pre></p>"},{"location":"user-guide/security-model/#5-document-security-decisions","title":"5. Document Security Decisions","text":"<p>In experiment configs, explain security choices: <pre><code>datasource:\n  security_level: SECRET\n  # JUSTIFICATION: Accessing classified government datasets\n  # CERTIFICATION: Approved by security team (ticket #1234)\n  # FILTER BEHAVIOR: Removes all SECRET-tagged rows when operating below SECRET\n\nllm:\n  security_level: OFFICIAL\n  # JUSTIFICATION: LLM endpoint not cleared for SECRET data\n  # CONSEQUENCE: Pipeline forced to OFFICIAL level (minimum)\n</code></pre></p>"},{"location":"user-guide/security-model/#visual-summary","title":"Visual Summary","text":""},{"location":"user-guide/security-model/#pipeline-security-flow","title":"Pipeline Security Flow","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 SECURITY LEVEL ENFORCEMENT                                      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                 \u2502\n\u2502  1. Components declare security levels (clearances)             \u2502\n\u2502     \u2193                                                           \u2502\n\u2502  2. System computes operating level = MIN(all clearances)       \u2502\n\u2502     \u2193                                                           \u2502\n\u2502  3. Each component validates: Can I operate at this level?      \u2502\n\u2502     \u2193                                                           \u2502\n\u2502  4. HIGH clearance \u2265 operating level \u2192 \u2705 PASS (trusted)        \u2502\n\u2502     LOW clearance &lt; operating level \u2192 \u274c FAIL (insufficient)    \u2502\n\u2502     \u2193                                                           \u2502\n\u2502  5. Pipeline runs (or aborts if validation failed)              \u2502\n\u2502                                                                 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"user-guide/security-model/#security-level-hierarchy_1","title":"Security Level Hierarchy","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   SECRET    \u2502  \u2190 Highest classification\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  PROTECTED  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  OFFICIAL   \u2502\n\u2502 :SENSITIVE  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  OFFICIAL   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 UNOFFICIAL  \u2502  \u2190 Lowest classification (public)\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nAllowed Operations:\n  \u2705 SECRET component \u2192 Can operate at OFFICIAL (trusted downgrade)\n  \u274c OFFICIAL component \u2192 Cannot operate at SECRET (insufficient clearance)\n</code></pre>"},{"location":"user-guide/security-model/#further-reading","title":"Further Reading","text":"<ul> <li>ADR-002: Multi-Level Security Enforcement - Full specification</li> <li>ADR-002a: ClassifiedDataFrame Constructor - Data classification model</li> <li>ADR-005: Frozen Plugin Protection - Strict level enforcement</li> </ul> <p>(See ADR Catalogue for links to full ADR documents in the repository)</p> <p>Key Takeaways</p> <ul> <li>Operating level = MIN of all component clearances</li> <li>\"No read up\" = Components can't access data above their clearance</li> <li>Trusted downgrade = High-clearance components can safely operate at lower levels</li> <li>Fail-fast = Validation happens before data retrieval</li> <li>Start with UNOFFICIAL for development, raise levels only when needed</li> </ul> <p>Security is enforced automatically - just declare levels correctly and the system handles the rest!</p>"}]}