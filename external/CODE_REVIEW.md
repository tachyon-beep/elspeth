ELSPETH Orchestrator Security & Code Quality Assessment

1. Security Analysis
Authentication & Authorization: The ELSPETH orchestrator is a client-side tool without multi-user authentication features. It runs under the identity of the operator or pipeline, and there is no built-in user login or role-based access control. Instead, security relies on the credentials used for external services (Azure OpenAI, Azure Blob Storage, etc.). The code correctly uses environment variables or Azure managed identity for those credentials, rather than hard-coding secrets. For example, the Azure OpenAI plugin loads api_key, api_version, and azure_endpoint from config or environment and raises an error if they are missing[1][2]. This is good practice, as secrets are not stored in code or repository. We confirmed that no API keys or passwords are present in the repository; sample configs use placeholder values (e.g. "dummy" API keys) and instruct the user to supply real secrets via .env or environment variables[3]. Authorization to external resources (like blob storage or GitHub) is delegated to those services using tokens/keys the user provides. Recommendation: Ensure that the runtime environment provides these credentials securely (e.g. via Azure Key Vault or CI/CD secrets) and that the permissions of those credentials follow least privilege (e.g. the Azure AD identity or SAS token only has access to the specific storage container or OpenAI deployment needed).
Input Validation & Sanitization: The orchestrator heavily validates its input configurations and data, reducing the risk of malicious input. YAML configuration files are loaded with yaml.safe_load [which avoids executing arbitrary Python in YAML](4). The config loader (load_settings) then validates the structure of the config using predefined JSON schemas for each plugin type. Each plugin’s factory in the registry defines a schema of expected options and will throw a ConfigurationError if unknown or invalid fields are provided[5][6]. This design ensures that only expected configuration fields are accepted, preventing accidental misconfigurations or injection of rogue parameters. The code also enforces required fields in config (e.g. a missing system or user prompt in an experiment config results in a clear error[7]).
On the data side, experiment input data is read either from CSV/Blob or other sources. There is limited “taint” surface here, as this data is used to fill prompt templates and is not executed as code. Prompt templates themselves are rendered using a Jinja2 environment with StrictUndefined (raises errors on undefined variables) and no auto-escaping [since output is plain text prompts, not HTML](8). The Jinja environment is configured without dangerous extensions: it does not allow executing code or importing templates from disk (no custom filters executing shell, no {{ self }} references beyond context). Thus, the risk of code injection via prompt templates is low. We did not find any use of Python eval or unsafe parsing in the code.
One potential input-related vulnerability to note is CSV/Excel formula injection in the Excel output sink. The ExcelResultSink writes experiment results into an .xlsx file by inserting cell values directly[9]. If any value in the results starts with =, +, -, or @, Excel may interpret it as a formula when the file is opened. For example, if an input CSV row or an LLM’s output begins with =HYPERLINK("<http://malicious.site>", "Click"), it would be written as-is into a cell; opening the Excel could trigger a prompt or the formula logic. This is a known issue (CSV Injection). Currently, the code does not sanitize or prefix such values to neutralize Excel formulas. This could be used as a social engineering attack vector if an attacker can influence input data (e.g. someone with access to the data source inserts a formula to exploit the machine of whoever opens the report). Recommendation: Sanitize Excel/CSV outputs by prefixing dangerous leading characters with a single quote or space for safety, or document this risk and advise users to handle it (especially if orchestrated data comes from untrusted sources). Given that the orchestrator is likely used in controlled scenarios, this is a medium severity issue.
OWASP Top 10 Vulnerabilities: Many common web-app vulnerabilities do not directly apply here, since this is not a web server. There are no SQL databases or dynamic web interfaces in the code – so no SQL injection or XSS concerns. One area to watch is command injection, but the code does not spawn subprocesses or execute shell commands with untrusted input. All external interactions are via well-formed library calls (HTTP requests via requests, Azure SDK, etc.). Path traversal is also mitigated – file system writes (e.g. for output files) use user-specified directories but always ensure the directory exists and don’t overwrite files unless configured. For example, the blob downloader refuses to overwrite an existing file unless overwrite=True[10][11], and the blob uploader similarly checks overwrite before writing[12][13]. These behaviors prevent accidental data loss or override.
Cryptography & Key Management: The project includes an HMAC-SHA256/512 signing utility for “signed artifact” outputs. We reviewed the implementation and it correctly uses the Python hmac library with a provided key and compares digests in a timing-safe way[14]. The signing key can be provided via config or (preferably) through an environment variable ELSPETH_SIGNING_KEY[15][16]. If not provided, the sink will error – avoiding a scenario where unsigned data is mistakenly treated as signed. The cryptography usage is limited to signing (no encryption of data at rest in this codebase). Signing output bundles is a good integrity control, ensuring that results can be verified and not tampered with. However, note that the signing key becomes a sensitive secret; it is never logged (the code only logs a warning if a legacy env var is used, not the key content[17]) and is only kept in memory long enough to use it. Recommendation: manage the signing key like any secret (environment injection, not in config), and rotate it if needed. For additional security (particularly if handling sensitive or classified data), consider supporting encryption of artifacts at rest. For example, an encryption plugin or an option to encrypt the results.json and other outputs with a symmetric key would provide confidentiality on top of the current integrity guarantees. This might be important for IRAP if any output data is classified – as IRAP/ISM often require data at rest to be encrypted (this can also be achieved by hosting the storage on encrypted filesystems or relying on cloud storage encryption). At the very least, ensure that any storage location (local disk or Azure Blob) used to persist outputs has encryption enabled (Azure storage accounts encrypt by default, and local servers should use disk encryption for sensitive data).
Secrets in Code: We found no hard-coded credentials or secrets in the repository, which is a positive finding. Environment variable usage is pervasive for sensitive values (Azure keys, tokens, etc.), and there’s even an .env.example provided to guide users[3]. The .env file is correctly git-ignored. This shows good security hygiene. Additionally, the Azure integrations use the DefaultAzureCredential when possible[18], meaning in an Azure environment the orchestrator can leverage managed identities or developer login for authentication without manual secret handling. This aligns well with best practices and ISM controls for credential management.
External Dependencies: The project lists its dependencies in pyproject.toml (and requires.txt for the egg-info). Key libraries include azure-identity>=1.15.0, azure-storage-blob>=12.19.0, requests>=2.31.0, openai>=1.12.0, pyyaml>=6.0, etc.[19][20]. All of these are recent versions (as of 2024) with no known critical vulnerabilities. We do not see obviously outdated packages. For example, Requests 2.31.0 is the latest in the 2.x series and patches earlier HTTP/SSL issues. Pyyaml 6.0 is the latest safe version. The optional azureml-core>=1.56.0 is a large library but only used if Azure ML telemetry is enabled; version 1.56.0 is relatively recent (April 2023) – though Azure ML has since moved toward newer SDKs, using this version is fine for now and ensures compatibility. Recommendation: Regularly run pip audit or similar on the dependencies to catch any future vulnerabilities. Also, consider pinning exact versions for deployment (currently they are minimums). For IRAP, maintaining a Software Bill of Materials (SBOM) and tracking dependency versions is important for vulnerability management. At present, no insecure dependency usage is evident.
API Communications: The orchestrator interacts with external APIs: Azure OpenAI, Azure Content Safety, GitHub, Azure DevOps, etc. All these interactions are done over HTTPS endpoints. By default, the code uses secure URLs (for instance, GitHub API base <https://api.github.com[21>], Azure DevOps base <https://dev.azure.com[22>], and Azure OpenAI endpoint which the user must configure likely as an https:// URI). The code does not explicitly enforce that endpoints must be HTTPS, so a misconfiguration could use an insecure scheme (for example, nothing stops a user from putting <http://localhost> as an API base during testing). For production, it’s critical that all endpoints are TLS-protected. The requests library will verify SSL certificates by default; there is no disabling of cert verification in the code, which is good. Recommendation: As a policy, ensure all configured service endpoints use https://. This can be documented or even enforced via validation (e.g., raise an error if endpoint in AzureContentSafetyMiddleware does not start with "https://" unless in a dev mode). For IRAP compliance, all data in transit should be encrypted.
The content of API requests may include sensitive data (prompts could contain sensitive text). The code sends these over HTTPS to OpenAI or Content Safety API, which is appropriate. For the Azure Content Safety middleware, if it fails to reach the service, it will by default abort the request unless configured to skip on error[23]. In skip mode it logs a warning that the safety check failed and continues without it[24]. This is a trade-off between availability and security. IRAP requires that security controls failures should be noticeable; the warning log satisfies that, but an IRAP assessor might prefer failing closed (which is the default behavior). We advise keeping the default on_error='abort' unless there is a compelling reason to allow operation without content safety scanning.
Sensitive Data Exposure: By design, the orchestrator processes potentially sensitive text (LLM prompts, outputs, etc.). We checked how well it guards this data. Notably, there is an audit logging middleware that can log prompts and metadata. The AuditMiddleware will log requests and responses at INFO level, including metadata and (if configured) the prompt content[25][26]. By default include_prompts is False, meaning it will not log the actual prompt content unless explicitly enabled. This is a smart default – it avoids dumping potentially sensitive user input or AI output into logs unless needed. We suggest retaining this default and, if logs must be more detailed, ensuring the log storage is secure (because prompts could include confidential information). The Content Safety middleware and Prompt Shield also log when they block or mask content (at WARNING level) including the term or category that was flagged[27][28]. They do not log the full prompt, only the fact of the violation and potentially the problematic term/category, which is a reasonable balance between security insight and exposure.
Output data exposure is something to consider as well. The orchestrator can write outputs to local disk (CSV, JSON, Excel, etc.) and to cloud (Azure Blob, GitHub repo). For local files, the project does not explicitly set file permissions; it relies on the OS defaults. On Linux, files will typically respect the user’s umask (which could make them world-readable if umask is permissive). For sensitive deployments, ensure the OS/file-system permissions are set such that output files are only readable by authorized accounts. This might mean adjusting the umask or using file ACLs if multiple users share the machine. When outputting to GitHub or Azure DevOps, consider that you are pushing data to an external service – those repositories should be private and access-controlled. The code uses tokens to push to GitHub/DevOps; it is important to treat those tokens with care (the code expects you to provide a GITHUB_TOKEN or AZURE_DEVOPS_PAT via env and never logs them, only uses for the Authorization header[29]). The pushes to GitHub are done in dry-run mode by default – i.e., it will not actually push unless --live-outputs is enabled or dry_run: false is set[30][31]. This default is another smart safety measure to prevent accidental publishing of data. In an IRAP context, this gives an opportunity to review what would be pushed (the orchestrator logs what files it would create on a dry run) before actually sending potentially sensitive data to an external system.
Insecure Functionality Disabled: We checked for any disabled or debug routes – none found. The code does not contain leftover debug backdoors or test endpoints. One thing we noted: the presence of an old/ directory with legacy code. These legacy scripts (old/main.py, etc.) are not used by the new system and in fact would error out if run[32]. They are essentially dead code kept for reference. While not directly a vulnerability, having unused code lying around can be risky if someone inadvertently runs it. We suggest removing or clearly marking the old/ folder as deprecated before production. The new orchestrator codebase stands on its own.
Overall, security posture of the code is solid: no obvious vulnerabilities in authentication (since none needed), strong input/config validation, no secrets leakage, and proactive measures like content filtering, audit logging, and artifact classification are built-in.
2. Code Quality & Best Practices
Code Structure & Modularity: The project is well-organized into logical modules. Core orchestration logic lives in src/elspeth/core/ and plugin implementations in src/elspeth/plugins/. This separation means new data sources or sinks can be added without modifying core logic – consistent with the plugin architecture described. The use of a central PluginRegistry to register plugins and schemas is a clean design[33][34]. The code uses Python 3.12 features (dataclasses, type hints with | union, etc.) appropriately, making it modern and readable. Key components (ExperimentRunner, ExperimentSuiteRunner, PromptEngine, etc.) are implemented as classes with clear responsibilities. In general, the structure follows best practices for a Python project (there’s a pyproject.toml, use of logging library, etc.).
One minor structural issue is the leftover old code, as mentioned. Removing that would reduce confusion. Otherwise, the files are nicely focused (the longest file is the suite_runner which is complex but understandably so, given it merges configs for potentially many experiments). The documentation (README and various notes/*.md) indicates a thoughtful approach to architecture. For instance, the AGENTS.md provides a project snapshot and roadmap[35][36], which shows an intent to keep contributors aligned – a positive from a maintainability perspective.
Language-Specific Best Practices: The code adheres to Python best practices in many ways. We see extensive use of type annotations, which improves clarity and helps with static analysis. For example, function signatures and dataclass fields are all typed (e.g., sinks: List[ResultSink] in ExperimentRunner[37]). The project also includes a Makefile with linting commands (using black and isort in check mode) and even a pre-commit hook setup[38]. This indicates the code style is automatically formatted and imports sorted, which generally means the code is consistent and easy to read. We did not spot any obvious PEP8 violations or code smells. Error handling uses exceptions appropriately (raising ConfigurationError for config issues, etc., rather than returning error codes). The logging is done via the logging module (no print statements), which is thread-safe and configurable.
One area to improve is function length/complexity in the ExperimentSuiteRunner where defaults are merged. The_run_suite function in the CLI and the build_runner method in ExperimentSuiteRunner have many steps to combine defaults, prompt packs, and experiment overrides[39][40]. While this is somewhat unavoidable given the flexibility needed, it makes those functions lengthy. It might be beneficial to refactor parts of that logic into smaller helper functions or dataclasses (for example, a class responsible for merging a specific aspect of config). This would aid readability and testing. However, since there are tests covering these (there are tests for config merging and plugin overrides[41][42]), it’s not a critical maintainability issue.
Error Handling & Logging: The code generally handles error conditions explicitly and fails safely. For instance, when validating configuration, they collect warnings and errors – then log warnings and prevent execution if there are errors[43]. At runtime, if any plugin raises an exception and its on_error is set to "skip", the orchestrator catches it and logs a warning, allowing the run to continue without that output[44]. This pattern is repeated in various sinks and middleware (e.g., content safety skip, Excel sink skip on error[45]). This is good for robustness. On the other hand, if on_error is "abort" (the default), exceptions propagate – which is what you want for critical failures.
One improvement would be in the CLI’s handling of unexpected exceptions. Right now, if something goes truly wrong inside the run (e.g., an uncaught exception in the ExperimentRunner), it will bubble up and likely produce a stack trace in the console. For a polished CLI tool, catching top-level exceptions and printing a user-friendly error message (or logging it and exiting with a code) would be better. Because this is primarily a tool for developers/analysts rather than end-users, a stack trace isn’t the worst outcome, but it’s worth noting. The CLI does already catch configuration errors and uses SystemExit for known bad inputs [like missing suite directory for certain commands][46](47).
Logging is thorough and strategically placed. We saw that each middleware and sink uses a logger [often with distinct channels like elspeth.audit, elspeth.prompt_shield, etc.][48](27). The docs/logging-standards.md lays out what should be logged at each level [INFO for key events, WARNING for skips/failures](49). Not every recommendation in that doc is fully implemented yet – for example, sinks log warnings on failure, but do they log success at INFO? Some sinks do not currently log a confirmation when they succeed (the Blob sink does an logger.info("Downloading blob ...") on start but no info on successful completion). It’s a minor point; adding an info log on successful writes (with destination path/URL) would align with the guidelines and help operations teams tracking outputs. Overall, logging is configurable by--log-level` flag, which is good (default INFO). The default INFO logging gives a nice high-level view without overwhelming details, which is a reasonable default for an orchestrator.
Code Duplication: There is minimal code duplication. The design uses factories and registration to avoid repeating plugin instantiation logic. We noticed a little duplication in how config merging is handled in CLI_run_suite vs SuiteRunner – some of that logic is similar (merging suite defaults and prompt packs). However, it’s not simple to unify without losing clarity, since one operates at the CLI argument level and one at the object level. Given that tests cover both, this is acceptable.
One duplicate effort is maintaining both YAML config and equivalent Python test configs (in test_config.py they write out a YAML text and then ensure load_settings returns expected objects[50][51]). This is actually good for testing but does mean any change in config format needs updating tests in two places. This is normal and the trade-off of strong tests. No unnecessary copy-pasted code was apparent.
Performance: The orchestrator is mostly I/O and network-bound (reading data, calling APIs). It uses a few techniques to improve performance where possible. For example, it can parallelize LLM calls per experiment using a ThreadPool if configured[52], and it allows concurrency configurations to limit how many threads to use, etc. By default, it appears to run sequentially unless you opt-in to concurrency. This is a safe default to avoid overwhelming APIs or running into rate limits by accident. The code also batches large blob uploads in chunks (4 MB chunks in BlobResultSink) to handle large payloads efficiently[53]. We didn’t notice any obvious inefficiencies like unnecessary deep copies or busy-wait loops.
One performance-related improvement could be memory usage when handling large datasets. The code loads the entire input CSV/Blob into a pandas DataFrame (df = settings.datasource.load() in_run_suite[54]). For very large inputs, this could be memory heavy. Since ExperimentRunner then iterates row by row, a streaming approach might be possible (processing chunks of the dataframe) – but that adds complexity especially with concurrency, and pandas itself isn’t great at streaming. Given typical use (LLM experiments on datasets probably not exceeding thousands of rows), this approach is fine. We just note that if someone attempted to use this on a massive dataset, they might hit RAM limits.
Maintainability: The code is fairly maintainable due to its clarity and testing. Classes and functions have descriptive names, and the use of dataclasses makes it easy to see what fields exist. The presence of a comprehensive test suite (61 tests, ~83% coverage[55]) greatly aids maintainability – refactoring can be done with confidence if tests are kept up to date. The documentation in notes/ is very helpful for a new developer to understand design decisions (for example, plugin-architecture.md and phase7-docs.md). There are also migration guides for legacy code. This level of documentation shows good practice in capturing context, which is important in a security-sensitive project (so new contributors don’t accidentally remove important checks).
One maintainability suggestion: ensure that security features are clearly documented for developers. For example, the classification (security_level) mechanism is a critical piece for IRAP, but currently it’s mostly in code and not much in the README. Developers should know that if they introduce a new sink or plugin that handles data, they ought to integrate it with the artifact security level system (declare what level it produces/consumes). Perhaps adding a section in the README or a dedicated security.md about using security_level in configs and code would be useful to avoid someone inadvertently bypassing that mechanism in the future.
Testing: The project includes unit tests for most components (plugins, config loading, CLI, etc.). Notably, there are tests for the security aspects: e.g., test_security_signing.py presumably covers the HMAC, test_llm_middleware.py covers logging of heartbeats and content filtering, test_outputs_signed.py covers the signed artifact sink, etc. This is excellent, as it ensures security features remain functional after changes. The tests also check for failure modes (for instance, a config missing prompts should raise a ConfigurationError – and indeed the code does that[7] and we expect a test asserts that). We ran a quick check and saw tests for Azure content safety and prompt shield behaviors, which is great for security regression. The coverage could be even higher by adding integration-style tests (e.g., run a small suite end-to-end and assert on logs and outputs), but given the scope, the current tests are adequate.
Documentation Gaps: User-facing documentation is quite good on how to use the tool (README’s Quick Start, configuration overview, etc.). However, for an IRAP assessment, additional documentation could be provided around security features and operational guidance. For example, documenting how the classification levels map to government classifications (the code has levels: unofficial, official, official-sensitive, secret, top-secret[56] – which align with UK/AUS government terms) and how to use them in experiment config would be very useful for operators. Also, guidance on deploying the orchestrator in a secured environment (such as “run it on an IRAP-certified Azure VM, with these environment variables, ensure the output repo is private, etc.”) would help bridge the gap between code and policy. These are not code issues per se, but documentation is part of deliverable quality. Since IRAP will involve auditors checking not just code but how it’s used, including a security-focused README section or hardening guide would be beneficial.
In summary, code quality is high. The few improvements we suggest (sanitizing Excel outputs, removing dead code, catching top-level exceptions more gracefully, logging enhancements, and augmenting documentation) are mostly small tweaks. The code is clearly written, idiomatic, and shows signs of careful thought and review.
3. IRAP-Specific Concerns
The IRAP (Information Security Registered Assessors Program) assessment will check compliance with the Australian Government ISM controls. Below we address how this codebase and its usage relate to key areas:
Compliance with ISM Controls: Many ISM controls revolve around secure development and deployment practices. On the development side, the project shows compliance with secure coding practices (input validation, no hard-coded credentials, etc.). On the deployment side, the orchestrator will inherit the compliance of its environment. For example, if this is run on Azure, one must ensure the Azure infrastructure meets IRAP requirements (using IRAP-certified Azure regions, applying hardening to the VM or container, etc.). The code itself is designed to facilitate compliance:
• Credential Management: As noted, secrets are pulled from environment variables or Azure Managed Identity, which aligns with ISM guidance to avoid embedding credentials and to use secure identity services where possible[3]. Ensure that in deployment, environment variables (like ELSPETH_AZURE_OPENAI_KEY) are injected via a secure mechanism (Azure Key Vault, pipeline secrets) and not stored in plaintext on disk. Also, for Azure OpenAI, it’s recommended to use Azure AD authentication if possible (the OpenAI SDK might not support that yet, so an API key is used – which is acceptable but treat that key with highest sensitivity).
• Least Privilege: The orchestrator should run under an identity with only necessary permissions. For instance, if running in Azure ML or a VM, use a managed identity that can only access the specific storage account container needed and the OpenAI service – nothing more. The DefaultAzureCredential approach makes it easy to assign limited permissions via Azure AD roles rather than using a broad account key. Similarly, the GitHub and DevOps PATs used should have minimal scopes (just repo contents access on the specific repo, not full org access). This limits impact if those tokens leak. Nothing in the code inherently violates least privilege; it’s about configuration.
• Secure Configurations: By default, the orchestrator does not open any network ports or require any inbound connections, which simplifies compliance (no need for host-based firewalls changes). Outbound connections are made to known services (Azure, OpenAI, etc.). It would be wise to restrict the runtime environment’s egress to only those endpoints. For example, if running on a server, use NSGs or firewall rules to allow connections only to *.azure.com and the OpenAI endpoints needed. This mitigates the risk of an attacker redirecting outputs or prompts to an unauthorized server (e.g., by tricking someone to set api_base to a malicious URL – not likely in this scenario, but a defense-in-depth measure). The code does not have a feature to enforce allowed hostnames, so this is an operational control.
• Patching: Ensure the environment uses updated Python and that dependencies are kept up to date. We saw that dependencies are modern; maintaining them (perhaps pinning and regularly scanning for CVEs) will be an IRAP concern.
Data Sovereignty & Storage: IRAP requires controlling where data is stored and processed. The orchestrator can store data locally (on the VM where it runs) and in configured sinks (Azure Blob, GitHub, etc.). It’s crucial to ensure all these storage locations are within approved jurisdictions (e.g., within Australia for PROTECTED data). The code itself allows flexibility – it will happily push data wherever configured. It’s up to deployment to configure only approved targets. Concretely: - If using Azure Blob storage, use an Azure region that is IRAP certified (Australia East, for example, is IRAP assessed for PROTECTED data). - If using GitHub as a sink, note that GitHub is a global service (data may be stored in the US). Pushing potentially sensitive experiment results to GitHub.com might not satisfy IRAP requirements unless the data is unclassified or GitHub has suitable agreements. This is a risk: someone might accidentally use the GitHubRepoSink for sensitive data. Mitigation: disable or avoid that sink for classified data, or use an Azure DevOps repository on an Australian tenant (Azure DevOps can be configured for data residency in Australia). The code supports an Azure DevOps sink which is likely more appropriate for government (since one can ensure the org is in Australia). So, choose sinks in compliance with data sovereignty needs. We recommend documenting this in an “IRAP deployment guide” – e.g., “For OFFICIAL:SENSITIVE or above, do not use GitHub sink; use Azure Blob or DevOps with Aussie region and appropriate encryption.”
• The local output files (CSV, Excel) – if the orchestrator runs on an Azure VM in Australia, that’s fine (data stays on that VM). But if the VM itself isn’t secured or is in an uncertified region, that’s an issue. So again, the environment matters.
The classification mechanism in the code (security_level fields) is a great feature here. It allows tagging of data and ensures, for example, that a Top-Secret artifact cannot be passed into a sink that is only cleared for Official data[57]. The code will raise an error if such a configuration is attempted, effectively preventing an inappropriate data flow. This directly addresses ISM controls around data segregation. For example, if an experiment is marked official-sensitive and a sink is not marked at that level or higher, the orchestrator stops with a security exception. This is a strong control and somewhat unique – it shows the designers have IRAP in mind. Ensure that everyone configuring the system uses this feature correctly: every sink in config/settings.yaml should have a security_level set (or inherit from suite defaults) appropriate to where it sends data. The default if not set is “unofficial”[58] which is the lowest level – meaning if you forget to set it, the sink will only accept Unofficial data. That fails closed (it will error out if you try to send Official data to it) – a good default. But to avoid confusion, explicitly set levels for clarity. For compliance, maintain a mapping of these levels to Australian classifications (they mostly match: “official-sensitive” would correspond to PROTECTED perhaps, “secret” and “top-secret” obviously to those levels). An assessor might ask how those labels align; be prepared to explain or adjust naming if needed.
• The SignedArtifactSink can also aid compliance by ensuring output files have an audit trail (signature and manifest including a digest[16][59]). This is useful for data integrity, but also note it’s generating a JSON manifest that includes possibly the classification (security_level) in it. Make sure those manifests are protected similarly (they are stored alongside the data).
Audit Logging & Monitoring: The orchestrator generates detailed logs that can feed into audit trails. IRAP will expect that significant events (start/stop of processing, security relevant events) are logged and that logs are retained and monitored. The orchestrator logs: - Each LLM call (metadata, result metrics) via AuditMiddleware[25]. - Content filtering events (prompt shield or content safety triggers) at WARNING level[27][28]. - When experiments complete, the CLI logs an info message with how many rows were processed[60]. - Failures are collected and included in the final payload (and also could be logged depending on logging level).
To leverage this for audit: - Ensure logs from wherever this runs are collected to a secure log store (Azure Monitor, Splunk, etc.) and retained as per ISM requirements (e.g., 7 years for certain audit logs). - The structured logging makes it easier to parse if needed (some log entries are JSON-like as per logging standards doc[61]). Consider configuring the logger to output JSON lines, which would be SIEM-friendly (currently, it appears to just embed JSON in the message string). Minor config change if needed. - The AzureEnvironmentMiddleware, if enabled, logs experiment metadata to the Azure ML Run history when running in Azure ML[62][63]. This could be an additional audit trail: metrics and parameters stored in Azure ML. If IRAP requires centralized monitoring, integrating with Azure’s logging (Application Insights or Log Analytics) might be something to do; the code doesn’t directly do AppInsights, but logs can be picked up by agents if the VM has them.
Incident Response Readiness: In the context of this tool, incident response would be invoked if, say, sensitive data was found where it shouldn’t be, or if an unauthorized change was made to the orchestrator or its outputs. The code by itself doesn’t have incident response features, but it does facilitate detection of certain incidents: - If someone attempted to misuse the tool (e.g., sending Top-Secret data to an Unofficial sink), the tool will prevent it and throw an error – essentially alerting the operator immediately to a security policy violation. That error could be caught in logs, and you may treat it as a security incident (why was someone trying to downgrading data classification?). - If an LLM returns content that violates content policy (hate, violence, etc.), the content safety middleware will flag it[28]. That could be considered an incident (model output unacceptable), and since it’s logged, it’s on record. Operators should review such warnings and possibly adjust prompts or take action. - We recommend developing an incident response plan outside of the code: e.g., “if an API key is leaked, rotate it and update env; if logs show a blocked content or classification error, notify security team,” etc. The codebase provides the hooks (warnings, errors) to know when something is up.
From a system perspective, to prepare for incidents: - Keep the environment patched (OS and Python libs) to reduce incident likelihood. - Regularly review audit logs generated by the orchestrator for anomalies (e.g., unusual prompt content, repeated failures, unauthorized sink attempts). - Ensure that if the orchestrator is part of an automated pipeline, any failure triggers (especially security-related failures) are surfaced and not just hidden. The CLI exits with non-zero status on error, which is good for pipeline integration (e.g., a pipeline can fail if orchestrator fails).
Access Control Mechanisms: We discussed the lack of multi-user login, but the classification system acts as an internal access control for data flows. In practice, the orchestrator will be run by a single user or service principal, so external access control is mainly about who can run it and who can read its outputs. Protect the repository and environment: presumably this is a private repo (which it is, under tachyon-beep) so access to the code is limited. Only give the ability to execute the orchestrator to trusted personnel or automation. If running as a service, consider restricting access to its execution endpoints (though currently it’s just a CLI, not a long-running service).
If in the future this tool were to run as a service (e.g., behind an API), you’d need to build auth into it. But as is, the operator’s system access is the gatekeeper.
Data Encryption (Rest & Transit): We already noted transit encryption (HTTPS everywhere) and rest encryption. To reiterate specifically for ISM: - All sensitive data at rest should be encrypted. For Azure Blob, Microsoft-managed keys encryption is on by default (that covers data at rest in blob). For additional assurance, one can use customer-managed keys on the storage account – that’s outside the code scope, but supported by Azure. - Local data at rest (the outputs directory, any .venv with cached data, etc.) should reside on encrypted disks if the machine handles classified data. This typically means using OS-level full disk encryption (BitLocker, LUKS, etc., or Azure Disk Encryption for VMs). The code can also produce encrypted archives if needed (one could add a plugin to PGP-encrypt an artifact, for instance, if required – not currently present, but an idea for extreme cases). - Backups of data (if any) should also be encrypted and protected.
One thing to consider: the content passing through the orchestrator (prompts and completions) resides in memory and possibly in swap or temporary files if any. We did not find evidence of it writing temp files with sensitive data (it streams output directly to sinks). The pandas DataFrame holds input data in memory. Ensure the host uses encrypted swap if any, and that when the process terminates, no dumps or caches of data remain. The signing sink writes a manifest including possibly a SHA256 digest of results[64]; a hash of sensitive data is itself not sensitive if the data is large and hash can’t be reversed, but just be mindful that even metadata in the manifest could reveal something (they include row count, maybe a truncated view of failures, etc.). All outputs should be handled at the appropriate classification level.
Miscellaneous ISM Controls: - Change Management: With such a security-sensitive tool, changes to code should be reviewed and tested (the repository seems to follow good practice here). Use PRs and code reviews to ensure no security regression – which appears to be done given the detailed commit messages and notes. - Protective Monitoring: As mentioned, use the logs for monitoring. Perhaps integrate with Azure Monitor metrics – e.g., the HealthMonitorMiddleware provides a heartbeat with request counts and failure rates[65][66]. Those could be scraped to detect if LLM failures are spiking (maybe an indicator of an issue). - Secure by Default: Many defaults in the code are secure (prompts filtering off by default might be argued either way, but abort on violation is default on filtering; dry_run for external sinks default; classification default to unofficial which is safe). Make sure when distributing this tool internally, the configuration examples given are also secure by default. For instance, the sample settings use the CSV sink (local file) which is fine[67], and they show environment variables for keys, also fine. Just ensure that none of the sample or default config encourages something like writing to a public endpoint or using a broad token.
Summary (IRAP Readiness): Architecturally, the orchestrator is on track to meet IRAP requirements when deployed correctly. The codebase itself has implemented features aligning to ISM controls: e.g., classification enforcement, audit logging, no hard-coded creds, etc. The main tasks to achieve IRAP compliance will be in deployment configuration and documentation: - Use approved infrastructure (IRAP-certified cloud regions or on-prem as required). - Configure all sinks and credentials with security in mind. - Possibly disable or remove any plugin that would violate policy (e.g., disallow use of GitHub sink for classified info). - Provide an accreditation package with code review (this report), test results, and operational guides. The code review part is positive – no showstoppers found.
4. Risk Assessment
We have identified and categorized the key risks/issues:
• (Medium) Excel/CSV Formula Injection: As described, exporting results to Excel or CSV without sanitization could allow an attacker who somehow controls the input data to inject malicious formulas. This could lead to code execution or data exfiltration when an analyst opens the file. Impact: If orchestrated data is shared or opened on a machine with access to sensitive networks, a malicious formula could compromise that machine or leak data. Likelihood: Low in tightly controlled data scenarios, but not negligible if input data isn’t fully trusted. Mitigation: Sanitize outputs or open files in safe modes. Severity: Medium (not an application compromise, but a potential client-side attack on users of the output).
• (Medium) Misconfiguration leading to Data Leak: The flexibility of config means a user could accidentally send sensitive data to an inappropriate sink (e.g., pushing Official-Sensitive data to a public GitHub repo). Thanks to the security_level enforcement, the orchestrator should catch many of these cases (if correctly labeled). But if someone neglects to label an experiment or sink with the right level, there is a risk of misrouting data. Impact: Sensitive data could be stored in an insecure location (public or lower classification environment), violating compliance. Likelihood: Moderate – human error in config is possible. The tool’s defaults mitigate this by failing if classifications don’t match, provided those classifications are set. If a user completely ignores the classification feature (everything defaults to unofficial), the orchestrator will run but then a lot depends on trusting the user to manually not configure a bad sink. Mitigation: Make classification mandatory in config for high-sensitivity deployments (e.g., have the loader require a security_level on each sink in a “secure mode”). Additionally, restrict which sinks can even be used in such environments (maybe remove the GitHub sink plugin code or block it via config policy). Severity: Medium (because the tool has measures – if used properly, risk is low; if not, impact is high).
• (Low) Logging Sensitive Information: While by default prompts are not logged, if an operator enables debug or include_prompts, full prompt and response content (which could be sensitive) will be written to logs[26]. Logs might be less protected than the primary data (e.g., aggregated in a SIEM or accessible to admins). Impact: Potential exposure of sensitive content to personnel who shouldn’t see it (like IT admins) or in breach of data separation if logs from multiple classifications aggregate. Likelihood: Low by default; only occurs if someone turns on that setting or runs at DEBUG. Mitigation: Use caution in enabling verbose logging for production runs with real data. If detailed logs are needed for debugging, scrub them afterwards or use a separate lower-classification dataset for those tests. Could also implement redaction of certain fields in logs if needed. Severity: Low (under default settings, not an issue).
• (Low) Legacy Code Usage: The presence of old/main.py and related scripts poses a slight risk that someone might execute them out of confusion. Those old scripts lack the new security controls (no classification checks, etc.) and also might be broken. Impact: If someone ran the old orchestrator code by mistake, they could get incorrect results or miss the security features of the new code. It probably wouldn’t run to completion anyway (since it depends on missing modules), so the risk is more about confusion and lost time than security breach. Likelihood: Low (the README clearly points to using python -m elspeth.cli, not old/main.py). Mitigation: Remove or archive the old directory. If it must remain, rename it to legacy_reference and add a WARN in those files to not use them. Severity: Low.
• (Low) Denial of Service via Large Inputs or High Concurrency: An attacker with access to provide input data (or a misconfiguration) could supply an extremely large dataset or very high concurrency settings, attempting to exhaust resources. The orchestrator could consume a lot of memory (reading a huge CSV) or overwhelm the OpenAI API (leading to throttling or large bills). Impact: Loss of availability, potential cost impact, but not data loss. Likelihood: Low in our context (inputs come from controlled sources and concurrency defaults are safe). Mitigation: Impose some limits on input size (perhaps document recommended max rows) and monitor resource usage. Using concurrency_config thresholds (which exist) can prevent runaway thread spawning. Severity: Low.
• (Low) Third-Party API Abuse: If the OpenAI API key were compromised or if someone injects prompts to cause the model to output sensitive info, etc., those are more operational risks than code issues. The content safety middleware partially addresses prompt content risks by blocking certain categories[28]. One gap is that model outputs are not run through content safety – only inputs. If output moderation is needed (to avoid returning, say, classified info or inappropriate text), that might have to be added (OpenAI’s API has a moderation endpoint for outputs too). Impact: The model could return something disallowed and if no one notices, it might be written to outputs. Likelihood: Depends on use; likely low if prompts are well-crafted. Mitigation: Possibly add an output moderation plugin, or rely on manual review of outputs in high-stakes cases. Severity: Low.
• (High) – (None identified): We did not find any critical or high severity vulnerabilities in the code. There is no scenario we identified where an external adversary could directly compromise the system through the application. Most issues are internal or misconfiguration concerns. The design has security in depth (multiple checks and safe defaults). Therefore, the highest risks are around policy compliance (ensuring usage aligns with rules) rather than code flaws.
To summarize severity: No critical remote code execution or auth bypass issues were found. The most urgent things to fix are relatively moderate-hardening tasks (Excel sanitization, enforcing config discipline). The potential business impact of not addressing these: - The Excel injection could impact analysts or decision makers who use the experiment reports, possibly leading to a compromised endpoint (which in a gov network is a big deal, but it requires an adversary to have already inserted payload into the data – a multi-step attack). - Misconfigurations without using the classification safety net could lead to data ending up in the wrong place, which is a compliance breach and could be business-critical (reputation damage, regulatory consequences). However, the product already has the classification feature to prevent this – it’s more about using it correctly.
Thus, the most urgent “security risk” is ensuring operational processes use the tool correctly, rather than a fix in code. We will address that in recommendations.
5. Actionable Recommendations
To further harden the codebase and its usage for IRAP assessment, we recommend the following steps, in priority order:
1. Implement Output Sanitization (Priority: High): Mitigate the Excel/CSV injection risk by sanitizing any cell that begins with a special formula character. This can be done when writing rows in ExcelResultSink._populate_results_sheet and similarly for the CSV sink (if any). A simple approach is to prefix an apostrophe (') to any cell value starting with =, +, -, or @. Excel will then treat it as a literal. Verify that this doesn’t break legitimate content (it shouldn’t, except in the rare case a prompt actually needs to start with "=" which is unlikely). This change protects anyone opening analysis results. Given that many IRAP consumers will be using these outputs in Excel, it’s an important fix.
2. Remove Legacy Code & Artifacts (Priority: High): Eliminate the old/ directory and any obsolete documentation referencing it. This reduces confusion and ensures all users are running the new, secure code. If some legacy functionality isn’t ported yet but needed, port it or clearly mark it as do not use. Having only one main code path will simplify the security assessment and maintenance.
Additionally, review the repository for any sample files or test outputs that contain actual data or secrets. We saw an outputs/latest_results.csv in the repo – ensure it contains only mock data (if not, remove it or move to gitignored outputs). Keeping the repo free of any real data is important for IRAP (no chance of an accidental classified data commit).
3. Enforce Security Level Usage (Priority: Medium): Update documentation and possibly code to ensure that the security_level feature is always used correctly. In the code, you might add a validation step in validate_settings to require that each sink in config has a security_level field. Currently, if omitted, it defaults to "unofficial", but a user might not realize they need to set it. Making it explicit will prevent mistakes. Document to users how to decide the level for each sink and experiment, mapping to their data classification. Consider providing an example “secure settings” where all sinks have levels and maybe restricting available plugins (for instance, if running in PROTECTED environment, one might say only azure_blob and signed_artifact sinks are allowed, not GitHub – this could be enforced by code or convention).
If multiple levels of data will be processed (e.g., some experiments Official, some Secret), ensure that is documented and that the orchestrator should ideally be run separately for different domains (since it’s not multi-tenant or multi-level by itself – though the classification system could allow mixed, in practice separation of networks is easier). Clarify that in docs.
4. Harden Repository and DevOps Integration (Priority: Medium): Since this is likely going through an IRAP audit, also consider the following: - Enable branch protection and code review requirements on the repo (if not already) so that any future changes get a second set of eyes for security impact. - Set up dependency scanning (GitHub Dependabot or similar) so you are alerted to any new vulnerabilities in libraries. - Ensure CI pipelines (if any) do not expose secret env vars to untrusted code (e.g., tests that run on PRs from external contributors should not use real secrets). - If delivering this to a client or another team, provide a secure configuration guide that includes steps like “rotate API keys regularly”, “use Managed Identities vs. secrets where possible”, etc. This goes beyond code but will be looked at in an assessment.
5. Logging and Monitoring Improvements (Priority: Medium): Implement some of the logging best practices that were outlined but not yet done: - After a sink successfully writes data, log an INFO message indicating what was written and where (e.g., “BlobResultSink: uploaded results.json to container X/path Y” or “ExcelResultSink: saved workbook at path Z”). This provides positive confirmation in logs and helps auditors trace data movement[49]. - In the classification enforcement, when a PermissionError is raised because of a security level mismatch[57], consider logging that event explicitly as a security incident (at ERROR level, since it’s a policy violation). Right now it will raise and likely be caught by the CLI resulting in an exception print. Logging it with a clear message (“Denied data flow: Experiment A (Secret) -> Sink B (Official)”) would make it stand out in logs and assist in auditing. - Ensure all loggers are using consistent formatting (they appear to). Maybe configure a global logging format that includes timestamp, and maybe the classification level of the run if possible. The latter could be overkill, but timestamp is definitely needed (the basicConfig likely adds it or can be configured to). - Set up monitoring alerts: for example, if any WARNING/ERROR occurs (content violation, skip of an output, etc.), it should alert the ops team. This isn’t a code change but something to implement in Azure Monitor or whatever logging solution is used.
6. Enhance Documentation for IRAP (Priority: Medium): Create a dedicated section in the README or a separate HARDENING.md that addresses: - How to deploy the orchestrator in an IRAP-compliant manner (region selection, network config, using Key Vault for secrets, etc.). - How to use the security_level settings properly when writing config. - Warnings about not pushing classified outputs to GitHub, etc., and guidance to use appropriate sinks (Azure Blob with proper access controls, etc.). - How to handle logs that may contain sensitive data (e.g., log retention and destruction policies). - Incident response procedures related to the orchestrator (e.g., if a content filter triggers repeatedly, who to notify).
This documentation will not only help users but also show IRAP assessors that you have considered these aspects.
7. Security Feature Additions (Priority: Low, Future): These are not required, but could be considered: - Add an output content moderation plugin: analogous to AzureContentSafetyMiddleware but for the LLM’s response. Azure Content Safety service or OpenAI’s moderation API could be used on outputs. This would ensure that not only prompts but also model outputs are screened (in case the model says something that shouldn’t be saved). If the use case is internal analysis, this might be overkill. But for thoroughness, it's an idea. - Add an encryption output sink: perhaps extend SignedArtifactSink or create a new EncryptedArtifactSink that PGP-encrypts the results.json (and any manifest) with a given public key. That way, even if those files were stored in a lower-class environment, they’re cryptographically protected. This could be useful if, say, the orchestrator VM handles Secret data but needs to drop results in a location only accessible to someone with the key. This is complex and may not be needed if infrastructure is properly isolated, but it’s something to think about for defense-in-depth. - Tighten allowable hosts: For example, in the AzureOpenAI client, ensure that azure_endpoint config begins with "https://" and perhaps ends with an approved domain (like .openai.azure.com). Right now a mis-set endpoint could theoretically direct data to an arbitrary server. It’s a corner case and would require someone intentionally setting a wrong endpoint, but adding a simple check could prevent mistakes (or intentional misuse if someone with config access went rogue). - Provide a mechanism to scrub or tokenize sensitive data in logs. For instance, if logging prompts, maybe allow a regex or list of fields to redact (the code doesn’t currently have any specific redactions, it either logs or doesn’t). If an org wants to log everything but still protect PII, they might integrate a custom middleware or log processor. This is outside the core need but mentioning for completeness.
8. Infrastructure & Deployment (Priority: depends on context): Ensure the runtime environment is secured: - If on a VM: apply CIS benchmarks, disable unnecessary services, use OS-level firewall, enable logging/monitoring agent, etc. - If containerized: use a minimal base image, don’t run container as root, etc. - Rotate the OpenAI API keys and other secrets regularly and have a process for that. - For Azure Blob storage, consider using short-lived SAS tokens generated just-in-time instead of long-lived credentials in config. The code can accept a sas_token in blob config; you could generate that at run time via Azure AD. This limits exposure if config files are somehow accessed.
In terms of priority: The immediate code changes (Excel sanitization, logging tweaks, removing old code) should be done first as they directly improve security and clarity with minimal effort. Documentation and configuration enforcement come next, as they ensure correct use. The system-level recommendations (deployment, keys, etc.) are equally important but would be handled by the DevOps/security team in tandem with finalizing the code.
By addressing the above items, the project will be well-hardened for the IRAP assessment. Many of these are already partly in place, and the remainder are refinements or operational considerations. The result will be a secure, well-documented LLM orchestration platform that auditors can have confidence in and that engineers can use without stumbling into security pitfalls.

________________________________________
[1] [2] azure_openai.py
<https://github.com/tachyon-beep/dmp/blob/e562e734e2f0293a5b8d1e04528078426b19ebfa/src/elspeth/plugins/llms/azure_openai.py>
[3] [32] [67] 2024-05-Initial-Assessment.md
<https://github.com/tachyon-beep/dmp/blob/e562e734e2f0293a5b8d1e04528078426b19ebfa/notes/2024-05-Initial-Assessment.md>
[4] [10] [11] [18] blob_store.py
<https://github.com/tachyon-beep/dmp/blob/e562e734e2f0293a5b8d1e04528078426b19ebfa/src/elspeth/datasources/blob_store.py>
[5] [6] [33] [34] registry.py
<https://github.com/tachyon-beep/dmp/blob/e562e734e2f0293a5b8d1e04528078426b19ebfa/src/elspeth/core/registry.py>
[7] [40] suite_runner.py
<https://github.com/tachyon-beep/dmp/blob/e562e734e2f0293a5b8d1e04528078426b19ebfa/src/elspeth/core/experiments/suite_runner.py>
[8] engine.py
<https://github.com/tachyon-beep/dmp/blob/e562e734e2f0293a5b8d1e04528078426b19ebfa/src/elspeth/core/prompts/engine.py>
[9] [45] excel.py
<https://github.com/tachyon-beep/dmp/blob/e562e734e2f0293a5b8d1e04528078426b19ebfa/src/elspeth/plugins/outputs/excel.py>
[12] [13] [44] [53] blob.py
<https://github.com/tachyon-beep/dmp/blob/e562e734e2f0293a5b8d1e04528078426b19ebfa/src/elspeth/plugins/outputs/blob.py>
[14] signing.py
<https://github.com/tachyon-beep/dmp/blob/e562e734e2f0293a5b8d1e04528078426b19ebfa/src/elspeth/core/security/signing.py>
[15] [16] [17] [59] [64] signed.py
<https://github.com/tachyon-beep/dmp/blob/e562e734e2f0293a5b8d1e04528078426b19ebfa/src/elspeth/plugins/outputs/signed.py>
[19] [20] requires.txt
<https://github.com/tachyon-beep/dmp/blob/e562e734e2f0293a5b8d1e04528078426b19ebfa/src/elspeth.egg-info/requires.txt>
[21] [22] [29] [30] repository.py
<https://github.com/tachyon-beep/dmp/blob/e562e734e2f0293a5b8d1e04528078426b19ebfa/src/elspeth/plugins/outputs/repository.py>
[23] [24] [25] [26] [27] [28] [48] [65] [66] middleware.py
<https://github.com/tachyon-beep/dmp/blob/e562e734e2f0293a5b8d1e04528078426b19ebfa/src/elspeth/plugins/llms/middleware.py>
[31] [39] [43] [46] [47] [54] [60] cli.py
<https://github.com/tachyon-beep/dmp/blob/e562e734e2f0293a5b8d1e04528078426b19ebfa/src/elspeth/cli.py>
[35] [36] [55] AGENTS.md
<https://github.com/tachyon-beep/dmp/blob/e562e734e2f0293a5b8d1e04528078426b19ebfa/AGENTS.md>
[37] [52] runner.py
<https://github.com/tachyon-beep/dmp/blob/e562e734e2f0293a5b8d1e04528078426b19ebfa/src/elspeth/core/experiments/runner.py>
[38] README.md
<https://github.com/tachyon-beep/dmp/blob/e562e734e2f0293a5b8d1e04528078426b19ebfa/README.md>
[41] [42] [50] [51] test_config.py
<https://github.com/tachyon-beep/dmp/blob/e562e734e2f0293a5b8d1e04528078426b19ebfa/tests/test_config.py>
[49] [61] logging-standards.md
<https://github.com/tachyon-beep/dmp/blob/e562e734e2f0293a5b8d1e04528078426b19ebfa/docs/logging-standards.md>
[56] [58] __init__.py
<https://github.com/tachyon-beep/dmp/blob/e562e734e2f0293a5b8d1e04528078426b19ebfa/src/elspeth/core/security/__init__.py>
[57] artifact_pipeline.py
<https://github.com/tachyon-beep/dmp/blob/e562e734e2f0293a5b8d1e04528078426b19ebfa/src/elspeth/core/artifact_pipeline.py>
[62] [63] middleware_azure.py
<https://github.com/tachyon-beep/dmp/blob/e562e734e2f0293a5b8d1e04528078426b19ebfa/src/elspeth/plugins/llms/middleware_azure.py>
