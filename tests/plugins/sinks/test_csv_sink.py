"""Tests for CSV sink plugin."""

import csv
import hashlib
from pathlib import Path

import pytest

from elspeth.plugins.context import PluginContext
from elspeth.plugins.protocols import SinkProtocol

# Dynamic schema config for tests - PathConfig now requires schema
DYNAMIC_SCHEMA = {"fields": "dynamic"}


class TestCSVSink:
    """Tests for CSVSink plugin."""

    @pytest.fixture
    def ctx(self) -> PluginContext:
        """Create a minimal plugin context."""
        return PluginContext(run_id="test-run", config={})

    def test_implements_protocol(self) -> None:
        """CSVSink implements SinkProtocol."""
        from elspeth.plugins.sinks.csv_sink import CSVSink

        sink = CSVSink({"path": "/tmp/test.csv", "schema": DYNAMIC_SCHEMA})
        assert isinstance(sink, SinkProtocol)

    def test_has_required_attributes(self) -> None:
        """CSVSink has name and input_schema."""
        from elspeth.plugins.sinks.csv_sink import CSVSink

        assert CSVSink.name == "csv"
        # input_schema is now set per-instance based on config
        sink = CSVSink({"path": "/tmp/test.csv", "schema": DYNAMIC_SCHEMA})
        assert hasattr(sink, "input_schema")

    def test_write_creates_file(self, tmp_path: Path, ctx: PluginContext) -> None:
        """write() creates CSV file with headers."""
        from elspeth.plugins.sinks.csv_sink import CSVSink

        output_file = tmp_path / "output.csv"
        sink = CSVSink({"path": str(output_file), "schema": DYNAMIC_SCHEMA})

        sink.write([{"id": "1", "name": "alice"}], ctx)
        sink.flush()
        sink.close()

        assert output_file.exists()
        with open(output_file) as f:
            reader = csv.DictReader(f)
            rows = list(reader)

        assert len(rows) == 1
        assert rows[0]["id"] == "1"
        assert rows[0]["name"] == "alice"

    def test_write_multiple_rows(self, tmp_path: Path, ctx: PluginContext) -> None:
        """Multiple writes append to CSV."""
        from elspeth.plugins.sinks.csv_sink import CSVSink

        output_file = tmp_path / "output.csv"
        sink = CSVSink({"path": str(output_file), "schema": DYNAMIC_SCHEMA})

        sink.write([{"id": "1", "name": "alice"}], ctx)
        sink.write([{"id": "2", "name": "bob"}], ctx)
        sink.write([{"id": "3", "name": "carol"}], ctx)
        sink.flush()
        sink.close()

        with open(output_file) as f:
            reader = csv.DictReader(f)
            rows = list(reader)

        assert len(rows) == 3
        assert rows[2]["name"] == "carol"

    def test_custom_delimiter(self, tmp_path: Path, ctx: PluginContext) -> None:
        """Support custom delimiter."""
        from elspeth.plugins.sinks.csv_sink import CSVSink

        output_file = tmp_path / "output.csv"
        sink = CSVSink({"path": str(output_file), "delimiter": ";", "schema": DYNAMIC_SCHEMA})

        sink.write([{"id": "1", "name": "alice"}], ctx)
        sink.flush()
        sink.close()

        content = output_file.read_text()
        assert ";" in content
        assert "," not in content.replace(",", "")  # No commas except possibly in data

    def test_close_is_idempotent(self, tmp_path: Path, ctx: PluginContext) -> None:
        """close() can be called multiple times."""
        from elspeth.plugins.sinks.csv_sink import CSVSink

        output_file = tmp_path / "output.csv"
        sink = CSVSink({"path": str(output_file), "schema": DYNAMIC_SCHEMA})

        sink.write([{"id": "1"}], ctx)
        sink.close()
        sink.close()  # Should not raise

    def test_flush_before_close(self, tmp_path: Path, ctx: PluginContext) -> None:
        """Data is available after flush, before close."""
        from elspeth.plugins.sinks.csv_sink import CSVSink

        output_file = tmp_path / "output.csv"
        sink = CSVSink({"path": str(output_file), "schema": DYNAMIC_SCHEMA})

        sink.write([{"id": "1"}], ctx)
        sink.flush()

        # File should have content before close
        content = output_file.read_text()
        assert "id" in content
        assert "1" in content

        sink.close()

    def test_batch_write_returns_artifact_descriptor(self, tmp_path: Path, ctx: PluginContext) -> None:
        """write() returns ArtifactDescriptor with content hash."""
        from elspeth.contracts import ArtifactDescriptor
        from elspeth.plugins.sinks.csv_sink import CSVSink

        output_file = tmp_path / "output.csv"
        sink = CSVSink({"path": str(output_file), "schema": DYNAMIC_SCHEMA})

        artifact = sink.write([{"id": "1", "name": "alice"}], ctx)
        sink.close()

        assert isinstance(artifact, ArtifactDescriptor)
        assert artifact.artifact_type == "file"
        assert artifact.content_hash  # Non-empty
        assert artifact.size_bytes > 0

    def test_batch_write_content_hash_is_sha256(self, tmp_path: Path, ctx: PluginContext) -> None:
        """content_hash is SHA-256 of file contents."""
        from elspeth.plugins.sinks.csv_sink import CSVSink

        output_file = tmp_path / "output.csv"
        sink = CSVSink({"path": str(output_file), "schema": DYNAMIC_SCHEMA})

        artifact = sink.write([{"id": "1", "name": "alice"}], ctx)
        sink.close()

        # Compute expected hash from file
        file_content = output_file.read_bytes()
        expected_hash = hashlib.sha256(file_content).hexdigest()

        assert artifact.content_hash == expected_hash

    def test_batch_write_multiple_rows(self, tmp_path: Path, ctx: PluginContext) -> None:
        """Batch write handles multiple rows."""
        from elspeth.plugins.sinks.csv_sink import CSVSink

        output_file = tmp_path / "output.csv"
        sink = CSVSink({"path": str(output_file), "schema": DYNAMIC_SCHEMA})

        rows = [
            {"id": "1", "name": "alice"},
            {"id": "2", "name": "bob"},
            {"id": "3", "name": "carol"},
        ]
        artifact = sink.write(rows, ctx)
        sink.close()

        assert artifact.size_bytes > 0

        # Verify all rows written
        with open(output_file) as f:
            reader = csv.DictReader(f)
            written_rows = list(reader)
        assert len(written_rows) == 3

    def test_batch_write_empty_list(self, tmp_path: Path, ctx: PluginContext) -> None:
        """Batch write with empty list returns descriptor with zero size."""
        from elspeth.contracts import ArtifactDescriptor
        from elspeth.plugins.sinks.csv_sink import CSVSink

        output_file = tmp_path / "output.csv"
        sink = CSVSink({"path": str(output_file), "schema": DYNAMIC_SCHEMA})

        artifact = sink.write([], ctx)
        sink.close()

        assert isinstance(artifact, ArtifactDescriptor)
        assert artifact.size_bytes == 0
        # Empty write still has a hash (of empty content)
        assert artifact.content_hash == hashlib.sha256(b"").hexdigest()

    def test_has_plugin_version(self) -> None:
        """CSVSink has plugin_version attribute."""
        from elspeth.plugins.sinks.csv_sink import CSVSink

        sink = CSVSink({"path": "/tmp/test.csv", "schema": DYNAMIC_SCHEMA})
        assert hasattr(sink, "plugin_version")
        assert sink.plugin_version == "1.0.0"

    def test_has_determinism(self) -> None:
        """CSVSink has determinism attribute."""
        from elspeth.contracts import Determinism
        from elspeth.plugins.sinks.csv_sink import CSVSink

        sink = CSVSink({"path": "/tmp/test.csv", "schema": DYNAMIC_SCHEMA})
        assert sink.determinism == Determinism.IO_WRITE

    def test_cumulative_hash_after_multiple_writes(self, tmp_path: Path, ctx: PluginContext) -> None:
        """Each write() returns hash of cumulative file contents, not just new rows.

        For audit integrity, the ArtifactDescriptor returned from each write()
        must accurately reflect the complete file state at that moment.
        """
        from elspeth.plugins.sinks.csv_sink import CSVSink

        output_file = tmp_path / "output.csv"
        sink = CSVSink({"path": str(output_file), "schema": DYNAMIC_SCHEMA})

        # First write
        artifact1 = sink.write([{"id": "1", "name": "alice"}], ctx)
        hash_after_first = artifact1.content_hash

        # Verify first hash matches file contents at this point
        file_content_after_first = output_file.read_bytes()
        expected_hash_after_first = hashlib.sha256(file_content_after_first).hexdigest()
        assert hash_after_first == expected_hash_after_first

        # Second write
        artifact2 = sink.write([{"id": "2", "name": "bob"}], ctx)
        hash_after_second = artifact2.content_hash

        # Verify second hash matches cumulative file contents (not just new rows)
        file_content_after_second = output_file.read_bytes()
        expected_hash_after_second = hashlib.sha256(file_content_after_second).hexdigest()
        assert hash_after_second == expected_hash_after_second

        # Third write
        artifact3 = sink.write([{"id": "3", "name": "carol"}], ctx)
        hash_after_third = artifact3.content_hash

        # Verify third hash matches full cumulative contents
        file_content_after_third = output_file.read_bytes()
        expected_hash_after_third = hashlib.sha256(file_content_after_third).hexdigest()
        assert hash_after_third == expected_hash_after_third

        sink.close()

        # Sanity check: each hash should be different (file grew each time)
        assert hash_after_first != hash_after_second
        assert hash_after_second != hash_after_third

    def test_explicit_schema_creates_all_headers_including_optional(self, tmp_path: Path, ctx: PluginContext) -> None:
        """Headers should include all schema fields, not just first row keys.

        Bug: P2-2026-01-19-csvsink-fieldnames-inferred-from-first-row
        When schema is explicit, headers should come from schema config,
        not from first row keys. This ensures optional fields are present.
        """
        from elspeth.plugins.sinks.csv_sink import CSVSink

        # Explicit schema with optional field 'score'
        explicit_schema = {
            "mode": "free",
            "fields": ["id: int", "score: float?"],
        }

        output_file = tmp_path / "output.csv"
        sink = CSVSink(
            {
                "path": str(output_file),
                "schema": explicit_schema,
            }
        )

        # First batch WITHOUT optional field
        sink.write([{"id": 1}], ctx)

        # Second batch WITH optional field - should NOT fail
        # Bug: This fails with ValueError because 'score' is not in fieldnames
        sink.write([{"id": 2, "score": 1.5}], ctx)

        sink.close()

        # Verify CSV has correct headers (including optional field)
        with open(output_file) as f:
            reader = csv.DictReader(f)
            fieldnames = reader.fieldnames
            rows = list(reader)

        # Headers should include 'score' even though first row didn't have it
        assert fieldnames is not None
        assert "score" in fieldnames

        assert len(rows) == 2

    def test_dynamic_schema_still_infers_from_row(self, tmp_path: Path, ctx: PluginContext) -> None:
        """Dynamic schema should continue to infer headers from first row."""
        from elspeth.plugins.sinks.csv_sink import CSVSink

        output_file = tmp_path / "dynamic_output.csv"
        sink = CSVSink(
            {
                "path": str(output_file),
                "schema": DYNAMIC_SCHEMA,
            }
        )

        # First row defines headers
        sink.write([{"a": 1, "b": 2}], ctx)
        sink.close()

        # Verify headers match first row
        with open(output_file) as f:
            reader = csv.DictReader(f)
            fieldnames = reader.fieldnames

        assert fieldnames is not None
        assert sorted(fieldnames) == ["a", "b"]
