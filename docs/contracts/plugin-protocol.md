# ELSPETH Plugin Protocol Contract

> **Status:** FINAL (v1.10)
> **Last Updated:** 2026-02-13
> **Authority:** This document is the master reference for all plugin interactions.

## Overview

ELSPETH follows the Sense/Decide/Act (SDA) model:

```
SOURCE (Sense) â†’ DECIDE â†’ SINK (Act)
```

**Critical distinction:**

| Layer | What | Requires Code? |
|-------|------|----------------|
| **Plugins** | External system integration + business logic | Yes (validated system code) |
| **System Operations** | Token routing, batching, forking, merging | No (config-driven) |

### Plugins (This Document)

Plugins are **system code** developed by the same team that operates ELSPETH. They are independently validated and treated as trusted components. This is NOT a plugin marketplaceâ€”arbitrary external code is not accepted.

| Plugin | Purpose | Touches |
|--------|---------|---------|
| **Source** | Load data from external systems | Row contents |
| **Transform** | Apply business logic to rows | Row contents |
| **Sink** | Write data to external systems | Row contents |

### System Operations (NOT Plugins)

These are **config-driven** infrastructure provided by the ELSPETH engine:

| Operation | Purpose | Config Example |
|-----------|---------|----------------|
| **Gate** | Route tokens based on conditions | `condition: "row['score'] > 0.8"` |
| **Aggregation** | Batch tokens until trigger | `trigger: "count >= 100"` |
| **Fork** | Split token to parallel paths | Routing action |
| **Coalesce** | Merge tokens from parallel paths | `policy: require_all` |

System operations work on **wrapped data** (tokens, metadata) and require no custom code.
Plugins work on **row contents** (the actual data) and are validated system code.

---

## Core Principles

### 1. Audit Is Non-Negotiable

Every plugin interaction is recorded. The audit trail must answer:
- What data entered the plugin?
- What did the plugin produce?
- When did it happen?
- Did it succeed or fail?

Plugins MUST return audit-relevant information. "Trust me, I did it" is not acceptable.

### 2. Plugins Control Their Own Schedule

ELSPETH doesn't dictate internal timing. Plugins may:
- Block waiting for external resources (satellite links, APIs, databases)
- Queue work internally and process on their own schedule
- Batch operations for efficiency

The contract specifies WHEN methods are called, not HOW FAST plugins must respond.

### 3. Exception Handling: Three-Tier Trust Model

ELSPETH uses a three-tier trust model that determines how exceptions should be handled:

| Tier | Trust Level | Coercion | Exception Handling |
|------|-------------|----------|-------------------|
| **External Data** (Source input) | Zero trust | âœ… Allowed | Validate, coerce, quarantine failures |
| **Pipeline Data** (Post-source rows) | Elevated ("probably ok") | âŒ Forbidden | Types trusted, wrap VALUE operations |
| **Our Code** (Plugin internals) | Full trust | âŒ N/A | Let it crash - bugs must surface |

#### The Key Insight: Type-Safe â‰  Operation-Safe

Data that passed source validation has correct **types**, but **values** can still cause operation failures:

```python
# Pipeline data is type-valid (int), but operation fails
row = {"divisor": 0}  # Passed source validation âœ“
result = 100 / row["divisor"]  # ðŸ’¥ ZeroDivisionError - WRAP THIS

# Pipeline data is type-valid (str), but content is problematic
row = {"date": "not-a-date"}  # Passed as str âœ“
parsed = datetime.fromisoformat(row["date"])  # ðŸ’¥ ValueError - WRAP THIS
```

#### The Divide-By-Zero Test

```python
# THEIR DATA value caused the error â†’ WRAP AND HANDLE
def process(self, row: PipelineRow, ctx: PluginContext) -> TransformResult:
    try:
        result = row["value"] / row["divisor"]  # User's divisor=0
    except ZeroDivisionError:
        return TransformResult.error(
            {"reason": "division_by_zero", "field": "divisor"},
            retryable=False,
        )
    return TransformResult.success(
        {"result": result},
        success_reason={"action": "calculated"},
    )

# OUR CODE caused the error â†’ LET IT CRASH
def process(self, row: PipelineRow, ctx: PluginContext) -> TransformResult:
    # If _batch_count is 0, that's MY bug - I should have initialized it
    average = self._total / self._batch_count  # Let it crash!
    return TransformResult.success(
        {"average": average},
        success_reason={"action": "averaged"},
    )
```

#### Coercion Rules by Plugin Type

| Plugin Type | May Coerce Types? | Why |
|-------------|-------------------|-----|
| **Source** | âœ… Yes | Normalizes external data at ingestion boundary (`"42"` â†’ `42`) |
| **Transform** | âŒ No | Receives validated data; wrong types = upstream bug |
| **Sink** | âŒ No | Receives validated data; wrong types = upstream bug |

If a transform receives `"42"` when its `input_schema` says `int`, that's a bug in the source or upstream transform. The correct fix is to fix the upstream plugin, NOT to coerce in the transform.

#### The Boundary

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     PLUGIN PROCESSING                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  THEIR DATA VALUES (row field values, external responses)           â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”‚
â”‚  â€¢ Types are trusted (source validated them)                        â”‚
â”‚  â€¢ Values may still cause operation failures                        â”‚
â”‚  â€¢ Wrap OPERATIONS in try/catch                                     â”‚
â”‚  â€¢ Return error result on failure                                    â”‚
â”‚  â€¢ Row gets quarantined, pipeline continues                          â”‚
â”‚                                                                      â”‚
â”‚  Examples:                                                           â”‚
â”‚  â€¢ row["value"] / row["divisor"] â†’ catch ZeroDivisionError          â”‚
â”‚  â€¢ datetime.fromisoformat(row["date"]) â†’ catch ValueError           â”‚
â”‚  â€¢ external_api.call(row["id"]) â†’ catch ApiError                    â”‚
â”‚  â€¢ json.loads(row["payload"]) â†’ catch JSONDecodeError               â”‚
â”‚                                                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  THEIR DATA TYPES (at Source boundary ONLY)                         â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”‚
â”‚  â€¢ External data may have wrong types                               â”‚
â”‚  â€¢ Sources MAY coerce: "42" â†’ 42, "true" â†’ True                     â”‚
â”‚  â€¢ Sources MUST quarantine rows that can't be coerced               â”‚
â”‚  â€¢ Transforms/Sinks MUST NOT coerce types                           â”‚
â”‚                                                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  OUR CODE (internal state, plugin logic)                            â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                          â”‚
â”‚  â€¢ Do NOT wrap in try/catch                                          â”‚
â”‚  â€¢ Let exceptions propagate                                          â”‚
â”‚  â€¢ Pipeline crashes - this is a bug to fix                           â”‚
â”‚                                                                      â”‚
â”‚  Examples:                                                           â”‚
â”‚  â€¢ self._counter / self._batch_size â†’ if batch_size=0, that's a bug â”‚
â”‚  â€¢ self._buffer[index] â†’ if index wrong, that's a bug               â”‚
â”‚  â€¢ self._connection.execute() â†’ if connection is None, that's a bug â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Contract

| Zone | On Error | Plugin Action | ELSPETH Action |
|------|----------|---------------|----------------|
| **Their Data Values** | Row value causes operation error | Catch, return error result | Quarantine row, continue |
| **Their Data Types** | Wrong type at Source boundary | Coerce if possible, else error | Quarantine row, continue |
| **Their Data Types** | Wrong type at Transform/Sink | â€” | This is an upstream bug, should crash |
| **Our Code** | Internal state causes error | Let it crash | Record in audit, halt pipeline |
| **Lifecycle** | `on_start`/`on_complete`/`close` fails | Let it crash | Halt pipeline (config/code bug) |

**Rules:**
- Plugins MUST wrap operations on row values and return error results on failure
- Sources MAY coerce external data types; Transforms/Sinks MUST NOT
- Plugins MUST NOT wrap operations on internal state - let bugs surface
- If you're tempted to add `try/except` around your own logic, you have a bug to fix
- If a transform receives wrong types, that's an upstream bug to fix, not something to coerce

### 4. Forced Pass for Lifecycle Hooks

All lifecycle hooks are REQUIRED in the protocol, even if implementation is `pass`.

**Why:** The audit trail records every lifecycle event. Even an empty `on_start()` produces an audit record: "plugin started at timestamp X". This is defensible under audit.

---

## Plugin Types

### Source

**Purpose:** Load data into the pipeline. Exactly one source per run.

**Data Flow:** Produces rows on its own schedule.

#### Required Attributes

```python
name: str                          # Plugin identifier (e.g., "csv", "api", "satellite")
output_schema: type[PluginSchema]  # Schema of rows this source produces
node_id: str | None                # Set by orchestrator after registration
determinism: Determinism           # See Determinism Declaration section for all 6 levels
plugin_version: str                # Semantic version for reproducibility
```

#### Required Configuration

```yaml
sources:
  my_source:
    type: csv_source
    path: data/input.csv
    schema:
      mode: strict
      fields:
        name: {type: string}
        age: {type: integer}
    # REQUIRED: Where do non-conformant rows go?
    on_validation_failure: quarantine_sink  # Sink name, or "discard"
```

**`on_validation_failure`** (REQUIRED):
- Specifies destination for rows that fail schema validation/coercion
- Value must be a sink name or `"discard"` for explicit drop
- Cannot be omitted, empty, or null - operator must acknowledge bad data handling
- Even when `"discard"`, a `QuarantineEvent` is recorded in the audit trail

**Quarantine behavior:**
1. Source attempts to validate/coerce row against `output_schema`
2. If validation fails:
   - `QuarantineEvent` recorded (always, even for discard)
   - Row routed to configured sink OR dropped if `"discard"`
   - Row does NOT enter the pipeline
3. If validation succeeds:
   - Row enters pipeline as normal

#### Required Methods

```python
def __init__(self, config: dict[str, Any]) -> None:
    """Initialize with configuration.

    Called once at pipeline construction.
    Validate config here - fail fast if misconfigured.
    """

def load(self, ctx: PluginContext) -> Iterator[SourceRow]:
    """Yield rows from the source.

    MAY BLOCK internally waiting for data (satellite downlink, API polling).
    Yields rows on source's own schedule.
    Iterator exhausts when source has no more data.

    Returns:
        Iterator yielding SourceRow instances - either SourceRow.valid() for
        valid rows or SourceRow.quarantined() for invalid rows.

    Example:
        try:
            validated = self._schema_contract.validate(row)
            yield SourceRow.valid(validated, contract=self._schema_contract)
        except ValidationError as e:
            if self._on_validation_failure != "discard":
                yield SourceRow.quarantined(
                    row=row,
                    error=str(e),
                    destination=self._on_validation_failure,
                )
    """

def close(self) -> None:
    """Release resources.

    Called after on_complete() or on error.
    MUST NOT raise - log errors internally if cleanup fails.
    """
```

#### Required Lifecycle Hooks

```python
def on_start(self, ctx: PluginContext) -> None:
    """Called before load().

    Use for: Opening connections, authenticating, preparing resources.
    Should be reasonably quick - heavy blocking belongs in load().

    If this fails, it's a CODE BUG - pipeline crashes.
    """

def on_complete(self, ctx: PluginContext) -> None:
    """Called after load() exhausts (before close).

    Use for: Finalization, recording completion metrics.

    If this fails, it's a CODE BUG - pipeline crashes.
    """
```

#### Lifecycle

```
__init__(config)
    â”‚
    â–¼
on_start(ctx)           â† Setup connections, auth
    â”‚
    â–¼
load(ctx) â”€â–º yields rows on source's schedule
    â”‚        (may block internally)
    â”‚
    â–¼ (iterator exhausts)
on_complete(ctx)        â† Finalization
    â”‚
    â–¼
close()                 â† Release resources
```

#### SourceRow Contract

```python
@dataclass
class SourceRow:
    row: Any  # dict for valid rows, Any for quarantined (may be malformed)
    is_quarantined: bool
    quarantine_error: str | None
    quarantine_destination: str | None
    contract: SchemaContract | None  # Enables to_pipeline_row() conversion

    @classmethod
    def valid(cls, row: dict[str, Any], contract: SchemaContract | None = None) -> SourceRow:
        """Create a valid source row.

        Args:
            row: Validated row data
            contract: Optional schema contract for the row (enables to_pipeline_row())

        Returns:
            SourceRow with is_quarantined=False
        """
        ...

    @classmethod
    def quarantined(cls, row: Any, error: str, destination: str) -> SourceRow:
        """Create a quarantined row result.

        Args:
            row: The original row data (may be non-dict for malformed data)
            error: The validation error message
            destination: The sink name to route this row to
        """
        ...

    def to_pipeline_row(self) -> PipelineRow:
        """Convert to PipelineRow for processing.

        Raises:
            ValueError: If row is quarantined or has no contract
        """
        ...
```

**Usage:**
- Valid rows: `SourceRow.valid(row_dict)` or `SourceRow.valid(row_dict, contract=schema_contract)`
- Invalid rows: `SourceRow.quarantined(row_data, error_message, destination_sink)`
- The `contract` parameter is optional but enables downstream conversion to PipelineRow
- Quarantined rows never have contracts

#### Audit Records

- `on_start` called timestamp
- Each row yielded (row_id, content_hash)
- `on_complete` called timestamp
- Total rows produced
- **QuarantineEvent** for each validation failure:
  - `run_id`, `source_id`, `row_index`
  - `raw_row` (original data before coercion attempt)
  - `failure_reason` (why validation failed)
  - `field_errors` (per-field error details)
  - `destination` (sink name or "discard")
  - `timestamp`

---

### Transform

**Purpose:** Apply business logic to rows. Stateless between calls.

**Data Flow:** Row(s) in â†’ row(s) out (possibly modified).

#### Required Attributes

```python
name: str
input_schema: type[PluginSchema]
output_schema: type[PluginSchema]
node_id: str | None
determinism: Determinism
plugin_version: str
is_batch_aware: bool = False  # Set True for batch processing at aggregation nodes
creates_tokens: bool = False  # Set True for deaggregation (1â†’N row expansion)
transforms_adds_fields: bool = False  # Set True if transform adds fields to the output schema
```

#### Token Creation (Deaggregation)

Transforms that expand one row into multiple rows must declare `creates_tokens = True`:

```python
class JSONExplode(BaseTransform):
    name = "json_explode"
    creates_tokens = True  # Engine creates new tokens for each output row
    # ...

    def process(self, row: PipelineRow, ctx: PluginContext) -> TransformResult:
        items = row["items"]  # Trust: source validated this is a list
        return TransformResult.success_multi(
            [{**row.to_dict(), "item": item, "item_index": i} for i, item in enumerate(items)],
            success_reason={"action": "exploded", "output_count": len(items)},
        )
```

**Invariants:**
- `creates_tokens=True` + `success()` â†’ single output (allowed, like passthrough)
- `creates_tokens=True` + `success_multi()` â†’ engine creates new tokens per output row
- `creates_tokens=False` + `success_multi()` â†’ RuntimeError (except in aggregation passthrough mode)
- `success_multi([])` is invalid (no silent drops) - use `success()` for a single-row empty case

**Engine semantics:**
- When a deaggregation transform (`creates_tokens=True`) returns `success_multi()` for a single input token, the engine creates N child tokens via `expand_token()` and the parent token reaches terminal state `EXPANDED` (children continue).

#### Batch-Aware Transforms

Transforms can declare `is_batch_aware = True` to receive batched rows when used at aggregation nodes:

```python
class SummaryTransform(BaseTransform):
    name = "summary"
    is_batch_aware = True  # Engine may pass aggregated data when used at aggregation node
    # ... other attrs ...

    def process(
        self,
        row: PipelineRow,
        ctx: PluginContext,
    ) -> TransformResult:
        # When used in aggregation, engine buffers rows and passes aggregated data
        # For this example, assume row contains aggregated batch metadata
        if "batch_rows" in row:
            # Batch mode: aggregate the rows
            batch_rows = row["batch_rows"]
            total = sum(r["value"] for r in batch_rows)
            return TransformResult.success(
                {"total": total, "count": len(batch_rows)},
                success_reason={"action": "aggregated", "batch_size": len(batch_rows)},
            )
        # Single row mode
        return TransformResult.success(
            row.to_dict(),
            success_reason={"action": "passthrough"},
        )
```

**How it works:**

1. Transform is configured at an aggregation node (see "Aggregation" below)
2. Engine buffers rows until trigger fires (count, timeout, condition)
3. Engine calls `transform.process(rows: list[PipelineRow], ctx)` with the batch
4. Transform returns aggregated result

> **Implementation note:** Batch-aware transforms implement `BatchTransformProtocol` â€” a separate protocol from `TransformProtocol`. The key difference is the `process()` signature: `BatchTransformProtocol.process(rows: list[PipelineRow], ctx)` receives a list of `PipelineRow`, while `TransformProtocol.process(row: PipelineRow, ctx)` receives a single row. Both protocols share most attributes (`name`, `input_schema`, `output_schema`, `is_batch_aware`, `creates_tokens`), though `transforms_adds_fields` is only on `TransformProtocol`.

**Key points:**
- `is_batch_aware = False` (default): Transform implements `TransformProtocol`, receives single `PipelineRow`
- `is_batch_aware = True`: Transform implements `BatchTransformProtocol`, receives `list[PipelineRow]` at aggregation nodes
- The engine decides when to batch based on pipeline configuration

#### Optional Configuration

```yaml
transforms:
  price_calculator:
    type: custom_transform
    # OPTIONAL: Where do rows go when transform returns error?
    on_error: failed_calculations  # Sink name, or "discard"
```

**`on_error`** (OPTIONAL):
- Specifies destination for rows where transform returns `TransformResult.error()`
- Value must be a sink name or `"discard"` for explicit drop
- If omitted and transform returns an error â†’ `ConfigurationError` (pipeline crashes)
- Even when `"discard"`, a `TransformErrorEvent` is recorded in the audit trail

**Error vs Bug distinction:**

| Scenario | Signal | Behavior |
|----------|--------|----------|
| **Processing Error** | `TransformResult.error(...)` | Route to `on_error` sink |
| **Transform Bug** | Exception thrown | CRASH immediately |

```python
# PROCESSING ERROR - legitimate, uses on_error routing
def process(self, row: PipelineRow, ctx: PluginContext) -> TransformResult:
    if row["quantity"] == 0:
        return TransformResult.error(
            {"reason": "division_by_zero", "field": "quantity"},
            retryable=False,
        )
    return TransformResult.success(
        {"unit_price": row["total"] / row["quantity"]},
        success_reason={"action": "calculated_unit_price"},
    )

# TRANSFORM BUG - crashes, does NOT use on_error routing
def process(self, row: PipelineRow, ctx: PluginContext) -> TransformResult:
    return TransformResult.success(
        {"value": row["nonexistent"]},  # KeyError = BUG
        success_reason={"action": "processed"},
    )
```

#### Required Methods

```python
def __init__(self, config: dict[str, Any]) -> None:
    """Initialize with configuration."""

def process(
    self,
    row: PipelineRow,
    ctx: PluginContext,
) -> TransformResult:
    """Process a single row.

    Args:
        row: Input row as PipelineRow (immutable, supports dual-name access)
        ctx: Plugin context

    Returns:
        TransformResult.success(row, success_reason={...}) - processed row
        TransformResult.success_multi(rows, success_reason={...}) - multiple output rows
        TransformResult.error(reason, retryable=bool) - processing failed

    Notes:
        - For batch-aware transforms (is_batch_aware=True), the engine may buffer
          rows and call this with aggregated data when used in aggregation nodes
        - PipelineRow is immutable and supports both normalized and original field names
        - MUST be a pure function for DETERMINISTIC transforms

    Exception handling:
        - Data validation errors â†’ return TransformResult.error()
        - Code bugs â†’ let exception propagate (will crash)
    """

def close(self) -> None:
    """Release resources."""
```

#### Required Lifecycle Hooks

```python
def on_start(self, ctx: PluginContext) -> None:
    """Called before any rows are processed."""

def on_complete(self, ctx: PluginContext) -> None:
    """Called after all rows are processed."""
```

#### TransformResult Contract

```python
@dataclass
class TransformResult:
    status: Literal["success", "error"]
    row: dict[str, Any] | PipelineRow | None           # Single output row (mutually exclusive with rows)
    rows: list[dict[str, Any] | PipelineRow] | None    # Multi-row output (mutually exclusive with row)
    reason: TransformErrorReason | None                # Error details or None (success)
    retryable: bool = False                            # Can this operation be retried?

    # Success metadata - REQUIRED for success results, None for error results
    success_reason: TransformSuccessReason | None

    # Audit fields (set by executor, NOT by plugin)
    input_hash: str | None
    output_hash: str | None
    duration_ms: float | None

    # Context snapshot for audit trail (optional)
    # Contains operational metadata like pool stats, ordering info
    context_after: dict[str, Any] | None

    # Schema contract for output (optional)
    # Enables conversion to PipelineRow via to_pipeline_row()/to_pipeline_rows()
    contract: SchemaContract | None

    @property
    def is_multi_row(self) -> bool:
        """True if this result contains multiple output rows."""
        return self.rows is not None

    @property
    def has_output_data(self) -> bool:
        """True if this result has any output data (single or multi-row)."""
        return self.row is not None or self.rows is not None

    def to_pipeline_row(self) -> PipelineRow:
        """Convert single-row success result to PipelineRow."""
        ...

    def to_pipeline_rows(self) -> list[PipelineRow]:
        """Convert multi-row success result to list of PipelineRows."""
        ...
```

**Invariants:**
- `status == "success"` requires `has_output_data == True` AND `success_reason is not None`
- `row` and `rows` are mutually exclusive (exactly one is set on success)
- `status == "error"` requires `reason is not None`
- If `success_reason` is missing on success, `__post_init__` will crash (plugin bug)

**Factory methods:**

```python
# REQUIRED: success_reason parameter (keyword-only)
TransformResult.success(
    row,
    success_reason={"action": "processed"},
    context_after=None,  # Optional operational metadata
    contract=None,       # Optional schema contract for to_pipeline_row()
)

TransformResult.success_multi(
    rows,
    success_reason={"action": "split", "count": len(rows)},
    context_after=None,  # Optional operational metadata
    contract=None,       # Optional schema contract for to_pipeline_rows()
)

TransformResult.error(
    reason={"reason": "invalid_input", "field": "amount"},
    retryable=False,
    context_after=None,  # Optional operational metadata from partial execution
)
```

**Multi-row usage:**

```python
# Deaggregation: 1 input â†’ N outputs
def process(self, row: PipelineRow, ctx: PluginContext) -> TransformResult:
    items = row["items"]  # Trust: source validated this is a list
    return TransformResult.success_multi(
        [{**row.to_dict(), "item": item, "item_index": i} for i, item in enumerate(items)],
        success_reason={"action": "exploded", "output_count": len(items)},
    )

# Aggregation passthrough: N inputs â†’ N enriched outputs
def process(self, row: PipelineRow, ctx: PluginContext) -> TransformResult:
    # When is_batch_aware=True and used in aggregation, engine may pass aggregated data
    rows = row if isinstance(row, list) else [row]
    return TransformResult.success_multi(
        [{**r, "batch_size": len(rows)} for r in rows],
        success_reason={"action": "batch_enriched", "batch_size": len(rows)},
    )
```

#### Lifecycle

```
__init__(config)
    â”‚
    â–¼
on_start(ctx)
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ process(row, ctx) Ã— N       â”‚  â† Called for each row
â”‚     â”‚                       â”‚
â”‚     â–¼                       â”‚
â”‚ TransformResult             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
on_complete(ctx)
    â”‚
    â–¼
close()
```

#### Audit Records

- Each `process()` call: input_hash, output_hash, duration_ms, status
- Errors: exception type, message, retryable flag
- **TransformErrorEvent** for each `TransformResult.error()`:
  - `run_id`, `token_id`, `transform_id`
  - `row` (input row data)
  - `error_details` (from TransformResult.error())
  - `destination` (sink name or "discard")
  - `input_hash` (for traceability)
  - `timestamp`

---

### Sink

**Purpose:** Output data to external destination. One or more per run.

**Data Flow:** Receives rows, produces artifacts.

#### Required Attributes

```python
name: str
input_schema: type[PluginSchema]
node_id: str | None
idempotent: bool                # Can this sink handle duplicate writes safely?
determinism: Determinism
plugin_version: str
```

#### Required Methods

```python
def __init__(self, config: dict[str, Any]) -> None:
    """Initialize with configuration."""

def write(self, rows: list[dict[str, Any]], ctx: PluginContext) -> ArtifactDescriptor:
    """Receive rows and return proof of work.

    The sink controls its own internal processing:
    - May write immediately (CSV, database)
    - May queue internally and process later (satellite, async API)
    - May batch for efficiency

    MUST return ArtifactDescriptor describing what was produced/queued.
    SHOULD NOT block for slow operations - queue internally, confirm in on_complete().

    Returns:
        ArtifactDescriptor with content_hash and size_bytes (REQUIRED for audit)
    """

def flush(self) -> None:
    """Ensure all buffered writes are durable.

    MUST guarantee that when this method returns:
    - All data passed to write() is persisted
    - Data survives process crash
    - Data survives power loss (for file/block storage)

    This method MUST block until durability is guaranteed.

    For file-based sinks: Call file.flush() + os.fsync(file.fileno())
    For database sinks: Call connection.commit()
    For async sinks: Await flush completion

    This is called by the orchestrator BEFORE creating checkpoints.
    If this method returns successfully, the checkpoint system assumes
    all data is durable and will NOT replay these writes on resume.

    Called periodically and before on_complete().
    """

def close(self) -> None:
    """Release resources.

    Called after on_complete() or on error.
    """
```

#### Required Lifecycle Hooks

```python
def on_start(self, ctx: PluginContext) -> None:
    """Called before any writes.

    Use for: Opening connections, creating output files, initializing queues.
    """

def on_complete(self, ctx: PluginContext) -> None:
    """Called after all writes, before close.

    CRITICAL: This method MAY BLOCK until all queued work is confirmed.

    Use for:
    - Committing database transactions
    - Waiting for satellite transmission confirmation
    - Finalizing multi-part uploads
    - Any operation that must complete before run is considered done

    The run CANNOT complete until on_complete() returns for ALL sinks.
    """
```

#### ArtifactDescriptor Contract

```python
@dataclass(frozen=True)
class ArtifactDescriptor:
    artifact_type: Literal["file", "database", "webhook"]
    path_or_uri: str              # Where the artifact lives
    content_hash: str             # SHA-256 of content (REQUIRED)
    size_bytes: int               # Size of artifact (REQUIRED)
    metadata: dict | None = None  # Type-specific metadata
```

**Factory methods:**

```python
ArtifactDescriptor.for_file(path: str, content_hash, size_bytes)
ArtifactDescriptor.for_database(url: SanitizedDatabaseUrl, table, content_hash, payload_size, row_count)
ArtifactDescriptor.for_webhook(url: SanitizedWebhookUrl, content_hash, request_size, response_code)
```

> **URL sanitization is enforced at the type level.** `for_database()` and `for_webhook()` require pre-sanitized URL types, NOT plain strings. Use `SanitizedDatabaseUrl.from_raw_url(url)` or `SanitizedWebhookUrl.from_raw_url(url)` to strip credentials before passing to the factory. This prevents secrets from entering the audit trail. Both factory methods include `isinstance()` enforcement and raise `TypeError` on plain strings.

#### content_hash Requirement

The `content_hash` field is REQUIRED and proves what was written. Hash computation differs by artifact type:

| Artifact Type | What Gets Hashed | Why |
|---------------|------------------|-----|
| **file** | SHA-256 of file contents | Proves exact bytes written |
| **database** | SHA-256 of canonical JSON payload BEFORE insert | Proves intent (DB may transform data) |
| **webhook** | SHA-256 of request body | Proves what was sent (response is separate) |

**Key principle:** Hash what YOU control, not what the destination does with it. For databases, you hash the payload you're sending, not what the DB stores (it may add timestamps, auto-increment IDs, etc.). This proves intent.

#### Lifecycle

```
__init__(config)
    â”‚
    â–¼
on_start(ctx)               â† Open connections, prepare
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ write(rows, ctx) â†’ ArtifactDescriptor        â”‚  â† May be called multiple times
â”‚     â”‚                                        â”‚
â”‚     â–¼                                        â”‚
â”‚ (sink processes on its own schedule)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â–¼
flush()                     â† Flush any buffers
    â”‚
    â–¼
on_complete(ctx)            â† BLOCKS until truly done
    â”‚                         (satellite confirms, transaction commits)
    â–¼
close()                     â† Release resources
```

#### Idempotency

Sinks declare `idempotent: bool`:
- `True`: Safe to retry writes with same data (uses idempotency key)
- `False`: Retry may cause duplicates (append-only files, non-idempotent APIs)

Idempotency key format: `{run_id}:{token_id}:{sink_name}`

#### Resume Capability

Sinks declare `supports_resume: bool` to indicate they can append to existing output on resume:

```python
supports_resume: bool   # Can this sink append to existing output on resume?
```

Sinks that support resume implement three additional methods:

| Method | Purpose |
|--------|---------|
| `configure_for_resume()` | Switch to append mode (called by engine on resume) |
| `validate_output_target()` | Validate existing output matches configured schema |
| `set_resume_field_resolution(mapping)` | Set source field resolution mapping before validation |

Sinks that don't support resume (`supports_resume=False`) will never have these methods called â€” the CLI rejects resume before execution.

#### Audit Records

- Each `write()`: ArtifactDescriptor (type, path, hash, size)
- `on_complete` timestamp (confirms delivery)
- Idempotency key if applicable

---

## System Operations (Engine-Level)

The following are **engine-level operations** that coordinate token flow through the DAG. They operate on **wrapped data** (tokens, routing metadata) rather than row contents directly.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     DATA FLOW ARCHITECTURE                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                   â”‚
â”‚  PLUGINS (touch row contents)                                    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                    â”‚
â”‚  Source â”€â”€â–º [row data] â”€â”€â–º Transform â”€â”€â–º [row data] â”€â”€â–º Sink    â”‚
â”‚                                    â”‚                              â”‚
â”‚                                    â–¼                              â”‚
â”‚  ROUTING DECISIONS                                                â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                               â”‚
â”‚  Gate: "where does this token go?" (config-driven)               â”‚
â”‚                                                                   â”‚
â”‚  STRUCTURAL OPERATIONS (config-driven only)                      â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                      â”‚
â”‚  Fork: "copy token to parallel paths"                            â”‚
â”‚  Coalesce: "merge tokens from paths"                             â”‚
â”‚  Aggregation: "batch tokens until trigger"                       â”‚
â”‚                                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Gate (Routing Decision)

**Purpose:** Evaluate a condition on row data and decide where the token goes next.

**Key property:** Gates make routing decisions. They evaluate conditions on row data and determine token destinations.

Gates are **config-driven operations** defined in YAML. They are NOT plugins.

#### Config Expression Gates

Config expressions provide declarative routing.

**Configuration:**

```yaml
pipeline:
  - source: csv_input

  - gate: quality_check
    condition: "row['confidence'] >= 0.85"
    routes:
      high: continue          # Continue to next node
      low: review_sink        # Route to named sink

  - transform: enrich_data

  - sink: output
```

**How It Works:**

1. Engine evaluates `condition` expression against row data
2. Expression returns a route label (`high`, `low`, etc.)
3. Engine looks up route label in `routes` config
4. Token is routed to destination (`continue` or sink name)

#### Expression Language

```python
# Simple field comparison
"row['score'] > 0.8"

# Multiple conditions
"row['status'] == 'active' and row['balance'] > 0"

# Field existence check
"'optional_field' in row"

# Null handling
"row.get('nullable_field') is not None"
```

#### Expression Safety

Gate conditions are evaluated using a **restricted expression parser**, NOT Python `eval()`.

**Allowed:**
- Field access: `row['field']`, `row.get('field')`
- Comparisons: `==`, `!=`, `<`, `>`, `<=`, `>=`
- Boolean operators: `and`, `or`, `not`
- Identity: `is`, `is not` (for `None` checks)
- Membership: `in`, `not in`
- Arithmetic: `+`, `-`, `*`, `/`, `//`, `%`
- Ternary: `x if condition else y`
- Literals: strings, numbers, booleans, `None`
- Collections: lists, dicts, tuples, sets (for membership checks)

**NOT Allowed:**
- Function calls (except `row.get()`)
- Imports
- Attribute access beyond row fields
- Assignment (including `:=`)
- Lambda/comprehensions
- `await`, `yield`
- F-strings with expressions
- Starred expressions (`*x`, `**x`)
- Slice syntax

This prevents code injection. An expression like `"__import__('os').system('rm -rf /')"` will be rejected at config validation time.

#### GateResult Contract

The engine evaluates gate conditions and produces `GateResult` objects internally.

```python
@dataclass
class GateResult:
    row: dict[str, Any]        # Row data (passed through)
    action: RoutingAction      # Where the token goes

    # Schema contract for output (optional)
    # Enables conversion to PipelineRow via to_pipeline_row()
    contract: SchemaContract | None

    # Audit fields (set by executor, NOT by plugin)
    input_hash: str | None
    output_hash: str | None
    duration_ms: float | None

    def to_pipeline_row(self) -> PipelineRow:
        """Convert to PipelineRow for downstream processing."""
        ...

# RoutingAction options:
RoutingAction.continue_()              # Continue to next node in pipeline
RoutingAction.route("label")           # Route to labeled destination (resolved via routes config)
RoutingAction.fork_to_paths(["path1", "path2"]) # Fork to multiple parallel paths
```

#### Gate Audit Trail

Gates record:
- Condition evaluated: expression text
- Route chosen: label + destination
- Timing: evaluation duration

---

### Fork (Token Splitting)

**Purpose:** Copy a single token to multiple parallel paths for concurrent processing.

**Key property:** Creates child tokens with same row data, different branch identities.

#### Configuration

```yaml
pipeline:
  - source: input

  - gate: parallel_analysis
    condition: "True"  # Always fork
    routes:
      all: fork         # Special keyword triggers fork
    fork_to:
      - sentiment_path
      - entity_path
      - summary_path

  # Parallel paths defined separately
  paths:
    sentiment_path:
      - transform: sentiment_analyzer
    entity_path:
      - transform: entity_extractor
    summary_path:
      - transform: summarizer
```

#### How It Works

1. Gate evaluates condition
2. If route is `fork`, engine creates N child tokens
3. Each child token:
   - Has same `row_id` as parent
   - Gets unique `token_id`
   - Records `parent_token_id` for lineage
   - Assigned to specific path/branch
4. Parent token marked with terminal state `FORKED`
5. Child tokens flow through their respective paths

#### Token Lineage

```
Parent Token (T1)
    â”‚ row_id: R1
    â”‚ status: FORKED
    â”‚
    â”œâ”€â”€â–º Child Token (T2)
    â”‚    row_id: R1
    â”‚    parent_token_id: T1
    â”‚    branch: sentiment_path
    â”‚
    â”œâ”€â”€â–º Child Token (T3)
    â”‚    row_id: R1
    â”‚    parent_token_id: T1
    â”‚    branch: entity_path
    â”‚
    â””â”€â”€â–º Child Token (T4)
         row_id: R1
         parent_token_id: T1
         branch: summary_path
```

#### Audit Trail

- Fork event: parent_token_id, child_token_ids, branch assignments
- Fork group ID: links all children from same fork

---

### Coalesce (Token Merging)

**Purpose:** Merge tokens from parallel paths back into a single token.

**Key property:** Waits for tokens from specified branches, combines based on policy.

> **Note:** Coalesce is fully structural â€” there is no plugin-level coalesce protocol. The engine uses config-driven merge strategies (union/nested/select) via the `CoalesceExecutor` in `engine/coalesce_executor.py`. Configure coalesce behavior via the `coalesce:` section in pipeline YAML.

#### Configuration

```yaml
pipeline:
  # ... fork happens earlier ...

  # List format (identity branches â€” no per-branch transforms):
  - coalesce: merge_results
    branches:
      - sentiment_path
      - entity_path
      - summary_path
    policy: require_all    # Wait for all branches
    timeout: 5m            # Max wait time
    merge: union           # How to combine row data
```

```yaml
pipeline:
  # ... fork happens earlier, transforms declared with input/on_success ...

  # Dict format (per-branch transforms â€” ARCH-15):
  # Keys are branch names (from fork_to), values are the input connections
  # that feed each branch into the coalesce node.
  - coalesce: merge_results
    branches:
      sentiment_path: sentiment_output    # branch routes through sentiment transform chain
      entity_path: entities_output        # branch routes through entity transform chain
      summary_path: summary_output        # branch routes through summary transform chain
    policy: require_all
    timeout: 5m
    merge: union
```

> **Note:** List format is syntactic sugar. `branches: [a, b]` normalizes to `branches: {a: a, b: b}` (identity mapping â€” tokens skip directly to coalesce with no intermediate transforms).

#### Policies

| Policy | Behavior |
|--------|----------|
| `require_all` | Wait for ALL branches, fail if any missing after timeout |
| `quorum` | Wait for N branches (configurable threshold) |
| `best_effort` | Wait until timeout, merge whatever arrived |
| `first` | Take first arrival, discard others |

#### Merge Strategies

| Strategy | Behavior |
|----------|----------|
| `union` | Combine all fields (later branches overwrite earlier) |
| `nested` | Each branch output becomes nested object: `{branch_name: output}` |
| `select` | Take output from specific branch only |

#### How It Works

1. Engine tracks arriving tokens by `row_id` and branch
2. When policy is satisfied (all arrived, quorum met, timeout):
   - Creates new merged token
   - Combines row data per merge strategy
   - Child tokens marked with terminal state `COALESCED`
3. Merged token continues down pipeline

#### Token Lineage

```
Child Tokens (arriving)
    â”‚
    â”œâ”€â”€ T2 (sentiment_path): {sentiment: "positive"}
    â”œâ”€â”€ T3 (entity_path): {entities: ["ACME", "NYC"]}
    â””â”€â”€ T4 (summary_path): {summary: "..."}
    â”‚
    â–¼ (coalesce with union strategy)
    â”‚
Merged Token (T5)
    row_id: R1
    parent_token_ids: [T2, T3, T4]
    row_data: {
        sentiment: "positive",
        entities: ["ACME", "NYC"],
        summary: "..."
    }
```

#### Audit Trail

- Coalesce event: input_token_ids, output_token_id, policy used
- Timing: wait duration, which branches arrived when
- Merge details: strategy used, any conflicts

---

### Aggregation (Token Batching)

**Purpose:** Collect multiple tokens until a trigger fires, then process as batch.

**Key property:** Converts N input tokens into M output tokens (often Nâ†’1 for aggregates).

**Architecture:** Aggregation is **fully structural** - the engine owns the buffer and decides when to flush. There is no plugin-level aggregation protocol. When you need batch processing:

1. Configure an aggregation node in the pipeline
2. Use a batch-aware transform (`is_batch_aware = True`) at that node
3. Engine buffers rows and calls `transform.process(rows: list[PipelineRow], ctx)` when trigger fires

#### Configuration

```yaml
pipeline:
  - source: events

  # Aggregation configuration
  aggregations:
    - node_id: batch_stats      # Links to transform below
      trigger:
        count: 100              # Fire after 100 rows
        timeout_seconds: 3600   # Or after 1 hour
        # condition: "row['type'] == 'flush_signal'"  # Optional: trigger on special row

  transforms:
    - plugin: summary_transform
      node_id: batch_stats      # Same as aggregation node_id
      # Transform must have is_batch_aware = True
```

#### Output Mode

Aggregation supports two output modes that determine how batch results are handled:

```yaml
aggregations:
  - node_id: batch_stats
    trigger: { count: 100 }
    output_mode: transform          # default: N inputs â†’ M outputs
    expected_output_count: 1        # optional: validate Nâ†’1 cardinality
```

| Mode | Input â†’ Output | Token Handling | While Buffering | Use Case |
|------|----------------|----------------|-----------------|----------|
| `transform` | N â†’ M | New tokens created | `CONSUMED_IN_BATCH` | Aggregation, group-by, splitting |
| `passthrough` | N â†’ N | Same tokens preserved | `BUFFERED` | Batch enrichment |

**Outcome semantics:**

- **`transform`** (default): Batch transformation. N rows become M rows with new tokens. All input tokens are terminal (`CONSUMED_IN_BATCH`). New tokens are created via `expand_token()` with parent linkage to the triggering token. Use `expected_output_count: 1` for classic Nâ†’1 aggregation (sum, count, mean) to validate cardinality.

- **`passthrough`**: Batch enrichment. N rows become N enriched rows with the same token IDs. Buffered tokens get `BUFFERED` (non-terminal) while waiting, then reappear as `COMPLETED` on flush. Transform must return `success_multi()` with exactly N rows.

**Error handling:** All modes are atomic - if the transform returns `error`, ALL buffered rows fail together.

#### Trigger Types

| Trigger | Fires When |
|---------|------------|
| `count` | N tokens accumulated |
| `timeout_seconds` | Duration elapsed since batch start |
| `condition` | Row matches expression |
| `end_of_source` | Source exhausted (implicit, always checked) |

Multiple triggers can be combined (first one to fire wins).

#### How It Works (Engine-Level)

1. Tokens arrive at aggregation node
2. **Engine buffers** the row internally:
   - Assigns `batch_id`
   - Records batch membership in audit trail
   - Updates trigger evaluator
   - Token outcome depends on `output_mode`:
     - `transform`: `CONSUMED_IN_BATCH` (terminal)
     - `passthrough`: `BUFFERED` (non-terminal, will reappear as `COMPLETED`)
3. Engine checks trigger conditions
4. When trigger fires:
   - Engine retrieves buffered rows as `list[PipelineRow]`
   - Engine calls `transform.process(rows, ctx)` with the batch
   - Batch state: `draft` â†’ `executing` â†’ `completed`
   - Output handling depends on `output_mode`:
     - `passthrough`: All buffered tokens continue with enriched data
     - `transform`: New tokens created via `expand_token()` continue
   - Buffer is cleared for next batch

**Important:** The engine owns the buffer, not the transform. Transforms simply receive `list[PipelineRow]` and return a result. This enables:
- Crash recovery (buffers are checkpointed)
- Consistent trigger evaluation
- Clean audit trail

#### Batch Lifecycle

```
Batch B1 (draft) - Engine buffers rows
    â”‚
    â”œâ”€â”€ Buffer T1.row_data â†’ batch: [row1]
    â”œâ”€â”€ Buffer T2.row_data â†’ batch: [row1, row2]
    â”œâ”€â”€ Buffer T3.row_data â†’ batch: [row1, row2, row3]
    â”‚
    â–¼ (trigger: count >= 3)
    â”‚
Batch B1 (executing)
    â”‚
    â–¼ Engine calls transform.process([row1, row2, row3], ctx)
    â”‚
Batch B1 (completed)
    â”‚
    â””â”€â”€â–º Output Token T4
         row_data: {count: 3, sum: 150, avg: 50}

Input tokens T1, T2, T3 â†’ terminal state: CONSUMED_IN_BATCH
```

#### Crash Recovery

The engine persists buffer state in checkpoints:
- `get_checkpoint_state()` serializes buffered rows and batch metadata
- `restore_from_checkpoint()` restores buffers after crash
- Trigger evaluators resume from correct count

This means in-progress batches survive crashes and can be resumed.

#### Audit Trail

- Batch created: batch_id, trigger config, aggregation_node_id
- Batch membership: which tokens belong to which batch (ordinal position)
- Batch state transitions: draft â†’ executing â†’ completed/failed
- Trigger event: which condition fired, when
- Transform input/output hashes for the batch call

---

### Why Not Plugins?

These operations are engine-level because:

1. **They don't touch row contents** - A gate evaluates a condition but doesn't modify the row. Fork copies tokens but doesn't change data. Coalesce combines outputs but the merge strategy is config, not code.

2. **They require DAG coordination** - Fork/coalesce semantics span multiple nodes. The engine must track token lineage, manage parallel paths, handle timeouts. This is orchestration, not business logic.

3. **Config is sufficient** - All behavior is expressible via:
   - Expressions: `"row['field'] > value"`
   - Policies: `require_all`, `quorum`, `best_effort`
   - Strategies: `union`, `nested`, `select`

4. **100% our code** - These are ELSPETH internals with comprehensive testing. No user extension points, no defensive programming needed.

---

## Exception Handling Summary

| Method | On Failure |
|--------|------------|
| `__init__` | Raise immediately - misconfiguration |
| `on_start` | CODE BUG â†’ crash |
| `on_complete` | CODE BUG â†’ crash |
| `close` | Log error, don't raise |
| `load` (Source) | Validation failure â†’ quarantine via `on_validation_failure`; Code bug â†’ crash |
| `process` (Transform) | Processing error â†’ route via `on_error`; Code bug â†’ crash |
| `write` (Sink) | Data error â†’ return error result; Code bug â†’ raise |

### Error Routing Summary

| Plugin Type | Config Field | Required? | On Missing Config + Error |
|-------------|--------------|-----------|---------------------------|
| **Source** | `on_validation_failure` | Yes | N/A (config validation fails) |
| **Transform** | `on_error` | No | `ConfigurationError` - pipeline crashes |
| **Sink** | N/A | N/A | Sinks don't route errors |

---

## Determinism Declaration

Plugins declare their determinism level:

```python
class Determinism(StrEnum):
    DETERMINISTIC = "deterministic"          # Same input â†’ same output, always
    SEEDED = "seeded"                        # Same input â†’ same output given captured seed
    IO_READ = "io_read"                      # Reads from external state (files/env/time)
    IO_WRITE = "io_write"                    # Writes side effects (files/db)
    EXTERNAL_CALL = "external_call"          # Network/API calls (record request/response)
    NON_DETERMINISTIC = "non_deterministic"  # Cannot be reproduced (record outputs)
```

**Implications:**
- `DETERMINISTIC`: Safe to replay; verify can recompute and compare
- `SEEDED`: Capture and replay with same seed
- `IO_READ`: Capture what was read (inputs/metadata) for replay/verify
- `IO_WRITE`: Side effects; replay requires care and idempotency
- `EXTERNAL_CALL`: Record request/response for replay; verify can diff responses
- `NON_DETERMINISTIC`: Record outputs; verify cannot recompute deterministically

---

## Engine Concerns (Not Plugin Contract)

The following are handled by the ELSPETH engine, not by plugins. They are documented here for completeness but are NOT part of the plugin contract.

### Schema Validation

The engine validates schema compatibility between connected nodes at pipeline construction time:
- Source `output_schema` â†’ Transform `input_schema`
- Transform `output_schema` â†’ next Transform `input_schema` or Sink `input_schema`

Plugins declare schemas; the engine enforces compatibility. Plugins do NOT validate schemas themselves.

### Retry Policy

Retry behavior is configured at the engine level, not declared by plugins:

```yaml
retry:
  max_attempts: 3
  base_delay: 1.0
  max_delay: 60.0
  jitter: 1.0
```

Plugins indicate whether errors are `retryable` in their result objects. The engine decides whether and when to retry based on policy.

### Rate Limiting

External call rate limits are engine configuration:

```yaml
rate_limit:
  calls_per_second: 10
  burst: 20
```

Plugins make calls; the engine throttles them.

---

## Version History

| Version | Date | Changes |
|---------|------|---------|
| 1.10 | 2026-02-13 | RC-3 alignment â€” Fixed stale "(config OR plugin)" gate description to "(config-driven)" (gate plugins removed 2026-02-11). Corrected gate key property to reflect routing-only behavior (no row modification). |
| 1.9 | 2026-02-08 | Second accuracy pass â€” Fixed `Determinism` comment (all 6 levels, not 3), `Enum` â†’ `StrEnum`, `list[dict]` â†’ `list[PipelineRow]` in aggregation section, `invalid_data` â†’ `invalid_input` error category, `RoutingAction.route()` parameter clarification, `BatchTransformProtocol` attribute precision |
| 1.8 | 2026-02-08 | Accuracy pass â€” Fixed `RoutingAction.fork()` â†’ `fork_to_paths()`, documented `SanitizedDatabaseUrl`/`SanitizedWebhookUrl` for ArtifactDescriptor factories, documented `BatchTransformProtocol` as separate protocol, added `transforms_adds_fields` attribute, expanded expression language reference, fixed `ctx.run_started_at` reference, documented sink resume capability, noted `CoalesceProtocol` existence |
| 1.7 | 2026-02-05 | **CRITICAL UPDATES**: (1) `TransformResult.success()` requires `success_reason` parameter (keyword-only, crashes if missing), (2) Row type changed from `dict[str, Any]` to `PipelineRow` in Transform/Gate signatures, (3) Added `context_after` field for operational metadata, (4) Added `contract` field and `to_pipeline_row()` methods on all result types, (5) Enhanced `flush()` documentation with durability guarantees, (6) Updated `SourceRow` to document `contract` parameter and `to_pipeline_row()` method, (7) Fixed `load()` return type to `Iterator[SourceRow]` |
| 1.6 | 2026-01-20 | Gate documentation: config expression gates, `GateResult` contract, routing actions |
| 1.5 | 2026-01-19 | Multi-row output: `creates_tokens` attribute, `TransformResult.success_multi()`, `rows` field, aggregation `output_mode` (passthrough/transform), `BUFFERED` and `EXPANDED` outcomes |
| 1.4 | 2026-01-19 | Batch-aware transforms (`is_batch_aware`), structural aggregation (engine owns buffers), crash recovery for batches |
| 1.3 | 2026-01-17 | Source `on_validation_failure` (required), Transform `on_error` (optional), QuarantineEvent, TransformErrorEvent |
| 1.2 | 2026-01-17 | Three-tier trust model, coercion rules by plugin type, type-safe â‰  operation-safe |
| 1.1 | 2026-01-17 | Add content_hash requirements, expression safety, engine concerns |
| 1.0 | 2026-01-17 | Initial contract - Source, Transform, Sink + System Operations |
