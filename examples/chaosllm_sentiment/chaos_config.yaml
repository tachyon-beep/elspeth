# ChaosLLM config for the chaosllm_sentiment example.
#
# Returns canned sentiment analysis responses from responses.jsonl,
# with ~20% error injection across HTTP errors, malformed responses,
# and connection failures. Latency is kept low so the example finishes fast.

server:
  host: "127.0.0.1"
  port: 8199
  workers: 1

metrics:
  database: "file:chaosllm-metrics?mode=memory&cache=shared"

response:
  mode: preset
  preset:
    file: examples/chaosllm_sentiment/responses.jsonl
    selection: sequential

latency:
  base_ms: 30
  jitter_ms: 20

error_injection:
  # HTTP errors (~12%)
  rate_limit_pct: 5.0
  capacity_529_pct: 2.0
  service_unavailable_pct: 1.0
  internal_error_pct: 1.0
  bad_gateway_pct: 1.0
  gateway_timeout_pct: 1.0
  retry_after_sec: [1, 2]

  # Connection-level failures (~3%)
  timeout_pct: 1.0
  timeout_sec: [2, 4]
  connection_reset_pct: 1.0
  slow_response_pct: 1.0
  slow_response_sec: [1, 3]

  # Malformed responses (~5%)
  invalid_json_pct: 2.0
  truncated_pct: 1.0
  empty_body_pct: 1.0
  missing_fields_pct: 1.0

  # Periodic bursts: 5 seconds of heavy rate limiting every 30 seconds
  burst:
    enabled: true
    interval_sec: 30
    duration_sec: 5
    rate_limit_pct: 60
    capacity_pct: 40
