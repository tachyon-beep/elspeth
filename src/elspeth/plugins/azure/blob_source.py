# src/elspeth/plugins/azure/blob_source.py
"""Azure Blob Storage source plugin for ELSPETH.

Loads rows from Azure Blob containers. Supports CSV, JSON array, and JSONL formats.

IMPORTANT: Sources use allow_coercion=True to normalize external data.
This is the ONLY place in the pipeline where coercion is allowed.
"""

from __future__ import annotations

import io
import json
import logging
import time
from collections.abc import Iterator
from typing import TYPE_CHECKING, Any, Literal, Self

import pandas as pd
from pydantic import BaseModel, Field, ValidationError, field_validator, model_validator

from elspeth.contracts import CallStatus, CallType, PluginSchema, SourceRow
from elspeth.plugins.azure.auth import AzureAuthConfig
from elspeth.plugins.base import BaseSource
from elspeth.plugins.config_base import DataPluginConfig
from elspeth.plugins.context import PluginContext
from elspeth.plugins.schema_factory import create_schema_from_config

if TYPE_CHECKING:
    from azure.storage.blob import BlobClient

logger = logging.getLogger(__name__)


class CSVOptions(BaseModel):
    """CSV parsing options."""

    model_config = {"extra": "forbid"}

    delimiter: str = ","
    has_header: bool = True
    encoding: str = "utf-8"


class JSONOptions(BaseModel):
    """JSON parsing options."""

    model_config = {"extra": "forbid"}

    encoding: str = "utf-8"
    data_key: str | None = None


class AzureBlobSourceConfig(DataPluginConfig):
    """Configuration for Azure Blob source plugin.

    Extends DataPluginConfig which requires schema configuration.
    Unlike file-based sources, does not extend PathConfig (no local file path).

    Supports four authentication methods (mutually exclusive):
    1. connection_string - Simple connection string auth (default)
    2. sas_token + account_url - Shared Access Signature token
    3. use_managed_identity + account_url - Azure Managed Identity
    4. tenant_id + client_id + client_secret + account_url - Service Principal

    Example configurations:

        # Option 1: Connection string (simplest)
        connection_string: "${AZURE_STORAGE_CONNECTION_STRING}"
        container: "my-container"
        blob_path: "data/input.csv"

        # Option 2: SAS token
        sas_token: "${AZURE_STORAGE_SAS_TOKEN}"
        account_url: "https://mystorageaccount.blob.core.windows.net"
        container: "my-container"
        blob_path: "data/input.csv"

        # Option 3: Managed Identity (for Azure-hosted workloads)
        use_managed_identity: true
        account_url: "https://mystorageaccount.blob.core.windows.net"
        container: "my-container"
        blob_path: "data/input.csv"

        # Option 4: Service Principal
        tenant_id: "${AZURE_TENANT_ID}"
        client_id: "${AZURE_CLIENT_ID}"
        client_secret: "${AZURE_CLIENT_SECRET}"
        account_url: "https://mystorageaccount.blob.core.windows.net"
        container: "my-container"
        blob_path: "data/input.csv"
    """

    # Auth Option 1: Connection string
    connection_string: str | None = Field(
        default=None,
        description="Azure Storage connection string",
    )

    # Auth Option 2: SAS token
    sas_token: str | None = Field(
        default=None,
        description="Azure Storage SAS token (with or without leading '?')",
    )

    # Auth Option 3: Managed Identity
    use_managed_identity: bool = Field(
        default=False,
        description="Use Azure Managed Identity for authentication",
    )
    account_url: str | None = Field(
        default=None,
        description="Azure Storage account URL (e.g., https://mystorageaccount.blob.core.windows.net)",
    )

    # Auth Option 4: Service Principal
    tenant_id: str | None = Field(
        default=None,
        description="Azure AD tenant ID for Service Principal auth",
    )
    client_id: str | None = Field(
        default=None,
        description="Azure AD client ID for Service Principal auth",
    )
    client_secret: str | None = Field(
        default=None,
        description="Azure AD client secret for Service Principal auth",
    )

    # Blob location (required for all auth methods)
    container: str = Field(
        ...,
        description="Azure Blob container name",
    )
    blob_path: str = Field(
        ...,
        description="Path to blob within container",
    )
    format: Literal["csv", "json", "jsonl"] = Field(
        default="csv",
        description="Data format: csv, json (array), or jsonl (newline-delimited)",
    )
    csv_options: CSVOptions = Field(
        default_factory=CSVOptions,
        description="CSV parsing options (delimiter, has_header, encoding)",
    )
    json_options: JSONOptions = Field(
        default_factory=JSONOptions,
        description="JSON parsing options (encoding, data_key)",
    )
    on_validation_failure: str = Field(
        ...,
        description="Sink name for non-conformant rows, or 'discard' for explicit drop",
    )

    @model_validator(mode="after")
    def validate_auth_config(self) -> Self:
        """Validate authentication configuration via AzureAuthConfig.

        Delegates to AzureAuthConfig for comprehensive auth validation,
        ensuring exactly one auth method is configured.
        """
        # Create AzureAuthConfig to validate auth fields
        # This will raise ValueError with descriptive messages if invalid
        AzureAuthConfig(
            connection_string=self.connection_string,
            sas_token=self.sas_token,
            use_managed_identity=self.use_managed_identity,
            account_url=self.account_url,
            tenant_id=self.tenant_id,
            client_id=self.client_id,
            client_secret=self.client_secret,
        )
        return self

    def get_auth_config(self) -> AzureAuthConfig:
        """Get the AzureAuthConfig for this source configuration.

        Returns:
            AzureAuthConfig instance with the auth fields from this config.
        """
        return AzureAuthConfig(
            connection_string=self.connection_string,
            sas_token=self.sas_token,
            use_managed_identity=self.use_managed_identity,
            account_url=self.account_url,
            tenant_id=self.tenant_id,
            client_id=self.client_id,
            client_secret=self.client_secret,
        )

    @field_validator("container")
    @classmethod
    def validate_container_not_empty(cls, v: str) -> str:
        """Validate that container is not empty or whitespace-only."""
        if not v or not v.strip():
            raise ValueError("container cannot be empty")
        return v

    @field_validator("blob_path")
    @classmethod
    def validate_blob_path_not_empty(cls, v: str) -> str:
        """Validate that blob_path is not empty or whitespace-only."""
        if not v or not v.strip():
            raise ValueError("blob_path cannot be empty")
        return v

    @field_validator("on_validation_failure")
    @classmethod
    def validate_on_validation_failure(cls, v: str) -> str:
        """Ensure on_validation_failure is not empty."""
        if not v or not v.strip():
            raise ValueError("on_validation_failure must be a sink name or 'discard'")
        return v.strip()


# Rebuild model to resolve forward references for dynamic module loading
AzureBlobSourceConfig.model_rebuild()


class AzureBlobSource(BaseSource):
    """Load rows from Azure Blob Storage.

    Config options:
        Authentication (exactly one required):
        - connection_string: Azure Storage connection string
        - use_managed_identity + account_url: Azure Managed Identity
        - tenant_id + client_id + client_secret + account_url: Service Principal

        Blob location:
        - container: Blob container name (required)
        - blob_path: Path to blob within container (required)
        - format: "csv", "json" (array), or "jsonl" (lines). Default: "csv"

        Parsing options:
        - csv_options: CSV parsing options (delimiter, has_header, encoding)
        - json_options: JSON parsing options (encoding, data_key)
        - schema: Schema configuration (required, via DataPluginConfig)
        - on_validation_failure: Sink name or "discard" (required)

    The schema can be:
        - Dynamic: {"fields": "dynamic"} - accept any fields
        - Strict: {"mode": "strict", "fields": ["id: int", "name: str"]}
        - Free: {"mode": "free", "fields": ["id: int"]} - at least these fields

    Three-tier trust model:
        - Azure Blob SDK calls = EXTERNAL SYSTEM -> wrap with try/except
        - Row data parsing/validation = THEIR DATA -> wrap, quarantine failures
        - Our internal state = OUR CODE -> let it crash
    """

    name = "azure_blob"

    def __init__(self, config: dict[str, Any]) -> None:
        super().__init__(config)
        cfg = AzureBlobSourceConfig.from_dict(config)

        # Store auth config for creating clients
        self._auth_config = cfg.get_auth_config()
        self._container = cfg.container
        self._blob_path = cfg.blob_path
        self._format = cfg.format
        self._csv_options = cfg.csv_options
        self._json_options = cfg.json_options

        # Store schema config for audit trail
        # DataPluginConfig ensures schema_config is not None
        self._schema_config = cfg.schema_config

        # Store quarantine routing destination
        self._on_validation_failure = cfg.on_validation_failure

        # CRITICAL: allow_coercion=True for sources (external data boundary)
        # Sources are the ONLY place where type coercion is allowed
        self._schema_class: type[PluginSchema] = create_schema_from_config(
            self._schema_config,
            "AzureBlobRowSchema",
            allow_coercion=True,
        )

        # Set output_schema for protocol compliance
        self.output_schema = self._schema_class

        # Lazy-loaded blob client
        self._blob_client: BlobClient | None = None

        # PHASE 1: Validate self-consistency

    def _get_blob_client(self) -> BlobClient:
        """Get or create the Azure Blob client.

        Uses the configured authentication method (connection string,
        managed identity, or service principal) to create the client.

        Returns:
            BlobClient for the configured blob.

        Raises:
            ImportError: If azure-storage-blob (or azure-identity for
                managed identity/service principal) is not installed.
        """
        if self._blob_client is None:
            # Use shared auth config to create the service client
            service_client = self._auth_config.create_blob_service_client()
            container_client = service_client.get_container_client(self._container)
            self._blob_client = container_client.get_blob_client(self._blob_path)

        return self._blob_client

    def load(self, ctx: PluginContext) -> Iterator[SourceRow]:
        """Load rows from Azure Blob Storage.

        Each row is validated against the configured schema:
        - Valid rows are yielded as SourceRow.valid()
        - Invalid rows are yielded as SourceRow.quarantined()

        Yields:
            SourceRow for each row (valid or quarantined).

        Raises:
            ImportError: If azure-storage-blob is not installed.
            azure.core.exceptions.ResourceNotFoundError: If blob does not exist.
            azure.core.exceptions.ClientAuthenticationError: If connection fails.
        """
        # EXTERNAL SYSTEM: Azure Blob SDK calls - wrap with try/except
        # Record call for audit trail (ctx.operation_id is set by orchestrator)
        start_time = time.perf_counter()
        try:
            blob_client = self._get_blob_client()
            blob_data = blob_client.download_blob().readall()
            latency_ms = (time.perf_counter() - start_time) * 1000

            # Record successful blob download in audit trail
            ctx.record_call(
                call_type=CallType.HTTP,
                status=CallStatus.SUCCESS,
                request_data={
                    "operation": "download_blob",
                    "container": self._container,
                    "blob_path": self._blob_path,
                },
                response_data={"size_bytes": len(blob_data)},
                latency_ms=latency_ms,
                provider="azure_blob_storage",
            )
        except ImportError:
            # Re-raise ImportError as-is for clear dependency messaging
            raise
        except Exception as e:
            latency_ms = (time.perf_counter() - start_time) * 1000

            # Record failed blob download in audit trail
            ctx.record_call(
                call_type=CallType.HTTP,
                status=CallStatus.ERROR,
                request_data={
                    "operation": "download_blob",
                    "container": self._container,
                    "blob_path": self._blob_path,
                },
                error={"type": type(e).__name__, "message": str(e)},
                latency_ms=latency_ms,
                provider="azure_blob_storage",
            )

            # Azure SDK errors (ResourceNotFoundError, ClientAuthenticationError, etc.)
            # are external system errors - propagate with context
            raise type(e)(f"Failed to download blob '{self._blob_path}' from container '{self._container}': {e}") from e

        # Log blob download for operator visibility
        blob_size_kb = len(blob_data) / 1024
        if blob_size_kb >= 1024:
            size_str = f"{blob_size_kb / 1024:.1f} MB"
        else:
            size_str = f"{blob_size_kb:.1f} KB"
        logger.info(
            "Downloaded blob '%s' from container '%s' (%s)",
            self._blob_path,
            self._container,
            size_str,
        )

        # Parse blob content based on format
        if self._format == "csv":
            yield from self._load_csv(blob_data, ctx)
        elif self._format == "json":
            yield from self._load_json_array(blob_data, ctx)
        elif self._format == "jsonl":
            yield from self._load_jsonl(blob_data, ctx)

    def _load_csv(self, blob_data: bytes, ctx: PluginContext) -> Iterator[SourceRow]:
        """Load rows from CSV blob data.

        Args:
            blob_data: Raw bytes from blob download.
            ctx: Plugin context for recording validation errors.

        Yields:
            SourceRow for each row (valid or quarantined).
        """
        # Access typed config fields directly - they have defaults from CSVOptions
        delimiter = self._csv_options.delimiter
        has_header = self._csv_options.has_header
        encoding = self._csv_options.encoding

        # THEIR DATA: Parsing blob content - wrap operations
        try:
            text_data = blob_data.decode(encoding)
        except UnicodeDecodeError as e:
            raise ValueError(f"Failed to decode blob as {encoding}: {e}") from e

        # BUG-BLOB-01 fix: Wrap pandas CSV parsing to quarantine on structural errors
        # Even with pandas' robustness, severely malformed CSVs can cause parse failures
        try:
            # Use pandas for robust CSV parsing (consistent with CSVSource)
            header_arg = 0 if has_header else None

            # When headerless CSV with explicit schema, use schema field names
            names_arg = None
            if not has_header and not self._schema_config.is_dynamic and self._schema_config.fields:
                names_arg = [field_def.name for field_def in self._schema_config.fields]

            df = pd.read_csv(
                io.StringIO(text_data),
                delimiter=delimiter,
                header=header_arg,
                names=names_arg,  # Map columns to schema field names
                dtype=str,  # Keep all values as strings for consistent handling
                keep_default_na=False,  # Don't convert empty strings to NaN
                on_bad_lines="warn",  # Warn but skip bad lines instead of crashing
            )
        except Exception as e:
            # Catastrophic CSV structure failure - entire file unparseable
            # This is rare with pandas but can happen with severely malformed files
            error_msg = f"CSV parse error: Unable to parse blob structure: {e}"
            raw_row = {"__raw_blob_preview__": text_data[:200], "__encoding__": encoding}

            ctx.record_validation_error(
                row=raw_row,
                error=error_msg,
                schema_mode="parse",
                destination=self._on_validation_failure,
            )

            if self._on_validation_failure != "discard":
                yield SourceRow.quarantined(
                    row=raw_row,
                    error=error_msg,
                    destination=self._on_validation_failure,
                )
            return  # No rows to process if file is completely unparseable

        # Log row count for operator visibility
        row_count = len(df)
        logger.info("Parsed %d rows from CSV blob '%s'", row_count, self._blob_path)

        # DataFrame columns are strings from CSV headers
        for record in df.to_dict(orient="records"):
            row = {str(k): v for k, v in record.items()}
            yield from self._validate_and_yield(row, ctx)

    def _load_json_array(self, blob_data: bytes, ctx: PluginContext) -> Iterator[SourceRow]:
        """Load rows from JSON array blob data.

        Args:
            blob_data: Raw bytes from blob download.
            ctx: Plugin context for recording validation errors.

        Yields:
            SourceRow for each row (valid or quarantined).

        Raises:
            ValueError: If JSON is invalid or not an array.
        """
        # Access typed config fields directly - they have defaults from JSONOptions
        encoding = self._json_options.encoding
        data_key = self._json_options.data_key

        # THEIR DATA: JSON parsing - wrap operations
        try:
            text_data = blob_data.decode(encoding)
            data = json.loads(text_data)
        except UnicodeDecodeError as e:
            raise ValueError(f"Failed to decode blob as {encoding}: {e}") from e
        except json.JSONDecodeError as e:
            raise ValueError(f"Invalid JSON in blob: {e}") from e

        # Extract from nested key if specified
        if data_key:
            if not isinstance(data, dict) or data_key not in data:
                raise ValueError(f"Expected JSON object with key '{data_key}', got {type(data).__name__}")
            data = data[data_key]

        if not isinstance(data, list):
            raise ValueError(f"Expected JSON array, got {type(data).__name__}")

        # Log row count for operator visibility
        logger.info("Parsed %d rows from JSON array blob '%s'", len(data), self._blob_path)

        for row in data:
            yield from self._validate_and_yield(row, ctx)

    def _load_jsonl(self, blob_data: bytes, ctx: PluginContext) -> Iterator[SourceRow]:
        """Load rows from JSONL (newline-delimited JSON) blob data.

        Args:
            blob_data: Raw bytes from blob download.
            ctx: Plugin context for recording validation errors.

        Yields:
            SourceRow for each row (valid or quarantined).
        """
        # Access typed config field directly - it has a default from JSONOptions
        encoding = self._json_options.encoding

        # THEIR DATA: Decoding blob content - wrap operations
        try:
            text_data = blob_data.decode(encoding)
        except UnicodeDecodeError as e:
            raise ValueError(f"Failed to decode blob as {encoding}: {e}") from e

        # Split lines and count non-empty for logging
        lines = text_data.splitlines()
        non_empty_count = sum(1 for line in lines if line.strip())
        logger.info("Parsed %d lines from JSONL blob '%s'", non_empty_count, self._blob_path)

        for line_num, line in enumerate(lines, start=1):
            line = line.strip()
            if not line:  # Skip empty lines
                continue

            # Catch JSON parse errors at the trust boundary
            try:
                row = json.loads(line)
            except json.JSONDecodeError as e:
                # External data parse failure - quarantine, don't crash
                # Store raw line + metadata for audit traceability
                raw_row = {"__raw_line__": line, "__line_number__": line_num}
                error_msg = f"JSON parse error at line {line_num}: {e}"

                ctx.record_validation_error(
                    row=raw_row,
                    error=error_msg,
                    schema_mode="parse",  # Distinct from schema validation
                    destination=self._on_validation_failure,
                )

                if self._on_validation_failure != "discard":
                    yield SourceRow.quarantined(
                        row=raw_row,
                        error=error_msg,
                        destination=self._on_validation_failure,
                    )
                continue

            yield from self._validate_and_yield(row, ctx)

    def _validate_and_yield(self, row: Any, ctx: PluginContext) -> Iterator[SourceRow]:
        """Validate a row and yield if valid, otherwise quarantine.

        Args:
            row: Row data to validate. May be non-dict for malformed external
                 data (e.g., JSON arrays containing primitives).
            ctx: Plugin context for recording validation errors.

        Yields:
            SourceRow.valid() if valid, SourceRow.quarantined() if invalid.
        """
        try:
            # Validate and potentially coerce row data
            validated = self._schema_class.model_validate(row)
            yield SourceRow.valid(validated.to_row())
        except ValidationError as e:
            # Record validation failure in audit trail
            # This is a trust boundary: external data may be invalid
            ctx.record_validation_error(
                row=row,
                error=str(e),
                schema_mode=self._schema_config.mode or "dynamic",
                destination=self._on_validation_failure,
            )

            # Yield quarantined row for routing to configured sink
            # If "discard", don't yield - row is intentionally dropped
            if self._on_validation_failure != "discard":
                yield SourceRow.quarantined(
                    row=row,
                    error=str(e),
                    destination=self._on_validation_failure,
                )

    def close(self) -> None:
        """Release resources (no-op for Azure Blob source)."""
        self._blob_client = None
