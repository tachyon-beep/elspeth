# ChaosLLM Preset: silent
# Purpose: Zero errors, maximum throughput - for baseline measurements.
# Use when: Measuring maximum pipeline throughput, establishing baselines,
#           verifying pipeline correctness without error injection noise.

server:
  host: "127.0.0.1"
  port: 8000
  workers: 4

metrics:
  database: "file:chaosllm-metrics?mode=memory&cache=shared"
  timeseries_bucket_sec: 1

response:
  mode: random
  random:
    min_words: 10
    max_words: 50
    vocabulary: english

latency:
  base_ms: 10
  jitter_ms: 5

error_injection:
  # Zero HTTP errors
  rate_limit_pct: 0.0
  capacity_529_pct: 0.0
  service_unavailable_pct: 0.0
  retry_after_sec: [1, 5]

  # Zero server errors
  internal_error_pct: 0.0
  bad_gateway_pct: 0.0
  gateway_timeout_pct: 0.0

  # Zero client errors
  forbidden_pct: 0.0
  not_found_pct: 0.0

  # Zero connection-level failures
  timeout_pct: 0.0
  timeout_sec: [30, 60]
  connection_reset_pct: 0.0
  slow_response_pct: 0.0
  slow_response_sec: [10, 30]

  # Zero malformed responses
  invalid_json_pct: 0.0
  truncated_pct: 0.0
  empty_body_pct: 0.0
  missing_fields_pct: 0.0
  wrong_content_type_pct: 0.0

  # No bursts
  burst:
    enabled: false
    interval_sec: 30
    duration_sec: 5
    rate_limit_pct: 80
    capacity_pct: 50
