# ChaosLLM Preset: gentle
# Purpose: Basic functionality testing with minimal error injection.
# Use when: Verifying basic pipeline operation, debugging, initial setup.

server:
  host: "127.0.0.1"
  port: 8000
  workers: 4

metrics:
  database: "file:chaosllm-metrics?mode=memory&cache=shared"
  timeseries_bucket_sec: 1

response:
  mode: random
  random:
    min_words: 10
    max_words: 50
    vocabulary: english

latency:
  base_ms: 50
  jitter_ms: 20

error_injection:
  # Minimal errors - 2% total
  rate_limit_pct: 1.0
  capacity_529_pct: 0.5
  service_unavailable_pct: 0.5
  retry_after_sec: [1, 3]

  # No server errors
  internal_error_pct: 0.0
  bad_gateway_pct: 0.0
  gateway_timeout_pct: 0.0

  # No client errors
  forbidden_pct: 0.0
  not_found_pct: 0.0

  # No connection-level failures
  timeout_pct: 0.0
  timeout_sec: [30, 60]
  connection_reset_pct: 0.0
  slow_response_pct: 0.0
  slow_response_sec: [10, 30]

  # No malformed responses
  invalid_json_pct: 0.0
  truncated_pct: 0.0
  empty_body_pct: 0.0
  missing_fields_pct: 0.0
  wrong_content_type_pct: 0.0

  # No burst patterns
  burst:
    enabled: false
    interval_sec: 30
    duration_sec: 5
    rate_limit_pct: 80
    capacity_pct: 50
