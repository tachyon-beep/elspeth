# Status Note (2025-05)

## Recently Completed
- Added Jinja-based prompt engine and updated ExperimentRunner/SuiteRunner to use prompt defaults, criteria templates, and middleware hooks.
- Implemented metrics plugins, adaptive sinks (blob/local bundle/repo/signed), and mock/local plugins for offline runs.
- Phase 7 completed: sample suite (`config/sample_suite`), bootstrap tooling (`scripts/bootstrap.sh`, `Makefile`), and documentation refresh (README, AGENTS, phase notes).
- Introduced LLM middleware (audit logger, prompt shield), adaptive rate limiter, and concurrency-aware runner execution with tests (`tests/test_llm_middleware.py`, `tests/test_experiments.py::test_experiment_runner_concurrency`).

## Outstanding Work
1. **Azure ML Telemetry** – port legacy logging (metric rows, config diffs) into optional middleware/sink.
2. **DevOps/Excel/Zip Archiving** – refactor legacy exporters/archivers into sink plugins (Excel bundle, DevOps uploader, environment capture).
3. **Advanced Statistics** – port `StatsAnalyzer` functionality (Bayesian comparisons, effect sizes, power analysis) as optional aggregation plugins with optional deps.
4. **Schema & Preflight Validation** – reintroduce experiment schema validation, duplicate detection, API call/time/cost estimates, and CLI warnings.
5. **Concurrency Metrics** – expose threads/queue utilization in metadata for monitoring.

## Next Up
a. Design telemetry middleware/sink for Azure ML (Phase after safety).
b. Plan DevOps/Excel sink plugins (Phase 8?).

